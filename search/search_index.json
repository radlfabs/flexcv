{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Flexible Cross Validation and Machine Learning for Regression on Tabular Data  <p>Find the repository here.</p> <p><code>flexcv</code> is a Python package that implements flexible cross validation and machine learning for tabular data. It provides a range of features for comparing machine learning models on different datasets with different sets of predictors, customizing just about everything around cross validations. It supports both fixed and random effects, as well as random slopes.</p>"},{"location":"#features","title":"Features","text":"<p>The <code>flexcv</code> package provides the following features:</p> <ol> <li>Cross-validation of model performance (generalization estimation)</li> <li>Selection of model hyperparameters using an inner cross-validation and a state-of-the-art optimization provided by optuna.</li> <li>Customization of objective functions for optimization to select meaningful model parameters.</li> <li>Fixed and mixed effects modeling (random intercepts and slopes).</li> <li>Scaling of inner and outer cross-validation folds separately.</li> <li>Easy usage of the state-of-the-art MLops platform <code>neptune</code> to track all of your experiments. Check out their website or explore our neptune project that we used for testing this package. Also check out the neptune integration guide.</li> <li>Integrates the <code>merf</code> package to apply correction for clustered data using the expectation maximization algorithm and supporting any <code>sklearn</code> BaseEstimator. Read more about that package in this blog post or go right to their repo.</li> <li>Adaptations for cross validation splits with stratification for continuous target variables.</li> <li>Easy local summary of all evaluation metrics in a single table.</li> <li>Wrapper classes for the <code>statsmodels</code> package to use their mixed effects models inside of a <code>sklearn</code> Pipeline. Read more about that package here.</li> <li>Inner cross validation implementation that let's you push groups to the inner split, e. g. to apply GroupKFold.</li> <li>Customizable ObjectiveScorer function for hyperparameter tuning, that let's you make a trade-off between under- and overfitting.</li> </ol> <p>These are the core packages used under the hood in <code>flexcv</code>:</p> <ol> <li><code>sklearn</code> - A very popular machine learning library. We use their Estimator API for models, the pipeline module, the StandardScaler, metrics and of course wrap around their cross validation split methods. Learn more here.</li> <li><code>Optuna</code> - A state-of-the-art optimization package. We use it for parameter selection in the inner loop of our nested cross validation. Learn more about theoretical background and opportunities here.</li> <li><code>neptune</code> - Awesome logging dashboard with lots of integrations. It is a charm in combination with <code>Optuna</code>. We used it to track all of our experiments. <code>Neptune</code> is quite deeply integrated into <code>flexcv</code>. Learn more about this great library here.</li> <li><code>merf</code> - Mixed Effects for Random Forests. Applies correction terms on the predictions of clustered data. Works not only with random forest but with every <code>sklearn</code> BaseEstimator.</li> </ol>"},{"location":"#why-would-you-use-flexcv","title":"Why would you use <code>flexcv</code>?","text":"<p>Working with cross validation in Python usually starts with creating a sklearn pipeline. Pipelines are super useful to combine preprocessing steps with model fitting and prevent data leakage.  However, there are limitations, e. g. if you want to push the training part of your clustering variable to the inner cross validation split. For some of the features, you would have to write a lot of boilerplate code to get it working, and you end up with a lot of code duplication. As soon as you want to use a linear mixed effects model, you have to use the <code>statsmodels</code> package, which is not compatible with the <code>sklearn</code> pipeline. <code>flexcv</code> solves these problems and provides a lot of useful features for cross validation and machine learning on tabular data, so you can focus on your data and your models.</p>"},{"location":"#earth-extension","title":"Earth Extension","text":"<p>An wrapper implementation of the Earth Regression package for R exists which you can use with flexcv. It is called flexcv-earth. It is not yet available on PyPI, but you can install it from GitHub with the command <code>pip install git+https://github.com/radlfabs/flexcv-earth.git</code>. You can then use the <code>EarthModel</code> class in your <code>flexcv</code> configuration by importing it from <code>flexcv_earth</code>. Further information is available in the documentation.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>We welcome contributions to this repository. If you have any questions, please don't hesitate to get in contact by reaching out or filing a github issue.</p>"},{"location":"about/","title":"About","text":"<p><code>flexcv</code> was developed at Hochschule D\u00fcsseldorf, Germany by Fabian Rosenthal, Patrick Bl\u00e4ttermann and Siegbert Vers\u00fcmer. It is used for the machine learning evaluations in Vers\u00fcmer et al. (2023).</p> <p><code>flexcv</code> is a Python package and provides a range of features for comparing machine learning models on different datasets with different sets of predictors customizing just about everything around cross validations. It supports both fixed effects and random effects including random slopes.</p>"},{"location":"about/#project-history","title":"Project History","text":"<p><code>flexcv</code> has been developed iteratively to switch and compare methods in calculations for the Soundscape research paper Vers\u00fcmer et al. (2023) which is currently being finished. The paper uses a large dataset of Indoor Soundscape observations, i.e. incorporating both situational and personal features as well as acoustic measurements of the Soundscapes. If you are interested in our research, you can find our previous papers here.</p> <p>In addition, an interactive visualisation app was released for the same dataset, which can be adapted to personal needs and to work with other datasets. Have a look at the technical report or the repo, if you're interested in the capabilities.</p>"},{"location":"about/#creators","title":"Creators","text":"<p>Fabian Rosenthal developed the overall structure of the package, designed the interface, implemented the core functionality and modules and is author of the documentation. As a novel addition to the methods for cross validation splits, he contributed a stratified split method for continuous target variables to ensure target distribution across folds. Fabian continues to lead the project as the sole maintainer without pay.</p> <p>Patrick Bl\u00e4ttermann implemented the evaluation based on random slopes for the linear mixed effects model and wrote the inner cross validation loop in order to allow customization of the objective function. Patrick served as a consultant on any statistical and machine learning related questions arising.</p> <p>Siegbert Vers\u00fcmer led the project as product owner in terms of its main ideas and requirements. He contributed a custom objective function (i.e. minimizing the MSE of the inner test set of a nested cross-validation while maintaining a small positive difference between the inner test and training MSE to avoid over- or under-fitting during hyperparameter tuning).</p>"},{"location":"user-guide/","title":"Overview","text":"<p>This user guide will give you a detailled information on how to use <code>flexcv</code> functions and objects.</p> <p>If you want to learn about nested cross validation in general and how we implemented it as a workflow in <code>flexcv</code>, check out our nested cross validation guide. This gives you a step by step overview of the process and explains the motivation behind nested cross validation.</p> <p>You can learn about the <code>neptune</code> integration and tracking your experiments by checking out our neptune integration guide. You will learn how to use this super cool MLOps tool and leverage it's power to give you great insights in data and model performance.</p> <p>Let's dive into how to fit a Random Forest Regressor on your data by tuning a hyperparameter in the inner cross validation and evaluate the model's performance in the outer cross validation.</p> <p>Also, our guide on how to evaluate random effects takes you on the journey through the land of hierarchical data and gives an overview what to consider when facing machine learning problems that have a grouped structure. <code>flexcv</code> has some tools for that.</p> <p>You might wonder, if your processes are influenced by randomness. Our guide on how to set up a repeated cross validation tackles this topic and shows how to use the <code>RepeatedCV</code> class with <code>flexcv</code>.</p> <p>You still can not decide on the model type? No problem with <code>flexcv</code> since it allows to run multiple models in a single run. This guide shows how you set up the configuration for multiple models using different methods on the interface class or by storing it in a yaml file.</p>"},{"location":"guides/flow/","title":"Nested CV Flow","text":"<p>In this guide we will go through the steps of a typical <code>flexcv</code> workflow for nested cross validation.</p> <p>Let's have a look at the following flowchart to get an overview of the steps we will take:</p> <p></p>"},{"location":"guides/flow/#why-do-we-need-nested-cross-validation","title":"Why do we need nested cross validation?","text":"<p>The motivation to perform nested cross validation is to get an unbiased estimate of the generalization performance of a model while achieving a fair hyperparameter tuning. If you tune the hyperparameters of your model on the whole data that you have, you overfit the model and lose ability to generalize on unseen data. If you fit your model on the whole dataset you lose data to estimate the generalization performance of your model. This is where nested cross validation comes in. It splits the data into training and testing data and then splits the training data again into training and validation data. The model is fit on the training data and evaluated on the validation data. This is done for every fold of the cross validation. The outer loop iterates over the folds and the inner loop evaluates a single model from the hyperparameter search space.</p>"},{"location":"guides/flow/#step-1-set-up-the-crossvalidation-instance","title":"Step 1: Set up the <code>CrossValidation</code> instance","text":"<p>Let's start with the first step. Here, we define our <code>CrossValidation</code> instance. After that we configure it to our needs by adding data, model mappings, choosing split methods and deciding if we need to evaluate mixed effects (i.e. with MERF correction for clustered data). Then we call <code>perform()</code> which under the hood calls the <code>cross_validate</code>function from the <code>core</code> module.</p> <p></p>"},{"location":"guides/flow/#step-2-perform-the-cross-validation","title":"Step 2: Perform the cross validation","text":"<p>The outer loop iterates over the folds which are split by our split classes into training and testing data. Inside this loop we also iterate over models, which are keys in the model mapping dictionary. Now, we have to perform inner cross validation for models that require hyperparameter tuning. This is done by the optuna package that uses samples a combination of parameters from the distribution in the search space with a TPE Sampler. Each sampled distribution is cross validated with the inner split to evaluate the model's performance with the given set of parameters.  The best model, i.e. the model at the objective value optimum after a given number of trials (<code>n_trials</code>), is then returned to the outer fold for evaluation on the outer test data which was unseen to this point. For tracking the optimization process we use the <code>neptune-optuna</code> integration which allows extensive plots that give you state-of-the-art insights into the optimization process. You can read more about the integration here.</p> <p>Then we check, if you want to predict mixed effects with MERF. If so, we have to perform a correction on the predictions of the model. This is done by the <code>merf</code> package where the best model from before is fit inside the MERF class. The corrected predictions are then returned to the outer fold for evaluation and training statistics are logged to neptune.</p> <p></p>"},{"location":"guides/flow/#step-3-evaluate-the-models","title":"Step 3: Evaluate the models","text":"<p>In the evaluation step we predict with the model and call all metrics functions defined by the metrics dict of the <code>CrossValidation</code> instance on the predictions and the true values. The metrics are then stored in a dictionary with the metric names as keys and the metric values as values. After that we log the metrics to neptune. When this is all done the outer fold continues with the next iteration and we start all over again.</p> <p>When every model was evaluated on every fold we can conclude the cross validation and return the results. The results are then stored in the <code>CrossValidation</code> instance and can be accessed by the <code>get_results()</code> method. The results are also stored in the <code>results</code> attribute of the <code>CrossValidation</code> instance. The results are stored in a dictionary with the model names as keys and the results as values. The results are also stored in a <code>Results</code> class instance which has a <code>summary</code> property that returns a pandas dataframe with all evaluation metrics for every model and every fold. This dataframe can be exported to excel or csv using the pandas methods <code>to_excel()</code> or <code>to_csv()</code>.</p> <p></p>"},{"location":"guides/flow/#conclusion","title":"Conclusion","text":"<p>In this guide we learned about the steps of a typical <code>flexcv</code> workflow for nested cross validation. We learned about the motivation to perform nested cross validation and why it is important to get an unbiased estimate of the generalization performance of a model while achieving a fair hyperparameter tuning. We learned about the steps of the workflow and how to access the results. We also learned about the <code>neptune</code> integration and how to use it to track the optimization process of the inner cross validation. We hope this guide was helpful to you and you can use it to get started with <code>flexcv</code> for your own projects.</p>"},{"location":"guides/multiple-models/","title":"Fit Mutiple Models","text":""},{"location":"guides/multiple-models/#evaluating-multiple-models","title":"Evaluating multiple models","text":"<p><code>flexcv</code> offers two ways of passing multiple models in set-up to our CrossValidation interface class. You either can call <code>add_model()</code> multiple times on the class instance or you can pass multiple models to <code>set_models()</code> to set a configuration for multiple models at once. The latter may be the preferred way of doing it when the number of models gets larger and you want to reuse the configuration. We will discuss both ways in this guide.</p> <p>For both ways of interacting with the <code>CrossValidation</code> class instance, a <code>ModelMappingDict</code> is created internally and stored to the instance's <code>config</code> attribute.  Since both <code>add_model()</code> and <code>set_models()</code> are updating the same attribute of the class instance, you can use both ways in combination. This is especially useful when you want to add a model to a configuration that you already set up using <code>set_models()</code>.</p> <p>In <code>CrossValidation.perform</code> the core function <code>cross_validate</code> is called and iterates over the keys in <code>ModelMappingDict</code> and fits every model to the data. As additional benefit, this provides extensive logging, results summaries and useful information such as progress bars for all layers of processes.</p>"},{"location":"guides/multiple-models/#using-add_model","title":"Using add_model()","text":"<p>So let's start with the way of adding two models the way we learned before. Say, we want to compare a LinearModel to a RandomForestRegressor. Thats as simple as this:</p> <pre><code>import optuna\nfrom sklearn.ensemble import RandomForestRegressor\nfrom flexcv import CrossValidation\nfrom flexcv.models import LinearModel\nfrom flexcv.merf import MERF\nfrom flexcv.model_postprocessing import RandomForestModelPostProcessor, LinearModelPostProcessor\nfrom flexcv.synthesizer import generate_regression\n\n\n# lets start with generating some clustered data\nX, y, group, random_slopes =generate_regression(\n    3,100,n_slopes=1,noise_level=9.1e-2\n)\n# define our hyperparameters for the random forest\nparams = {\n    \"max_depth\": optuna.distributions.IntDistribution(5,100),\n}\n\ncv =CrossValidation()\n(\n  cv.set_data(X, y, group, random_slopes)\n  .set_inner_cv(3)\n  .set_splits(n_splits_out=3)\n  .add_model(model_class=LinearModel, post_processor=LinearModelPostProcessor)\n  .add_model(model_class=RandomForestRegressor, requires_inner_cv=True, params=params, post_processor=RandomForestModelPostProcessor)\n)\n</code></pre>"},{"location":"guides/multiple-models/#configuration-using-yaml","title":"Configuration using yaml","text":"<p>A great and convenient method to configure multiple models at once is passing yaml-code to the interface. This is especially useful when you want to reuse the configuration for multiple runs and save it to a file. <code>.set_models</code> takes either a yaml-string or a path to a yaml-file.</p> <p>As a hidden gem, we implemented a yaml-parser that can take care of imports of model classes and postprocessors. It also takes care of instantiating the optuna distributions for hyperparameter optimization.</p> <p>Just use the following yaml tags:</p> <ul> <li><code>!Int</code> for <code>optuna.distributions.IntDistribution</code></li> <li><code>!Float</code> for <code>optuna.distributions.FloatDistribution</code></li> <li><code>!Categorical</code> for <code>optuna.distributions.CategoricalDistribution</code></li> </ul> <p>Note: Don't put commas to end the lines of the distributions in the yaml file. This will break the instantiation of the distributions since the yaml parser will interpret the comma as part of the distribution and cast it to float.</p> <p>Please also note, that the yaml parser does not allow scientific notation at the moment. This is due to the fact that the yaml parser will interpret the scientific notation as a string and not as a float. This may be improved in future versions when pyaml is updating their regex that constructs floats.</p> <p>Use the following syntax to define the distribution:</p> <p><pre><code>!Int\n  low: 5\n  high: 100\n  step: 1\n  log: true\n</code></pre> Note: You have to provide keys for the distribution parameters. Also you have to provide low and high values.  Exceptions are the <code>step</code> parameter for <code>IntDistribution</code> which defaults to 1 and the <code>log</code> parameter which defaults to False.</p> <p>With our yaml configuration we could define models like this:</p> <p><pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\nfrom flexcv import CrossValidation\nfrom flexcv.merf import MERF\nfrom flexcv.model_mapping import ModelConfigDict, ModelMappingDict\nfrom flexcv.models import LinearMixedEffectsModel, LinearModel\n\nyaml_mapping = \"\"\"\nLinearModel:\n  requires_inner_cv: False\n  n_jobs_model: 1\n  n_jobs_cv: 1\n  model: flexcv.models.LinearModel\n  post_processor: flexcv.model_postprocessing.LinearModelPostProcessor\n\nLMER:\n  requires_inner_cv: False\n  n_jobs_model: 1\n  n_jobs_cv: 1\n  model: flexcv.models.LinearMixedEffectsModel\n  post_processor: flexcv.model_postprocessing.LMERModelPostProcessor\n\nRandomForest:\n  requires_inner_cv: true\n  n_trials: 10\n  n_jobs_model: -1\n  n_jobs_cv: 1\n  model: sklearn.ensemble.RandomForestRegressor\n  params:\n    max_depth: !Int\n      low: 5\n      high: 100\n    min_samples_split: !Int\n      low: 2\n      high: 1000\n      log: true\n    min_samples_leaf: !Int\n      low: 2\n      high: 5000\n      log: true\n    max_samples: !Float\n      low: 0.0021\n      high: 0.9\n    max_features: !Int\n      low: 1\n      high: 10\n    max_leaf_nodes: !Int\n      low: 10\n      high: 40000\n    min_impurity_decrease: !Float\n      low: 0.0000000008\n      high: 0.02\n      log: true\n    min_weight_fraction_leaf: !Float\n      low: 0\n      high: 0.5\n    ccp_alpha: !Float\n      low: 0.000008\n      high: 0.01\n    n_estimators: !Int\n      low: 2\n      high: 7000\n  post_processor: flexcv.model_postprocessing.RandomForestModelPostProcessor\n\"\"\"\n\n# and then call .set_models on your CrossValidation instance\ncv = CrossValidation()\ncv.set_models(yaml_string=yaml_mapping)\n</code></pre> In addition to the yaml configuration, you can also pass a path to a yaml file to the <code>.set_models</code> method by using the <code>yaml_path</code> keyword argument.</p> <pre><code>import yaml\nfrom flexcv import CrossValidation\n\nyaml_code = \"\"\"\nLinearModel:\n  requires_inner_cv: False\n  n_jobs_model: 1\n  n_jobs_cv: 1\n  model: flexcv.models.LinearModel\n  post_processor: flexcv.model_postprocessing.LinearModelPostProcessor\n\"\"\"\nwith open(\"my_yaml.yaml\", \"w\") as f:\n    yaml.safe_dump(yaml_code, f)\n\ncv = CrossValidation()\ncv.set_models(yaml_path=\"my_yaml.yaml\")\n</code></pre>"},{"location":"guides/multiple-models/#configuration-using-a-modelmappingdict","title":"Configuration using a ModelMappingDict","text":"<p>Of course, when you want to compare a larger number of models you can assign them to a customized ModelMappingDict directly and pass the mapping directly to the <code>.set_models</code> method.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nimport optuna\n\nfrom flexcv import CrossValidation\nfrom flexcv.merf import MERF\nfrom flexcv.model_mapping import ModelConfigDict, ModelMappingDict\nfrom flexcv.models import LinearMixedEffectsModel, LinearModel\nimport flexcv.model_postprocessing as mp\n\nmodel_map = ModelMappingDict(\n    {\n        \"LinearModel\": ModelConfigDict(\n            {\n                \"model\": LinearModel,\n                \"post_processor\": mp.LinearModelPostProcessor,\n                \"requires_inner_cv\": False,\n            }\n            ),\n        \"LinearMixedEffectsModel\": ModelConfigDict(\n            {\n                \"model\": LinearMixedEffectsModel,\n                \"post_processor\": mp.LMERModelPostProcessor,\n                \"requires_inner_cv\": False,\n            }\n        ),\n        \"RandomForest\": ModelConfigDict(\n            {\n                \"model\": RandomForestRegressor,\n                \"params\": {\n                    \"max_depth\": optuna.distributions.IntDistribution(5,100),\n                    \"n_estimators\": optuna.distributions.CategoricalDistribution(\n                        [10]\n                    ),\n                \"post_processor\": mp.RandomForestModelPostProcessor,\n                \"requires_inner_cv\": True,\n                },\n            }\n        )\n    }\n)\n\n# and then call .set_models on your CrossValidation instance\ncv = CrossValidation()\ncv.set_models(model_map)\n</code></pre> <p>In this guide you learned several ways to set up your models for cases where you want to compare multiple models in the same run. You have seen how to use the <code>add_model()</code> method, how to use yaml configuration and how to use a <code>ModelMappingDict</code> to set up your models. You can use all of these methods in combination to set up your models and to fully customize your cross validation setup. This makes it easy to compare multiple models on your data and to find the best model for your use case. A big help is the neptune integration that we provide. You can find a more detailled guide on how to use it here.</p>"},{"location":"guides/neptune-integration/","title":"Neptune Integration","text":""},{"location":"guides/neptune-integration/#neptune-integration","title":"Neptune Integration","text":"<p>Neptune is a cutting edge MLOps framework that allows you to track your experiments and models in a reproducible way. `flexcv`` deeply integrates neptune in order to track experiments all along the nested cross validation pipelines. You can find more information about Neptune here. We highly recommend registering for a free account and trying it out. You can find our neptune project that we use for testing here.</p>"},{"location":"guides/neptune-integration/#what-you-get","title":"What you get","text":"<p>Let's start with an overview of what we log and how it looks before diving in the specifics how to set up neptune in <code>flexcv</code>. The <code>set_run</code> method will automatically create a new experiment in your neptune project and log the following information:</p> <ul> <li>The whole configuration of the <code>CrossValidation</code> instance<ul> <li>The configuration of each model</li> <li>The split methods, the number of folds, the number of trials, etc.</li> <li>Scaling methods</li> <li>...</li> </ul> </li> <li>The results of each model evaluated with every metric defined in the metrics dict of the <code>CrossValidation</code> instance</li> <li>Every model instance from the outer folds</li> <li>The best parameters of each inner cross validation</li> <li>Plots of the hyperparameter tuning</li> <li>Plots generated by the post processors (SHAP beeswarm plots etc.)</li> <li>Diagnostics plots of the cross validation</li> <li>The data used for the cross validation</li> </ul>"},{"location":"guides/neptune-integration/#configuration","title":"Configuration","text":"<p>In neptune you can easily inspect your configuration and results. Here is an example: </p>"},{"location":"guides/neptune-integration/#evaluation","title":"Evaluation","text":"<p>Neptune let's you keep track of all of your metrics: </p>"},{"location":"guides/neptune-integration/#feature-importance","title":"Feature Importance","text":"<p>Here is an example of a beeswarm plot generated by the <code>RandomForestModelPostProcessor</code> to explore the feature importances of the random forest model: </p>"},{"location":"guides/neptune-integration/#training-statistics","title":"Training Statistics","text":"<p>And here is an example of a plot of the training statistics for the MERF class:</p> <p></p> <p>Learn more about working with MERF models.</p>"},{"location":"guides/neptune-integration/#optuna-hyperparameter-tuning","title":"Optuna Hyperparameter Tuning","text":"<p>Thanks to the integration of optuna, you can also inspect the hyperparameter tuning process. The parallel coordinate plot is one possibility to visualize the hyperparameter tuning process and uunderstand the influence of each hyperparameter on the performance of the model.</p> <p></p>"},{"location":"guides/neptune-integration/#setting-up-neptune","title":"Setting up Neptune","text":"<p>The neptune python package is installed as a default requirement of this package. After registering for a free account, you can create a new project and copy the project name and API token to your clipboard. See the Neptune documentation for more information on how to set an environment variable for your API token. This is the recommended way of setting the API token, since it will be automatically loaded by the neptune package and will not be visible in your code.</p>"},{"location":"guides/neptune-integration/#neptune-in-flexcv","title":"Neptune in <code>flexcv</code>","text":"<p>To track your experiment in neptune, you create an neptune <code>run</code>object and pass it to the <code>set_run</code> method of the <code>CrossValidation</code> class.</p> <pre><code>import neptune\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\n\n\nX, y, _, _ =generate_regression(\n    3, 25, n_slopes=1, noise_level=9.1e-2\n)\n\nyaml_config = \"\"\"\nRandomForest:\n  model: sklearn.ensemble.RandomForestRegressor\n  post_processor: flexcv.model_postprocessing.RandomForestModelPostProcessor\n  requires_inner_cv: false\n\"\"\"\n\n# create neptune run object and pass your project name\nrun = neptune.init_run(project=\"radlfabs/flexcv-testing\")\n\n# set up the CrossValidation instance\ncv = CrossValidation()\nresults = (\n    cv.set_data(X, y)\n    # pass the run object to the CrossValidation instance and set flag for additional plots\n    .set_run(run, diagnostics=True)\n    .set_splits(n_splits_out=3)\n    #... do some configuration\n    .set_models(yaml_string=yaml_config)\n    .perform()\n    .get_results()\n)\nrun.stop()\n\n# it's good practice to stop the run after you are done\nrun.stop()\n</code></pre>"},{"location":"guides/neptune-integration/#fitting-an-xgboost-regressor-with-neptune-integration","title":"Fitting an XGBoost Regressor with Neptune Integration","text":"<p>This example shows how to fit an XGBoost Regressor and visualize the training and evaluation with the Neptune-XGBoost integration. We are using the <code>generate_regression</code> function from the <code>flexcv.synthesizer</code> module to generate a regression dataset with 3 features and 1000 samples. We are fitting a <code>XGBRegressor</code> using the <code>XGBNeptuneCallback</code> callback from the Neptune integration. The callback is initialized with the Neptune run and a base namespace which is used to organize the Neptune channels. We pass everything to the <code>CrossValidation</code> class and call the <code>perform</code> method to start the cross validation. For additional logging we still use a custom PostProcessor which is passed to the <code>CrossValidation</code> class. In it we generate some nice-looking beeswarm shap-plots that help us further undestand the model. </p> <p><pre><code>from xgboost import XGBRegressor\nimport neptune\nfrom neptune.integrations.xgboost import NeptuneCallback as XGBNeptuneCallback\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\n\nX, y, _, _ = generate_regression(3, 25)\n\n# init a neptune run in our project\nrun = neptune.init_run(project=\"radlfabs/flexcv-testing\")\n\n# instantiate our xgboost model\nmodel = XGBRegressor\n\n# initialize the callback with the neptune run and a base namespace\n# plots, metrics and parameters that are logged by the callback will be organized in this namespace\ncallback = XGBNeptuneCallback(\n    run=run, \n    base_namespace=f\"XGBRegressor/Callback\", \n)\n\n_ = (\n    CrossValidation()\n    .set_run(run, diagnostics=True)\n    .set_data(X, y)\n    .set_splits(n_splits_out=3, break_cross_val=True)\n    # add the callback to the model, remember, that callbacks expects a list of callbacks\n    # that way you can easily add multiple callbacks to the model (e.g. for early stopping)\n    .add_model(model, callbacks=[callback])  \n    .perform()\n)\n\nrun.stop()\n</code></pre> Now let's take a look at the Neptune dashboard. In just a couple of lines of code we have logged a lot of information.</p> <p>Here is the plot for the shap values: </p> <p>The regression summary from the neptune-sklearn integration is doing a great job at offering plots for model fit: </p> <p>In this guide you learned how to leverage the Neptune integration to log your experiments and models in a reproducible way. The <code>flexcv</code> integration with neptune is a great way to benefit from experiment tracking, especially when configuration gets more complex, you are dealing with multiple models of different types or you are using nested cross validation. Then it is very easy to lose track of your experiments and models. Neptune helps you to keep track of everything and offers a great dashboard to explore your results.</p>"},{"location":"guides/random-effects/","title":"Model Random Effects","text":""},{"location":"guides/random-effects/#working-with-mixed-effects-models","title":"Working with Mixed Effects Models","text":"<p>To this point, we used regression models to make predictions based on the features in the data. We used basic kFold cross validation [1] to tune the hyperparameters of our random forest regressor and [2] estimate model generalization.</p> <p>However, sometimes the data is not only structured by the feature matrix and the target values but also defined by a data hierarchy. This is common in study data that has been generated by participants. Mixed effects models are a type of statistical model used to analyze hierarchical data, where the data is organized into groups or clusters. In mixed effects models, both fixed and random effects are included in the model to account for the variation within and between the groups. The fixed effects are the same for all groups, while the random effects vary between the groups. Mixed effects models are useful when the data has a nested or hierarchical structure, such as in longitudinal studies, where measurements are taken over time on the same individuals, or in clustered data, where individuals are grouped together based on some characteristic. Mixed effects models can provide more accurate estimates of the effects of predictors and can account for the correlation between observations within the same group.</p>"},{"location":"guides/random-effects/#choosing-a-split","title":"Choosing a split","text":"<p>In order to respect the hierarchical structure of the data one has to choose an appropriate method for the cross validation splits.</p> <p>The choice of split method in mixed effects models can have a significant impact on the model performance and the interpretation of the results. In hierarchical data, the observations are often clustered or grouped together, and the choice of split method can affect how the data is partitioned into training and validation sets. For example, if the data is clustered by geographic region, a split method that takes into account the clustering structure, such as the <code>GroupKFold</code> or <code>StratifiedGroupKFold</code> methods, may be more appropriate than a method that ignores the clustering structure, such as the <code>KFold</code> method. The choice of split method can also affect the estimation of the model parameters and the prediction accuracy of the model. Therefore, it's important to carefully consider the choice of split method when fitting mixed effects models to hierarchical data.</p> <p><code>flexcv</code> makes chosing and switching between split methods straight forward. You can simply exchange methods from the method calls.</p> <pre><code>from flexcv import CrossValidation\ncv = CrossValidation().set_splits(split_out=\"GroupKFold\",split_in=\"GroupKFold\")\n</code></pre> <p><code>flexcv</code> solves the problems, where different splitting methods would not share signatures, internally, so you can concentrate on the research questions and do not have to jump into implementation.</p>"},{"location":"guides/random-effects/#stratification","title":"Stratification","text":"<p>In addition to that, we have to talk about stratification. Stratifying the grouping variable guarantees that both training and validation sets include a well-distributed sample of the groups or clusters within the data. This step is instrumental in reducing the likelihood of the model overfitting to specific groups or clusters, ultimately enhancing the model's generalization performance.</p> <p>Similarly, stratifying the target variable ensures that the training and validation sets encompass a representative sample of target variable values. By doing so, it minimizes the risk of overfitting to particular ranges of target variable values, contributing to improved generalization performance. Usually this is the go-to way for multiclass problems. We implemented stratification for continuous targets by performing stratification based on a discretized copy of the continuous target. The data is discretized into percentiles and the distribution of the percentiles is preserved. The folding is, of course, taken out on the continuous target variable again.</p> <p>Furthermore, our adapted stratification approach for continuous target variables ensures the inclusion of representative target variable values in both training and validation sets, even when the target variable is continuous. This refinement significantly enhances the model's generalization performance and mitigates the risk of overfitting to specific target variable value ranges.</p> <p>In summary, stratifying both the grouping variable and the target variable is crucial for boosting the generalization performance of mixed effects models and reducing the risk of overfitting, especially when dealing with small sample sizes.</p>"},{"location":"guides/random-effects/#choosing-a-model-type","title":"Choosing a model type","text":"<p>For mixed effects modeling, <code>flexcv</code> offers two great possibilities:</p> <ol> <li>Linear Mixed Effects Models (LMM)</li> <li>Mixed Effects for Random Forests (MERF)</li> </ol> <p>A linear mixed effects model, often abbreviated as LMM or just mixed model, is a statistical approach used to analyze data that exhibit both fixed and random effects. It is particularly valuable in situations where the data involves nested or hierarchical structures, repeated measures, or other forms of dependency.</p> <p>The random effects capture the random variability or \"nuisance\" components in your data. Random effects are used to account for correlations and variations that may be specific to particular groups or clusters within your data. These effects are often assumed to follow a normal distribution.</p> <p>The primary advantage of a linear mixed effects model is its ability to model and account for the variability within the data due to both fixed and random effects. This allows for more accurate and efficient modeling in cases where traditional linear regression may not be appropriate, such as when dealing with repeated measurements on the same subjects or when the data has a hierarchical structure.</p> <p>Linear mixed effects models are widely used in various fields, including biology, psychology, social sciences, and economics, to address complex data analysis problems. They provide a flexible and powerful framework for understanding the underlying structure of data and making meaningful inferences.</p> <p><code>flexcv</code> provides a wrapper class to use LMM inside a nested cross validation pipeline alongside with other machine learning models.</p> <p>Here is an example how you would apply an LMM to grouped sample data.</p> <p>In our implementation, we think of the model names passed as keys in the <code>ModelMappingDict</code> as referring to base estimators, i. e. fixed effects models. Mixed effects models often make use of base estimators. Therefore, we just append the LMM to the linear model's configuration.</p> <pre><code>from flexcv import CrossValidation\nfrom flexcv.models import LinearModel, LinearMixedEffectsModel\nfrom flexcv.synthesizer import generate_regression\n\nX, y, group, random_slopes =generate_regression(\n    3,100,n_slopes=1,noise_level=9.1e-2\n)\n\ncv =CrossValidation()\nresults = (\n    cv.set_data(X, y, group, random_slopes)\n    .set_splits(n_splits_out=3)\n    .add_model(LinearMixedEffectsModel)\n    .perform()\n    .get_results()\n)\n</code></pre>"},{"location":"guides/random-effects/#merf","title":"MERF","text":"<p>The MERF class can be used to optimize any base estimator for mixed effects utilizing the expectation maximization (EM) algorithm. In the cross validation process, the base estimator is passed to MERF after hyperparameter tuning. There, a new instantance is created and fit to the data using the EM algorithm. MERF is added to every estimator (i.e. model) in the model mapping of the <code>CrossValidation</code> class where <code>add_merf</code> is set to <code>True</code>. This is can be done globally for every model that is passed to the <code>CrossValidation</code> class instance by setting it to True in the <code>set_merf()</code> method. You can also set <code>add_merf</code> for every model in the model mapping individually. In the following example we will use a random forest regressor as base estimator for MERF and use the global setting to add MERF to every model in the model mapping. In the <code>CrossValidationResults</code> object that is returned by <code>get_results()</code> the MERF instance is always named with the scheme \"MERF(BaseEstimatorName)\". In this case, the MERF instance is named \"MERF(RandomForestRegressor)\".</p> <p><pre><code>import optuna\nfrom sklearn.ensemble import RandomForestRegressor\nfrom flexcv import CrossValidation\nfrom flexcv.merf import MERF\nfrom flexcv.model_postprocessing import RandomForestModelPostProcessor\nfrom flexcv.synthesizer import generate_regression\n\n\n# lets start with generating some clustered data\nX, y, group, random_slopes =generate_regression(\n    3, 50, n_slopes=1, noise_level=9.1e-2\n)\n# define our hyperparameters\nparams = {\n    \"max_depth\": optuna.distributions.IntDistribution(5, 100),\n    \"n_estimators\": optuna.distributions.CategoricalDistribution([10]),\n}\n\ncv =CrossValidation()\nresults = (\n    cv.set_data(X, y, group, random_slopes)\n    .set_inner_cv(3)\n    .set_splits(n_splits_out=3)\n    .add_model(model_class=RandomForestRegressor, requires_inner_cv=True, params=params, post_processor=RandomForestModelPostProcessor)\n    .set_merf(True, em_max_iterations=5)\n    .perform()\n    .get_results()\n)\n</code></pre> Our neptune integration automatically logs training statistics and plots for every MERF model you fit to the data. Learn more about the neptune integration here.</p> <p></p> <p>Neptune makes it easy to keep track of the training process and observe convergence of the expectation maximization algorithm that is used in MERF.</p>"},{"location":"guides/repeated-guide/","title":"Repeated Cross Validation","text":""},{"location":"guides/repeated-guide/#repeated-cross-validation","title":"Repeated Cross Validation","text":"<p>Some of the cross validation splits are performed with shuffling the data before dividing in train and test splits. Therefore, you might wonder if your evaluation varies for multiple runs.</p> <p>In the standard configuration, you would seed every run to make it absolutely reproducible. Now we want to explore, how different seeds influence the cross validation results. This is call repeated cross validation. We can still seed this process though by randomly generating a number of seeds. This makes even the repeated CV reproducible.</p> <p>First, we create our random data set and a basic model mapping just as in a single run.</p> <p>Second, we instantiate a <code>RepeatedCV</code> object. This class not only has the <code>set</code>-methods just as CrossValidation but also implements <code>set_n_repeats()</code> and <code>set_neptune()</code>. We can chain these methods because they also return the class <code>self</code> and we use them to set the number of repetitions as well as passing the credentials for Neptune runs. <code>RepeatedCV</code> then takes care of instantiating the desired number of runs and logs every single cross validation to it's own neptune run.</p> <p>Most importantly <code>RepeatedCV</code> implements the iteration over single cross validation runs in it's <code>perform()</code> method. We can chain <code>perform()</code> in the same manner as we are now used to. The last element of our chain should also be <code>get_results</code>. This will allow us to inspect summary statistics as a measure of variance in the runs.</p> <p>Here is the full code to perform cross validation 3 times and get summary statistics for all folds and models.</p> <pre><code>from flexcv.synthesizer import generate_regression\nfrom flexcv.models import LinearModel\nfrom flexcv.model_postprocessing import LinearModelPostProcessor\nfrom flexcv.repeated import RepeatedCV\n\n# make sample data\nX, y, _, _ = generate_regression(3,25,n_slopes=1,noise_level=9.1e-2, random_seed=42)\n\nrcv = (\n    RepeatedCV()\n    .add_model(model_class=LinearModel, post_processor=LinearModelPostProcessor)\n    .set_data(X, y, dataset_name=\"ExampleData\")\n    .set_splits(n_splits_out=3)\n    .set_n_repeats(3)\n    .perform()\n    .get_results()\n)\n\nrcv.summary.to_excel(\"repeated_cv.xlsx\")  # save dataframe to excel file\n</code></pre>"},{"location":"guides/rf-regressor/","title":"Fit a Random Forest","text":""},{"location":"guides/rf-regressor/#fit-a-random-forest-regressor","title":"Fit a Random Forest Regressor","text":"<p>In this section, we will use a Random Forest Regressor to predict a target variable. We use cross validation to estimate the regressor's ability to generalize on unseen data. Also, we want to tune a single hyperparameter, the max_depth of the trees, in the inner cross validation and evaluate the best estimator's performance in the outer cross validation loop. We will use the randomly generated dataset just as in the example in our Getting-Started guide.</p> <p>Note how we use Optuna distributions to specify the hyperparameter search space. In the same syntax, you can add all kinds of hyperparameter distributions to the params dictionary in the mapping.</p> <p>Also, we use a model post processing function to extract the feature importances and plot them as SHAP beeswarm plots in Neptune. This is a very powerful feature of <code>flexcv</code> and you can use it to implement any kind of post processing you want. You can also use it to log additional metrics or results to Neptune.</p> <pre><code>import optuna\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\n\n# lets start with generating some clustered data\nX, y, group, random_slopes = generate_regression(\n    3, 100, n_slopes=1 ,noise_level=9.1e-2\n)\n\n# define a set of hyperparameters to tune\nparams = {\n    \"max_depth\": optuna.distributions.IntDistribution(5, 100),\n    \"n_estimators\": optuna.distributions.CategoricalDistribution([10]),\n}\n\n# instantiate the CrossValidation class\ncv =CrossValidation()\n\n# pass everything to CrossValidation using method chaining\nresults = (\n    cv.set_data(X, y)\n    .add_model(\n        model_class=RandomForestRegressor,\n        requires_inner_cv=True,\n        params=params,\n    )\n    .set_inner_cv(3)\n    .set_splits(n_splits_out=3)\n    .perform()\n    .get_results()\n)\n\n# Print the averaged R\u00b2\nn_values =len(results[\"RandomForestRegressor\"][\"metrics\"])\nr2_values =[results[\"RandomForestRegressor\"][\"metrics\"][k][\"r2\"] for k in range(n_values)]\nprint(np.mean(r2_values))\n# save the results table to an excel file\nresults.summary.to_excel(\"results.xlsx\")\n</code></pre>"},{"location":"reference/core/","title":"Core Function","text":""},{"location":"reference/core/#flexcv.core","title":"<code>flexcv.core</code>","text":""},{"location":"reference/core/#flexcv.core.cross_validate","title":"<code>flexcv.core.cross_validate(*, X, y, target_name, run, groups, slopes, split_out, split_in, break_cross_val, scale_in, scale_out, n_splits_out, n_splits_in, random_seed, model_effects, mapping, metrics, objective_scorer, em_max_iterations=None, em_stopping_threshold=None, em_stopping_window=None, predict_known_groups_lmm=True, diagnostics=False, **kwargs)</code>","text":"<p>This function performs a cross-validation for a given regression formula, using one or a number of specified machine learning models and a configurable cross-validation method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Features.</p> required <code>y</code> <code>Series</code> <p>Target.</p> required <code>target_name</code> <code>str</code> <p>Custom target name.</p> required <code>run</code> <code>Run</code> <p>A Run object to log to.</p> required <code>groups</code> <code>Series</code> <p>The grouping or clustering variable.</p> required <code>slopes</code> <code>DataFrame | Series</code> <p>Random slopes variable(s)</p> required <code>split_out</code> <code>CrossValMethod | BaseCross</code> <p>Outer split strategy.</p> required <code>split_in</code> <code>CrossValMethod</code> <p>Inner split strategy.</p> required <code>break_cross_val</code> <code>bool</code> <p>If True, only the first outer fold is evaluated.</p> required <code>scale_in</code> <code>bool</code> <p>If True, the features are scaled in the inner cross-validation to zero mean and unit variance. This works independently of the outer scaling.</p> required <code>scale_out</code> <code>bool</code> <p>If True, the features are scaled in the outer cross-validation to zero mean and unit variance.</p> required <code>n_splits_out</code> <code>int</code> <p>Number of outer cross-validation folds.</p> required <code>n_splits_in</code> <code>int</code> <p>Number of inner cross-validation folds.</p> required <code>random_seed</code> <code>int</code> <p>Seed for all random number generators.</p> required <code>model_effects</code> <code>str</code> <p>If \"fixed\", only fixed effects are used. If \"mixed\", both fixed and random effects are used.</p> required <code>mapping</code> <code>ModelMappingDict</code> <p>The mapping providing model instances, hyperparameter distributions, and postprocessing functions.</p> required <code>metrics</code> <code>MetricsDict</code> <p>A dict of metrics to be used as the evaluation metric for the outer cross-validation.</p> required <code>objective_scorer</code> <code>ObjectiveScorer</code> <p>A custom objective scorer object to provide the evaluation metric for the inner cross-validation.</p> required <code>em_max_iterations</code> <code>int</code> <p>For use with MERF. Maximum number of iterations for the EM algorithm. (Default: None)</p> <code>None</code> <code>em_stopping_threshold</code> <code>float</code> <p>For use with MERF. Threshold for the early stopping criterion of the EM algorithm. (Default: None)</p> <code>None</code> <code>em_stopping_window</code> <code>int</code> <p>For use with MERF. Window size for the early stopping criterion of the EM algorithm. (Default: None)</p> <code>None</code> <code>predict_known_groups_lmm</code> <code>bool</code> <p>For use with Mixed Linear Models. If True, the model will predict the known groups in the test set. (Default: True)</p> <code>True</code> <code>diagnostics</code> <code>bool</code> <p>If True, diagnostics plots are logged to Neptune. (Default: False)</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, list]]</code> <p>Dict[str, Dict[str, list]]: A dictionary containing the results of the cross-validation, organized by machine learning models.</p> <p>The function returns a nested dictionary with the following structure: <pre><code>results_all_folds = {\n                    model_name: {\n                        \"model\": [],\n                        \"parameters\": [],\n                        \"results\": [],\n                        \"r2\": [],\n                        \"y_pred\": [],\n                        \"y_test\": [],\n                        \"shap_values\": [],\n                        \"median_index\": [],\n                    }\n</code></pre></p> Source code in <code>flexcv/core.py</code> <pre><code>def cross_validate(\n    *,\n    X: pd.DataFrame,\n    y: pd.Series,\n    target_name: str,\n    run: NeptuneRun,\n    groups: pd.Series,\n    slopes: pd.DataFrame | pd.Series,\n    split_out: CrossValMethod | BaseCrossValidator | Iterator,\n    split_in: CrossValMethod | BaseCrossValidator | Iterator,\n    break_cross_val: bool,\n    scale_in: bool,\n    scale_out: bool,\n    n_splits_out: int,\n    n_splits_in: int,\n    random_seed: int,\n    model_effects: str,\n    mapping: ModelMappingDict,\n    metrics: MetricsDict,\n    objective_scorer: ObjectiveScorer,\n    em_max_iterations: int = None,\n    em_stopping_threshold: float = None,\n    em_stopping_window: int = None,\n    predict_known_groups_lmm: bool = True,\n    diagnostics: bool = False,\n    **kwargs,\n) -&gt; Dict[str, Dict[str, list]]:\n    \"\"\"This function performs a cross-validation for a given regression formula, using one or a number of specified machine learning models and a configurable cross-validation method.\n\n    Args:\n        X (pd.DataFrame): Features.\n        y (pd.Series): Target.\n        target_name (str): Custom target name.\n        run (NeptuneRun): A Run object to log to.\n        groups (pd.Series): The grouping or clustering variable.\n        slopes (pd.DataFrame | pd.Series): Random slopes variable(s)\n        split_out (CrossValMethod | BaseCross): Outer split strategy.\n        split_in (CrossValMethod): Inner split strategy.\n        break_cross_val (bool): If True, only the first outer fold is evaluated.\n        scale_in (bool): If True, the features are scaled in the inner cross-validation to zero mean and unit variance. This works independently of the outer scaling.\n        scale_out (bool): If True, the features are scaled in the outer cross-validation to zero mean and unit variance.\n        n_splits_out (int): Number of outer cross-validation folds.\n        n_splits_in (int): Number of inner cross-validation folds.\n        random_seed (int): Seed for all random number generators.\n        model_effects (str): If \"fixed\", only fixed effects are used. If \"mixed\", both fixed and random effects are used.\n        mapping (ModelMappingDict): The mapping providing model instances, hyperparameter distributions, and postprocessing functions.\n        metrics (MetricsDict): A dict of metrics to be used as the evaluation metric for the outer cross-validation.\n        objective_scorer (ObjectiveScorer): A custom objective scorer object to provide the evaluation metric for the inner cross-validation.\n        em_max_iterations (int): For use with MERF. Maximum number of iterations for the EM algorithm. (Default: None)\n        em_stopping_threshold (float): For use with MERF. Threshold for the early stopping criterion of the EM algorithm. (Default: None)\n        em_stopping_window (int): For use with MERF. Window size for the early stopping criterion of the EM algorithm. (Default: None)\n        predict_known_groups_lmm (bool): For use with Mixed Linear Models. If True, the model will predict the known groups in the test set. (Default: True)\n        diagnostics (bool): If True, diagnostics plots are logged to Neptune. (Default: False)\n        **kwargs: Additional keyword arguments.\n\n\n    Returns:\n      Dict[str, Dict[str, list]]: A dictionary containing the results of the cross-validation, organized by machine learning models.\n\n    The function returns a nested dictionary with the following structure:\n    ```python\n    results_all_folds = {\n                        model_name: {\n                            \"model\": [],\n                            \"parameters\": [],\n                            \"results\": [],\n                            \"r2\": [],\n                            \"y_pred\": [],\n                            \"y_test\": [],\n                            \"shap_values\": [],\n                            \"median_index\": [],\n                        }\n    ```\n\n    \"\"\"\n\n    if objective_scorer is None:\n        objective_scorer = ObjectiveScorer(mse_wrapper)\n    else:\n        objective_scorer = ObjectiveScorer(objective_scorer)\n\n    try:\n        npt_handler = NeptuneHandler(run=run)\n        logger.addHandler(npt_handler)\n    except TypeError:\n        logger.warning(\n            \"No Neptune run object passed. Logging to Neptune will be disabled.\"\n        )\n\n    print()\n    re_formula = get_re_formula(slopes)\n    formula = get_fixed_effects_formula(target_name, X)\n    cross_val_split_out = make_cross_val_split(\n        method=split_out, groups=groups, n_splits=n_splits_out, random_state=random_seed  # type: ignore\n    )\n\n    model_keys = list(mapping.keys())\n    for model_name, inner_dict in mapping.items():\n        if \"add_merf\" in inner_dict and inner_dict[\"add_merf\"]:\n            merf_name = f\"{model_name}_MERF\"\n            model_keys.append(merf_name)\n\n    results_all_folds = {}\n\n    ######### OUTER FOLD LOOP #########\n    for k, (train_index, test_index) in enumerate(\n        tqdm(\n            cross_val_split_out(X=X, y=y),\n            total=n_splits_out,\n            desc=\" cv\",\n            position=0,\n            leave=False,\n        )\n    ):\n        print()  # for beautiful tqdm progressbar\n\n        groups_exist = groups is not None\n        slopes_exist = slopes is not None\n\n        # check if break is requested and if this is the 2nd outer fold\n        if break_cross_val and k == 1:\n            break\n\n        # Assign the outer folds data\n        X_train = X.iloc[train_index]\n        y_train = y.iloc[train_index]  # type: ignore\n\n        X_test = X.iloc[test_index]\n        y_test = y.iloc[test_index]  # type: ignore\n\n        cluster_train = groups.iloc[train_index] if groups_exist else None  # type: ignore\n        cluster_test = groups.iloc[test_index] if groups_exist else None  # type: ignore\n\n        if scale_out:\n            X_train_scaled, X_test_scaled = preprocess_features(X_train, X_test)\n\n        else:\n            X_train_scaled = X_train\n            X_test_scaled = X_test\n\n        if slopes_exist:\n            Z_train, Z_test = preprocess_slopes(\n                Z_train_slope=slopes.iloc[train_index],\n                Z_test_slope=slopes.iloc[test_index],\n                must_scale=scale_out,\n            )\n\n        else:\n            Z_train = np.ones((len(X_train), 1))  # type: ignore\n            Z_test = np.ones((len(X_test), 1))  # type: ignore\n\n        # run diagnostics if requested\n        if diagnostics:\n            to_diagnose = (\n                {\n                    \"effects\": model_effects,\n                    \"cluster_train\": cluster_train,\n                    \"cluster_test\": cluster_test,\n                }\n                if model_effects == \"mixed\"\n                else {\"effects\": model_effects}\n            )\n\n            log_diagnostics(\n                X_train_scaled, X_test_scaled, y_train, y_test, run, **to_diagnose\n            )\n\n        # Loop over all models\n        for model_name in mapping.keys():\n            logger.info(f\"Evaluating {model_name}...\")\n\n            skip_inner_cv = not mapping[model_name][\"requires_inner_cv\"]\n\n            model_class = mapping[model_name][\"model\"]\n\n            try:\n                model_kwargs = mapping[model_name][\"model_kwargs\"]\n            except KeyError as e:\n                raise KeyError(\n                    f\"No model_kwargs passed for {model_name}. Check your configuration.\"\n                ) from e\n\n            fit_kwargs = mapping[model_name][\"fit_kwargs\"]\n            evaluate_merf = mapping[model_name][\"add_merf\"]\n            param_grid = mapping[model_name][\"params\"]\n            n_trials = mapping[model_name][\"n_trials\"]\n            requires_formula = mapping[model_name][\"requires_formula\"]\n\n            if requires_formula:\n                fit_kwargs[\"formula\"] = formula\n                fit_kwargs[\"re_formula\"] = re_formula\n\n            # build inner cv folds\n            cross_val_split_in = make_cross_val_split(\n                method=split_in, groups=cluster_train, n_splits=n_splits_in, random_state=random_seed  # type: ignore\n            )\n\n            if skip_inner_cv:\n                # Instantiate model directly without inner cross-validation\n                # has to be best_model because it is the only one instantiated\n                best_model = model_class(**model_kwargs)\n                best_params = best_model.get_params()\n                # set study to None since no study is instantiated otherwise\n                study = None\n\n            else:\n                # this block performs the inner cross-validation with Optuna\n\n                n_trials = mapping[model_name][\"n_trials\"]\n                n_jobs_cv_int = mapping[model_name][\"n_jobs_cv\"]\n\n                pipe_in = Pipeline(\n                    [\n                        (\"scaler\", StandardScaler()) if scale_in else (),\n                        (\n                            \"model\",\n                            model_class(**model_kwargs),\n                        ),\n                    ]\n                )\n\n                # add \"model__\" to all keys of the param_grid\n                param_grid = add_model_to_keys(param_grid)\n\n                neptune_callback = CustomNeptuneCallback(\n                    run[f\"{model_name}/Optuna/{k}\"],\n                    # reduce load on neptune, i.e., reduce number of items and plots\n                    log_plot_contour=False,\n                    log_plot_optimization_history=True,\n                    log_plot_edf=False,\n                    study_update_freq=10,  # log every 10th trial,\n                )\n\n                # generate numpy random_state object for seeding the sampler\n                random_state = check_random_state(random_seed)\n                sampler_seed = random_state.randint(0, np.iinfo(\"int32\").max)\n\n                # get sampler to be used for the inner cross-validation\n                sampler = optuna.samplers.TPESampler(seed=sampler_seed)\n\n                # instantiate the study object\n                study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n\n                # run the inner cross-validation\n                study.optimize(\n                    lambda trial: objective_cv(\n                        trial,\n                        cross_val_split=cross_val_split_in,\n                        pipe=pipe_in,\n                        params=param_grid,\n                        X=X_train,\n                        y=y_train,\n                        run=run,\n                        n_jobs=n_jobs_cv_int,\n                        objective_scorer=objective_scorer,\n                    ),\n                    n_jobs=1,\n                    n_trials=n_trials,\n                    callbacks=[neptune_callback],\n                )\n                # get best params from study and rm \"model__\" from keys\n                best_params = rm_model_from_keys(study.best_params)\n\n            # add random_state to best_params if it is not already in there\n            if \"random_state\" not in best_params and \"random_state\" in model_kwargs:\n                best_params.update({\"random_state\": random_seed})\n\n            # add formula to dict if it is required by the model type\n            # to_dict can be unpacked in the fit method\n\n            train_pred_kwargs = {}\n            test_pred_kwargs = {}\n            pred_kwargs = {}\n\n            if mapping[model_name][\"consumes_clusters\"]:\n                fit_kwargs[\"clusters\"] = cluster_train\n                fit_kwargs[\"re_formula\"] = re_formula\n                pred_kwargs[\"predict_known_groups_lmm\"] = predict_known_groups_lmm\n\n                test_pred_kwargs[\"clusters\"] = cluster_test\n                test_pred_kwargs[\"Z\"] = Z_test\n\n                train_pred_kwargs[\"clusters\"] = cluster_train\n                train_pred_kwargs[\"Z\"] = Z_train\n\n            # make new instance of the model with the best parameters\n            best_model = model_class(\n                **handle_duplicate_kwargs(model_kwargs, best_params)\n            )\n\n            if \"callbacks\" not in mapping[model_name]:\n                logger.info(f\"No callbacks passed for {model_name}. Moving on...\")\n            elif hasattr(best_model, \"callbacks\") and hasattr(best_model, \"set_params\"):\n                best_model.set_params(**mapping[model_name][\"callbacks\"])\n            else:\n                logger.info(f\"Callbacks not supported by {model_name}. Moving on...\")\n\n            # Fit the best model on the outer fold\n            fit_result = best_model.fit(\n                X=X_train_scaled, y=y_train, **handle_duplicate_kwargs(fit_kwargs)\n            )\n            # get test predictions\n            y_pred = best_model.predict(\n                X=X_test_scaled,\n                **handle_duplicate_kwargs(pred_kwargs, test_pred_kwargs),\n            )\n            # get training predictions\n            y_pred_train = best_model.predict(\n                X=X_train_scaled,\n                **handle_duplicate_kwargs(pred_kwargs, train_pred_kwargs),\n            )\n\n            # store the results of the outer fold of the current model in a dataclass\n            # this makes passing to the postprocessor easier\n            model_data = SingleModelFoldResult(\n                k=k,\n                model_name=model_name,\n                best_model=best_model,\n                best_params=best_params,\n                y_pred=y_pred,\n                y_test=y_test,\n                X_test=X_test_scaled,\n                y_pred_train=y_pred_train,\n                y_train=y_train,\n                X_train=X_train_scaled,\n                fit_result=fit_result,\n                fit_kwargs=fit_kwargs,\n            )\n            all_models_dict = model_data.make_results(\n                run=run,\n                results_all_folds=results_all_folds,\n                study=study,\n                metrics=metrics,\n            )\n\n            try:\n                # call model postprocessing on the single results dataclass\n                postprocessor = mapping[model_name][\"post_processor\"]()\n                all_models_dict = postprocessor(\n                    results_all_folds=results_all_folds,\n                    fold_result=model_data,\n                    run=run,\n                    features=X_train.columns,\n                )\n            except (KeyError, TypeError):\n                logger.info(f\"No postprocessor passed for {model_name}. Moving on...\")\n\n            if evaluate_merf:\n                ###### MERF EVALUATION #################\n                # The base model is passed to the MERF class for evaluation in Expectation Maximization (EM) algorithm\n\n                merf_name = f\"MERF({model_name})\"\n                logger.info(f\"Evaluating {merf_name}...\")\n\n                # tag the base prediction\n                y_pred_base = y_pred.copy()\n\n                # instantiate the mixed model with the best fixed effects model\n                merf = MERF(\n                    fixed_effects_model=mapping[model_name][\"model\"](**best_params),\n                    max_iterations=em_max_iterations,\n                    gll_early_stop_threshold=em_stopping_threshold,\n                    gll_early_stopping_window=em_stopping_window,\n                    log_gll_per_iteration=False,\n                )\n                # fit the mixed model using cluster variable and Z for slopes\n                fit_result = merf.fit(\n                    X=X_train_scaled,\n                    y=y_train,\n                    Z=Z_train,\n                    clusters=cluster_train,\n                )\n\n                # get test predictions\n                y_pred = merf.predict(\n                    X=X_test_scaled,\n                    clusters=cluster_test,\n                    Z=Z_test,\n                )\n                # get training predictions\n                y_pred_train = merf.predict(\n                    X=X_train_scaled,\n                    clusters=cluster_train,\n                    Z=Z_train,\n                )\n\n                merf_data = SingleModelFoldResult(\n                    k=k,\n                    model_name=merf_name,\n                    best_model=merf,\n                    best_params=best_params,\n                    y_pred=y_pred,\n                    y_test=y_test,\n                    X_test=X_test_scaled,\n                    y_pred_train=y_pred_train,\n                    y_train=y_train,\n                    X_train=X_train_scaled,\n                    fit_result=fit_result,\n                )\n\n                all_models_dict = merf_data.make_results(\n                    run=run,\n                    results_all_folds=results_all_folds,\n                    study=study,\n                    metrics=metrics,\n                )\n\n                postprocessor = MERFModelPostProcessor()\n                all_models_dict = postprocessor(\n                    results_all_folds=all_models_dict,\n                    fold_result=merf_data,\n                    run=run,\n                    y_pred_base=y_pred_base,\n                    mixed_name=merf_name,\n                )\n\n        print()\n        print()\n\n    return results_all_folds\n</code></pre>"},{"location":"reference/core/#flexcv.core.preprocess_features","title":"<code>flexcv.core.preprocess_features(X_train, X_test)</code>","text":"<p>Scales the features to zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Features for the training set.</p> required <code>X_test</code> <code>DataFrame</code> <p>Features for the test set.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>The preprocessed features as a tuple of pandas DataFrames: (X_train_scaled, X_test_scaled)</p> Source code in <code>flexcv/core.py</code> <pre><code>def preprocess_features(\n    X_train: pd.DataFrame, X_test: pd.DataFrame\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Scales the features to zero mean and unit variance.\n\n    Args:\n        X_train (pd.DataFrame): Features for the training set.\n        X_test (pd.DataFrame): Features for the test set.\n\n    Returns:\n        (tuple[pd.DataFrame, pd.DataFrame]): The preprocessed features as a tuple of pandas DataFrames: (X_train_scaled, X_test_scaled)\n    \"\"\"\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"X_train must be a pandas DataFrame, not {type(X_train)}\")\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"X_test must be a pandas DataFrame, not {type(X_test)}\")\n    if X_train.shape[1] != X_test.shape[1]:\n        raise ValueError(\n            f\"X_train and X_test must have the same number of columns. X_train has {X_train.shape[1]} columns, X_test has {X_test.shape[1]} columns.\"\n        )\n\n    scaler = StandardScaler()\n\n    X_train_scaled = pd.DataFrame(\n        scaler.fit_transform(X_train),\n        columns=X_train.columns,\n        index=X_train.index,\n    )\n    X_test_scaled = pd.DataFrame(\n        scaler.transform(X_test), columns=X_test.columns, index=X_test.index\n    )\n    return X_train_scaled, X_test_scaled\n</code></pre>"},{"location":"reference/core/#flexcv.core.preprocess_slopes","title":"<code>flexcv.core.preprocess_slopes(Z_train_slope, Z_test_slope, must_scale)</code>","text":"<p>This function preprocesses the random slopes variable(s) for use in the mixed effects model.</p> <p>Parameters:</p> Name Type Description Default <code>Z_train_slope</code> <code>DataFrame | Series</code> <p>Random slopes variable(s) for the training set.</p> required <code>Z_test_slope</code> <code>DataFrame | Series</code> <p>Random slopes variable(s) for the test set.</p> required <code>must_scale</code> <code>bool</code> <p>If True, the random slopes are scaled to zero mean and unit variance.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>The preprocessed random slopes as a tuple of numpy arrays: (Z_train, Z_test)</p> Source code in <code>flexcv/core.py</code> <pre><code>def preprocess_slopes(\n    Z_train_slope: pd.DataFrame | pd.Series,\n    Z_test_slope: pd.DataFrame | pd.Series,\n    must_scale: bool,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"This function preprocesses the random slopes variable(s) for use in the mixed effects model.\n\n    Args:\n        Z_train_slope (pd.DataFrame | pd.Series): Random slopes variable(s) for the training set.\n        Z_test_slope (pd.DataFrame | pd.Series): Random slopes variable(s) for the test set.\n        must_scale (bool): If True, the random slopes are scaled to zero mean and unit variance.\n\n    Returns:\n        (tuple[np.ndarray, np.ndarray]): The preprocessed random slopes as a tuple of numpy arrays: (Z_train, Z_test)\n    \"\"\"\n    is_dataframe_train = isinstance(Z_train_slope, pd.DataFrame)\n    is_dataframe_test = isinstance(Z_test_slope, pd.DataFrame)\n    if not is_dataframe_train and not isinstance(Z_train_slope, pd.Series):\n        raise TypeError(\n            f\"Z_train_slope must be a pandas DataFrame or pandas Series, not {type(Z_train_slope)}\"\n        )\n    if not is_dataframe_test and not isinstance(Z_test_slope, pd.Series):\n        raise TypeError(\n            f\"Z_test_slope must be a pandas DataFrame or pandas Series, not {type(Z_test_slope)}\"\n        )\n    if not isinstance(must_scale, bool):\n        raise TypeError(f\"must_scale must be a bool, not {type(must_scale)}\")\n\n    # check dimensions\n    if is_dataframe_train and (Z_train_slope.shape[1] != Z_test_slope.shape[1]):\n        raise ValueError(\n            f\"Z_train_slope and Z_test_slope must have the same number of columns. Z_train_slope has {Z_train_slope.shape[1]} columns, Z_test_slope has {Z_test_slope.shape[1]} columns.\"\n        )\n    # convert to DataFrame\n    if not is_dataframe_train:\n        Z_train_slope = pd.DataFrame(Z_train_slope)\n    if not is_dataframe_test:\n        Z_test_slope = pd.DataFrame(Z_test_slope)\n\n    if must_scale:\n        scaler = StandardScaler()\n        Z_train_slope_scaled = pd.DataFrame(\n            scaler.fit_transform(Z_train_slope),\n            columns=Z_train_slope.columns,\n            index=Z_train_slope.index,\n        )\n        Z_test_slope_scaled = pd.DataFrame(\n            scaler.transform(Z_test_slope),\n            columns=Z_test_slope.columns,\n            index=Z_test_slope.index,\n        )\n    else:\n        Z_train_slope_scaled = Z_train_slope\n        Z_test_slope_scaled = Z_test_slope\n\n    Z_train_slope_scaled[\"Intercept\"] = 1\n    cols = Z_train_slope_scaled.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    Z_train_slope_scaled = Z_train_slope_scaled[cols]\n\n    Z_test_slope_scaled[\"Intercept\"] = 1\n    cols = Z_test_slope_scaled.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    Z_test_slope_scaled = Z_test_slope_scaled[cols]\n\n    Z_train = Z_train_slope_scaled.to_numpy()\n    Z_test = Z_test_slope_scaled.to_numpy()\n    return Z_train, Z_test\n</code></pre>"},{"location":"reference/interface/","title":"Interface","text":""},{"location":"reference/interface/#flexcv.interface","title":"<code>flexcv.interface</code>","text":"<p>This module contains the CrossValidation class. This class is the central interface to interact with <code>flexcv</code>.</p>"},{"location":"reference/interface/#flexcv.interface.CrossValidation","title":"<code>flexcv.interface.CrossValidation</code>  <code>dataclass</code>","text":"<p>This class is the central interface to interact with <code>flexcv</code>. Use this dataclass to configure your cross validation run with it's <code>set_***()</code> methods. You can use method chaining to set multiple configurations at once. This allows you to provide extensive configuration with few lines of code. It also helps you to log the configuration and results to Neptune.</p> Example <pre><code>&gt;&gt;&gt; import flexcv\n&gt;&gt;&gt; import neptune\n&gt;&gt;&gt; X = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"z\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; mapping = flexcv.ModelMappingDict(\n...     {\n...         \"LinearModel\": flexcv.ModelConfigDict(\n...             {\n...                 \"model\": \"LinearRegression\",\n...                 \"kwargs\": {\"fit_intercept\": True},\n...             }\n...         ),\n...     }\n... )\n&gt;&gt;&gt; run = neptune.init_run()\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; results = (\n...     cv\n...     .with_data(X, y)\n...     .with_models(mapping)\n...     .log(run)\n...     .perform()\n...     .get_results()\n... )\n</code></pre> <p>Methods:</p> Name Description <code>set_data</code> <p>Sets the data for cross validation. Pass your dataframes and series here.</p> <code>set_splits</code> <p>Sets the cross validation strategy for inner and outer folds. You may need to import <code>flexcv.CrossValMethod</code>.</p> <code>set_models</code> <p>Sets the models to be cross validated. Pass hyperparameter distributions for model tuning here. You may need to import <code>flexcv.ModelMappingDict</code> and <code>flexcv.ModelConfigDict</code>.</p> <code>set_inner_cv</code> <p>Sets the inner cross validation configuration. Pass arguments regarding the hyperparameter optimization process.</p> <code>set_mixed_effects</code> <p>Sets the mixed effects parameters. Control if mixed effects are modeled and set arguments regarding the Expectation Maximization algorithm.</p> <code>set_run</code> <p>Sets the run parameters. Pass your Neptune run object here.</p> <code>perform</code> <p>Performs cross validation. Just call this method without args to trigger the nested cross validation run.</p> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>CrossValidation object.</p> Source code in <code>flexcv/interface.py</code> <pre><code>@dataclass\nclass CrossValidation:\n    \"\"\"This class is the central interface to interact with `flexcv`.\n    Use this dataclass to configure your cross validation run with it's `set_***()` methods.\n    You can use method chaining to set multiple configurations at once.\n    This allows you to provide extensive configuration with few lines of code.\n    It also helps you to log the configuration and results to Neptune.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import flexcv\n        &gt;&gt;&gt; import neptune\n        &gt;&gt;&gt; X = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"z\": [1, 2, 3, 4, 5]})\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; mapping = flexcv.ModelMappingDict(\n        ...     {\n        ...         \"LinearModel\": flexcv.ModelConfigDict(\n        ...             {\n        ...                 \"model\": \"LinearRegression\",\n        ...                 \"kwargs\": {\"fit_intercept\": True},\n        ...             }\n        ...         ),\n        ...     }\n        ... )\n        &gt;&gt;&gt; run = neptune.init_run()\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; results = (\n        ...     cv\n        ...     .with_data(X, y)\n        ...     .with_models(mapping)\n        ...     .log(run)\n        ...     .perform()\n        ...     .get_results()\n        ... )\n        ```\n\n    Methods:\n        set_data: Sets the data for cross validation. Pass your dataframes and series here.\n        set_splits: Sets the cross validation strategy for inner and outer folds. You may need to import `flexcv.CrossValMethod`.\n        set_models: Sets the models to be cross validated. Pass hyperparameter distributions for model tuning here. You may need to import `flexcv.ModelMappingDict` and `flexcv.ModelConfigDict`.\n        set_inner_cv: Sets the inner cross validation configuration. Pass arguments regarding the hyperparameter optimization process.\n        set_mixed_effects: Sets the mixed effects parameters. Control if mixed effects are modeled and set arguments regarding the Expectation Maximization algorithm.\n        set_run: Sets the run parameters. Pass your Neptune run object here.\n        perform: Performs cross validation. Just call this method without args to trigger the nested cross validation run.\n\n    Returns:\n      (CrossValidation): CrossValidation object.\n\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._was_logged_ = False\n        self._was_performed_ = False\n        self._config_logged_ = False\n        self._result_logged_ = False\n        self.config = {\n            # Data related\n            \"X\": None,\n            \"y\": None,\n            \"target_name\": \"\",\n            \"dataset_name\": \"\",\n            \"groups\": None,\n            \"slopes\": None,\n            # CV strategy related\n            \"n_splits_out\": 5,\n            \"n_splits_in\": 5,\n            \"split_out\": CrossValMethod.KFOLD,\n            \"split_in\": CrossValMethod.KFOLD,\n            \"scale_out\": True,\n            \"scale_in\": True,\n            \"metrics\": None,\n            # models and optimisation\n            \"mapping\": ModelMappingDict({}),\n            \"model_effects\": \"fixed\",\n            # optimization related\n            \"n_trials\": 100,\n            \"objective_scorer\": None,\n            # regarding mixed effects\n            \"em_max_iterations\": 100,\n            \"em_stopping_threshold\": None,\n            \"em_stopping_window\": None,\n            \"predict_known_groups_lmm\": True,\n            # run related\n            \"random_seed\": 42,\n            \"diagnostics\": False,\n            \"break_cross_val\": False,\n            \"run\": DummyRun(),\n        }\n\n    def __repr__(self) -&gt; str:\n        return pformat(self.config)\n\n    def __str__(self) -&gt; str:\n        return pformat(self.config)\n\n    def set_data(\n        self,\n        X: pd.DataFrame,\n        y: pd.DataFrame | pd.Series,\n        groups: pd.DataFrame | pd.Series = None,\n        slopes: pd.DataFrame | pd.Series = None,\n        target_name: str = \"\",\n        dataset_name: str = \"\",\n    ):\n        \"\"\"Set the data for cross validation.\n\n        Args:\n            X (pd.DataFrame): The features. Must not contain the target or groups.\n            y (pd.DataFrame | pd.Series): The target variable.\n            groups (pd.DataFrame | pd.Series): The grouping/clustering variable. (Default value = None)\n            slopes (pd.DataFrame | pd.Series): The random slopes variable(s) (Default value = None)\n            target_name (str): Customize the target's name. This string will be used in logging. (Default value = \"\")\n            dataset_name (str): Customize your datasdet's name. This string will be used in logging. (Default value = \"\")\n\n        Returns:\n            (CrossValidation): self\n\n        Example:\n            ```python\n            &gt;&gt;&gt; X = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"z\": [1, 2, 3, 4, 5]})\n            &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; cv.set_data(X, y)\n            ```\n        \"\"\"\n        # check values\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"X must be a pandas DataFrame\")\n        if not isinstance(y, pd.DataFrame) and not isinstance(y, pd.Series):\n            raise TypeError(\"y must be a pandas DataFrame or Series\")\n        if (\n            not isinstance(groups, pd.DataFrame)\n            and not isinstance(groups, pd.Series)\n            and groups is not None\n        ):\n            raise TypeError(\"group must be a pandas DataFrame or Series\")\n        if (\n            not isinstance(slopes, pd.DataFrame)\n            and not isinstance(slopes, pd.Series)\n            and slopes is not None\n        ):\n            raise TypeError(\"slopes must be a pandas DataFrame or Series\")\n        if not isinstance(target_name, str):\n            raise TypeError(\"target_name must be a string\")\n        if not isinstance(dataset_name, str):\n            raise TypeError(\"dataset_name must be a string\")\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must have the same number of rows\")\n        if groups is not None and X.shape[0] != groups.shape[0]:\n            raise ValueError(\"X and group must have the same number of rows\")\n        if slopes is not None:\n            if X.shape[0] != slopes.shape[0]:\n                raise ValueError(\"X and slopes must have the same number of rows\")\n            # make sure slopes (which can be multiple columns or a series) is in X\n            if isinstance(slopes, pd.Series):\n                if slopes.name not in X.columns:\n                    raise ValueError(f\"{slopes.name} is not in X\")\n            elif isinstance(slopes, pd.DataFrame):\n                for slope in slopes.columns:\n                    if slope not in X.columns:\n                        raise ValueError(f\"{slope} is not in X\")\n\n        # assign values\n        self.config[\"X\"] = X\n        self.config[\"y\"] = y\n        self.config[\"groups\"] = groups\n        self.config[\"slopes\"] = slopes\n        if target_name:\n            self.config[\"target_name\"] = target_name\n        else:\n            self.config[\"target_name\"] = str(y.name)\n        if dataset_name:\n            self.config[\"dataset_name\"] = dataset_name\n        return self\n\n    def set_splits(\n        self,\n        split_out: str\n        | CrossValMethod\n        | BaseCrossValidator\n        | Iterator = CrossValMethod.KFOLD,\n        split_in: str\n        | CrossValMethod\n        | BaseCrossValidator\n        | Iterator = CrossValMethod.KFOLD,\n        n_splits_out: int = 5,\n        n_splits_in: int = 5,\n        scale_out: bool = True,\n        scale_in: bool = True,\n        break_cross_val: bool = False,\n        metrics: MetricsDict = None,\n    ):\n        \"\"\"Set the cross validation strategy.\n        Set the split method simply by passing the `CrossValMethod` as a string or enum value. Passing as string might be more convenient for you but could lead to typos.\n        When passing as string, the string must be a valid value of the `CrossValMethod` enum.\n        See the reference for `CrossValMethod` for more details.\n\n        Valid strings for `split_out` and `split_in`:\n            - \"KFold\"\n            - \"StratifiedKFold\"\n            - \"CustomStratifiedKFold\"\n            - \"GroupKFold\"\n            - \"StratifiedGroupKFold\"\n            - \"CustomStratifiedGroupKFold\"\n\n        Args:\n            split_out (str | CrossValMethod): Outer split method. (Default value = CrossValMethod.KFOLD)\n            split_in (str | CrossValMethod): Inner split method for hyperparameter tuning. (Default value = CrossValMethod.KFOLD)\n            n_splits_out (int): Number of splits in outer loop. (Default value = 5)\n            n_splits_in (int): Number of splits in inner loop. (Default value = 5)\n            scale_out (bool): Whether or not the Features of the outer loop will be scaled to mean 0 and variance 1. (Default value = True)\n            scale_in (bool): Whether or not the Features of the inner loop will be scaled to mean 0 and variance 1. (Default value = True)\n            break_cross_val (bool): If True, the outer loop we break after first iteration. Use for debugging. (Default value = False)\n            metrics (MetricsDict): A dict containing evaluation metrics for the outer loop results. See MetricsDict for details. (Default value = None)\n\n        Returns:\n          (CrossValidation): self\n\n        Example:\n            Passing the method as instance of CrossValMethod:\n            ```python\n            &gt;&gt;&gt; from flexcv import CrossValidation, CrossValMethod\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; cv.set_splits(split_out=CrossValMethod.KFOLD, split_in=CrossValMethod.KFOLD)\n            ```\n            Passing the method as a string:\n            ```python\n            &gt;&gt;&gt; from flexcv import CrossValidation\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; cv.set_splits(split_out=\"KFold\", split_in=\"KFold\")\n            # Valid strings: \"KFold\", \"StratifiedKFold\", \"CustomStratifiedKFold\", \"GroupKFold\", \"StratifiedGroupKFold\", \"CustomStratifiedGroupKFold\"\n            ```\n\n\n        Split methods:\n            The split strategy is controlled by the `split_out` and `split_in` arguments. You can pass the actual `CrossValMethod` enum or a string.\n\n            The `split_out` argument controls the fold assignment in the outer cross validation loop.\n            In each outer loop the model is fit on the training fold and model performance is evaluated on unseen data of the test fold.\n            The `split_in` argument controls the inner loop split strategy. The inner loop cross validates the hyperparameters of the model.\n            A model is typically built by sampling from a distribution of hyperparameters. It is fit on the inner training fold and evaluated on the inner test fold.\n            Of course, the inner loop is nested in the outer loop, so the inner split is performed on the outer training fold.\n\n            Read more about it in the respective documentation of the `CrossValMethod` enum.\n\n        \"\"\"\n\n        # get values of CrossValMethod enums\n        ALLOWED_STRINGS = [method.value for method in CrossValMethod]\n        ALLOWED_METHODS = [method for method in CrossValMethod]\n\n        if isinstance(split_out, str) and (split_out not in ALLOWED_STRINGS):\n            raise TypeError(\n                f\"split_out must be a valid CrossValMethod name, was {split_out}. Choose from: \"\n                + \", \".join(ALLOWED_STRINGS)\n                + \".\"\n            )\n\n        if isinstance(split_in, str) and (split_in not in ALLOWED_STRINGS):\n            raise TypeError(\n                f\"split_in must be a valid CrossValMethod name, was {split_in}. Choose from: \"\n                + \", \".join(ALLOWED_STRINGS)\n                + \".\"\n            )\n\n        if not any(\n            [\n                isinstance(split_out, str),\n                isinstance(split_out, CrossValMethod),\n                isinstance(split_out, BaseCrossValidator),\n                isinstance(split_out, Iterator),\n            ]\n        ):\n            raise TypeError(\n                \"split_out must be of Type str, CrossValMethod, BaseCrossValidator or Iterator.\"\n            )\n\n        if not any(\n            [\n                isinstance(split_in, str),\n                isinstance(split_in, CrossValMethod),\n                isinstance(split_in, BaseCrossValidator),\n                isinstance(split_in, Iterator),\n            ]\n        ):\n            raise TypeError(\n                \"split_in must be of Type str, CrossValMethod, BaseCrossValidator or Iterator.\"\n            )\n\n        if isinstance(split_out, CrossValMethod) and (split_out not in ALLOWED_METHODS):\n            raise TypeError(\"split_out must be a valid CrossValMethod \")\n\n        if isinstance(split_in, CrossValMethod) and (split_in not in ALLOWED_METHODS):\n            raise TypeError(\"split_in must be a valid CrossValMethod\")\n\n        if not isinstance(n_splits_out, int):\n            raise TypeError(\"n_splits_out must be an integer\")\n        if not isinstance(n_splits_in, int):\n            raise TypeError(\"n_splits_in must be an integer\")\n\n        if not isinstance(scale_in, bool):\n            raise TypeError(\"scale_in must be a boolean\")\n        if not isinstance(scale_out, bool):\n            raise TypeError(\"scale_out must be a boolean\")\n\n        if not isinstance(break_cross_val, bool):\n            raise TypeError(\"break_cross_val must be a boolean\")\n        if metrics and not isinstance(metrics, MetricsDict):\n            raise TypeError(\"metrics must be a MetricsDict\")\n\n        if isinstance(split_out, str):\n            split_out = string_to_crossvalmethod(split_out)\n        if isinstance(split_in, str):\n            split_in = string_to_crossvalmethod(split_in)\n        # assign values\n        self.config[\"split_out\"] = split_out\n        self.config[\"split_in\"] = split_in\n        self.config[\"n_splits_out\"] = n_splits_out\n        self.config[\"n_splits_in\"] = n_splits_in\n        self.config[\"scale_in\"] = scale_in\n        self.config[\"scale_out\"] = scale_out\n        self.config[\"break_cross_val\"] = break_cross_val\n        if metrics:\n            self.config[\"metrics\"] = metrics\n        return self\n\n    def set_models(\n        self,\n        mapping: ModelMappingDict = None,\n        yaml_path: str | pathlib.Path = None,\n        yaml_string: str = None,\n    ):\n        \"\"\"Set your models and related parameters. Pass a ModelMappingDict or pass yaml code or a path to a yaml file.\n        The mapping attribute of the class is a ModelMappingDict that contains a ModelConfigDict for each model.\n        The class attribute self.config[\"mapping\"] is always updated in this method. \n        Therefore, you can call this method multiple times to add models to the mapping.\n        You can also call set_models() with a ModelMappingDict and then call set_models() again with yaml code or a path to a yaml file or after you already called add_models().\n\n\n        Args:\n          mapping (ModelMappingDict[str, ModelConfigDict]): Dict of model names and model configurations. See ModelMappingDict for more information. (Default value = None)\n          yaml_path (str | pathlib.Path): Path to a yaml file containing a model mapping. See flexcv.yaml_parser for more information. (Default value = None)\n          yaml_string (str): String containing yaml code. See flexcv.yaml_parser for more information. (Default value = None)\n\n        Returns:\n          (CrossValidation): self\n\n        Example:\n            In your yaml file:\n            ```yaml\n            RandomForest:\n                model: sklearn.ensemble.RandomForestRegressor\n                post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n                requires_inner_cv: True\n                params:\n                    max_depth: !Int\n                        low: 1\n                        high: 10\n            ```\n            In your code:\n            ```python\n            &gt;&gt;&gt; from flexcv import CrossValidation\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; cv.set_models(yaml_path=\"path/to/your/yaml/file\")\n            ```\n            This will automatically read the yaml file and create a ModelMappingDict.\n            It even takes care of the imports and instantiates the classes of model, postprocessor and for the optune distributions.\n        \"\"\"\n        if not any((mapping, yaml_path, yaml_string)):\n            raise ValueError(\n                \"You must provide either mapping, yaml_path, or yaml_string\"\n            )\n\n        if sum(bool(x) for x in (mapping, yaml_path, yaml_string)) &gt; 1:\n            raise ValueError(\n                \"You must provide either mapping, yaml_path or yaml_string, not multiple\"\n            )\n\n        if mapping is not None:\n            if not isinstance(mapping, ModelMappingDict):\n                raise TypeError(\"mapping must be a ModelMappingDict\")\n            self.config[\"mapping\"].update(mapping)\n\n        elif yaml_path is not None:\n            if not isinstance(yaml_path, str) and not isinstance(\n                yaml_path, pathlib.Path\n            ):\n                raise TypeError(\"yaml_path must be a string or pathlib.Path\")\n            self.config[\"mapping\"].update(read_mapping_from_yaml_file(yaml_path))\n\n        elif yaml_string is not None:\n            if not isinstance(yaml_string, str):\n                raise TypeError(\"yaml_string must be a string\")\n            self.config[\"mapping\"].update(read_mapping_from_yaml_string(yaml_string))\n\n        return self\n\n    def set_inner_cv(\n        self,\n        n_trials: int = 100,\n        objective_scorer: ObjectiveScorer = None,\n    ):\n        \"\"\"Configure parameters regarding inner cross validation and Optuna optimization.\n\n        Args:\n          n_trials (int): Number of trials to sample from the parameter distributions (Default value = 100)\n          objective_scorer (ObjectiveScorer): Callable to provide the optimization objective value. Is called during Optuna SearchCV (Default value = None)\n\n        Returns:\n          (CrossValidation): self\n\n        \"\"\"\n        # check values\n        if not isinstance(n_trials, int):\n            raise TypeError(\"n_trials must be an integer\")\n        if objective_scorer and not isinstance(objective_scorer, ObjectiveScorer):\n            raise TypeError(\"objective_scorer must be an ObjectiveScorer\")\n\n        # assign values\n        self.config[\"n_trials\"] = n_trials\n        self.config[\"objective_scorer\"] = objective_scorer\n        return self\n\n    def set_lmer(self, predict_known_groups_lmm: bool = True):\n        \"\"\"Configure parameters regarding linear mixed effects regression models.\n\n        Args:\n          predict_known_groups_lmm (bool): For use with LMER, whether or not known groups should be predicted (Default value = True)\n\n        Returns:\n          (CrossValidation): self\n\n        \"\"\"\n        # check values\n        if not isinstance(predict_known_groups_lmm, bool):\n            raise TypeError(\"predict_known_groups_lmm must be a boolean\")\n\n        # assign values\n        self.config[\"predict_known_groups_lmm\"] = predict_known_groups_lmm\n        return self\n\n    def set_merf(\n        self,\n        add_merf_global: bool = False,\n        em_max_iterations: int = 100,\n        em_stopping_threshold: float = None,\n        em_stopping_window: int = None,\n    ):\n        \"\"\"Configure mixed effects parameters.\n\n        Args:\n          add_merf_global (bool): If True, the model is passed into the MERF class after it is evaluated, to obtain mixed effects corrected predictions. (Default value = False)\n          em_max_iterations (int): For use with EM. Max number of iterations (Default value = 100)\n          em_stopping_threshold (float): For use with EM. Threshold of GLL residuals for early stopping (Default value = None)\n          em_stopping_window (int): For use with EM. Number of consecutive iterations to be below threshold for early stopping (Default value = None)\n\n\n        Returns:\n          (CrossValidation): self\n\n        \"\"\"\n        if not isinstance(add_merf_global, bool):\n            raise TypeError(\"add_merf must be a boolean\")\n        if not isinstance(em_max_iterations, int):\n            raise TypeError(\"em_max_iterations must be an integer\")\n        if em_stopping_threshold and not isinstance(em_stopping_threshold, float):\n            raise TypeError(\"em_stopping_threshold must be a float\")\n        if em_stopping_window and not isinstance(em_stopping_window, int):\n            raise TypeError(\"em_stopping_window must be an integer\")\n\n        # assign values\n        self.config[\"add_merf_global\"] = add_merf_global\n        self.config[\"em_max_iterations\"] = em_max_iterations\n        self.config[\"em_stopping_threshold\"] = em_stopping_threshold\n        self.config[\"em_stopping_window\"] = em_stopping_window\n        return self\n\n    def set_run(\n        self,\n        run: NeptuneRun = None,\n        diagnostics: bool = False,\n        random_seed: int = 42,\n    ):\n        \"\"\"\n\n        Args:\n          run (NeptuneRun): The run object to use for logging (Default value = None)\n          diagnostics (bool): If True, extended diagnostic plots are logged (Default value = False)\n          random_seed (int): Seed for random processes (Default value = 42)\n\n        Returns:\n            (CrossValidation): self\n\n        \"\"\"\n        # check values\n        if run and not isinstance(run, NeptuneRun):\n            raise TypeError(\"run must be a NeptuneRun\")\n        if not isinstance(diagnostics, bool):\n            raise TypeError(\"diagnostics must be a boolean\")\n        if not isinstance(random_seed, int):\n            raise TypeError(\n                \"random_seed is not 42 hahaha. No seriously, random_seed must be an integer\"\n            )\n\n        # assign values\n        self.config[\"run\"] = run\n        self.config[\"diagnostics\"] = diagnostics\n        self.config[\"random_seed\"] = random_seed\n        return self\n\n    def add_model(\n        self,\n        model_class: object,\n        requires_inner_cv: bool = False,\n        model_name: str = \"\",\n        post_processor: ModelPostProcessor = None,\n        params: dict = None,\n        callbacks: Optional[Sequence[TrainingCallback]] = None,\n        model_kwargs: dict = None,\n        fit_kwargs: dict = None,\n        **kwargs,\n    ):\n        \"\"\"Add a model to the model mapping dict.\n        This method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes.\n\n        Args:\n          model_class (object): The model class. Must have a fit() method.\n          requires_inner_cv (bool): Whether or not the model requires inner cross validation. (Default value = False)\n          model_name (str): The name of the model. Used for logging.\n          post_processor (ModelPostProcessor): A post processor to be applied to the model. (Default value = None)\n          callbacks (Optional[Sequence[TrainingCallback]]): Callbacks to be passed to the fit method of the model in outer CV. (Default value = None)\n          kwargs: A dict of additional keyword arguments that will be passed to the model constructor.\n          fit_kwargs: A dict of keyword arguments that will be passed to the model fit method.\n          **kwargs: Arbitrary keyword arguments that will be passed to the ModelConfigDict.\n\n        Returns:\n            (CrossValidation): self\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from flexcv import CrossValidation\n            &gt;&gt;&gt; from flexcv.models import LinearModel\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; cv.add_model(LinearModel, model_name=\"LinearModel\", skip_inner_cv=True)\n            ```\n            Is equivalent to:\n            ```python\n            &gt;&gt;&gt; from flexcv import CrossValidation\n            &gt;&gt;&gt; from flexcv.models import LinearModel\n            &gt;&gt;&gt; from flexcv.model_mapping import ModelMappingDict, ModelConfigDict\n            &gt;&gt;&gt; cv = CrossValidation()\n            &gt;&gt;&gt; mapping = ModelMappingDict(\n            ...     {\n            ...         \"LinearModel\": ModelConfigDict(\n            ...             {\n            ...                 \"model\": LinearModel,\n            ...                 \"skip_inner_cv\": True,\n            ...             }\n            ...         ),\n            ...     }\n            ... )\n            &gt;&gt;&gt; cv.set_models(mapping)\n            ```\n            As you can see the `add_model()` method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes.\n            In cases of multiple models per run, or if you want to reuse the model mapping dict, you should look into the `ModelMappingDict` and the `set_models()` method.\n\n        \"\"\"\n\n        # check values\n        if not isinstance(model_name, str):\n            raise TypeError(\"model_name must be a string\")\n\n        if not issubclass(model_class, object):\n            raise TypeError(\"model_class must be a class\")\n\n        if not isinstance(requires_inner_cv, bool):\n            raise TypeError(\"skip_inner_cv must be a boolean\")\n\n        if params is not None and not isinstance(params, dict):\n            raise TypeError(\"params must be a dict\")\n\n        # check if post_processor is a class that inherits ModelPostProcessor object that is not instantiated\n        if post_processor and not issubclass(post_processor, ModelPostProcessor):\n            raise TypeError(\"post_processor must be a ModelPostProcessor\")\n\n        # params and kwargs may not contain the same keys\n        if kwargs is not None and not isinstance(kwargs, dict):\n            raise TypeError(\"kwargs must be a dict\")\n\n        if (\n            kwargs is not None\n            and params is not None\n            and (set(params.keys()) &amp; set(kwargs.keys()))\n        ):\n            raise ValueError(\n                \"params and additional kwargs may not contain the same keys\"\n            )\n\n        if callbacks is not None and not isinstance(callbacks, Sequence):\n            raise TypeError(\"callbacks must be a Sequence of TrainingCallbacks\")\n\n        if requires_inner_cv and params is None:\n            warnings.warn(\n                f\"You did not provide hyperparameters for the model {model_name} but set requires_inner_cv to True.\",\n                UserWarning,\n            )\n\n        if model_kwargs is not None and not isinstance(model_kwargs, dict):\n            raise TypeError(\"model_kwargs must be a dict\")\n\n        if fit_kwargs is not None and not isinstance(fit_kwargs, dict):\n            raise TypeError(\"fit_kwargs must be a dict\")\n\n        if not requires_inner_cv and params is not None and params != {}:\n            requires_inner_cv = True\n\n        if not params:\n            params = {}\n\n        if not model_name:\n            model_name = model_class.__name__\n\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        if fit_kwargs is None:\n            fit_kwargs = {}\n\n        if kwargs is None:\n            kwargs = {}\n\n        callbacks_dict = {}\n        if callbacks is not None:\n            callbacks_dict = {\"callbacks\": callbacks}\n\n        config_dict = {\n            \"model\": model_class,\n            \"post_processor\": post_processor,\n            \"requires_inner_cv\": requires_inner_cv,\n            \"params\": params,\n            \"callbacks\": callbacks_dict,\n            \"fit_kwargs\": fit_kwargs,\n            \"model_kwargs\": model_kwargs,\n            **kwargs,\n        }\n\n        self.config[\"mapping\"][model_name] = ModelConfigDict(config_dict)\n        return self\n\n    def _describe_config(self):\n        \"\"\"This function creates a representation of the config dict for logging purposes. It includes the Model Mapping and a target variable description.\"\"\"\n\n        mapping_str = \" + \".join(self.config[\"mapping\"].keys())\n        target_str = self.config[\"target_name\"]\n        return f\"CrossValidation Summary for Regression of Target {target_str} with Models {mapping_str}\"\n\n    def _prepare_before_perform(self):\n        \"\"\"Make preparation steps before performing the cross validation.\n\n        - Checks if a neptune run object has been set. If the user did not provide a neptune run object, a dummy run is instantiated.\n        - Checks if the split_out and split_in attributes are set to strings and converts the strings to a CrossValMethod enum.\n\n        Iterates over the ModelMappingDict:\n        - Checks if n_trials is set for every model. If not, set to the value of self.config[\"n_trials\"]. If n_trials is not set for a model and not set for the CrossValidation object, it is not used.\n        - Checks if \"add_merf\" is set for a model or if it set globally in the class. In the latter case, the value is set for the model.\n        - Checks if the model signature contains \"clusters\". If so, the model is a mixed effects model and we set a flag \"consumes_clusters\" to True.\n\n        This method is called by the `perform()` method.\n        \"\"\"\n        if isinstance(self.config[\"split_out\"], str):\n            self.config[\"split_out\"] = string_to_crossvalmethod(\n                self.config[\"split_out\"]\n            )\n\n        if isinstance(self.config[\"split_in\"], str):\n            self.config[\"split_in\"] = string_to_crossvalmethod(self.config[\"split_in\"])\n\n        if not \"run\" in self.config:\n            self.config[\"run\"] = DummyRun()\n\n        # check if add_merf is set globally\n        # iterate over all items in the model mapping\n        # if the inenr dict has a key \"add_merf\" do nothing\n        # if it doesnt and add_merf is set globally, set it for the model\n        self.config[\"add_merf_global\"] = self.config.setdefault(\n            \"add_merf_global\", False\n        )\n        self.config[\"n_trials\"] = self.config.setdefault(\"n_trials\", 100)\n\n        for model_key, inner_dict in self.config[\"mapping\"].items():\n            if \"fit_kwargs\" not in inner_dict:\n                self.config[\"mapping\"][model_key][\"fit_kwargs\"] = {}\n\n            if \"n_trials\" not in inner_dict:\n                self.config[\"mapping\"][model_key][\"n_trials\"] = self.config[\"n_trials\"]\n\n            if \"add_merf\" not in inner_dict:\n                self.config[\"mapping\"][model_key][\"add_merf\"] = self.config[\n                    \"add_merf_global\"\n                ]\n\n            # check model signature for groups and slopes\n            # if the model signature contains groups and slopes, the model is a mixed effects model\n            model_class = inner_dict[\"model\"]\n            model_signature_parameters = inspect.signature(model_class).parameters\n            model_fit_signature_parameters = inspect.signature(\n                model_class.fit\n            ).parameters\n\n            self.config[\"mapping\"][model_key][\"consumes_clusters\"] = (\n                \"clusters\" in model_fit_signature_parameters\n            )\n            self.config[\"mapping\"][model_key][\"requires_formula\"] = (\n                \"formula\" in model_fit_signature_parameters\n            )\n\n            if (\n                \"model_kwargs\" not in self.config[\"mapping\"][model_key]\n            ):  # TODO test case\n                self.config[\"mapping\"][model_key][\"model_kwargs\"] = {}\n\n            if \"n_jobs\" in model_signature_parameters:\n                self.config[\"mapping\"][model_key][\"model_kwargs\"][\n                    \"n_jobs\"\n                ] = self.config[\"mapping\"][model_key][\"n_jobs_model\"]\n\n            if \"random_state\" in model_signature_parameters:\n                self.config[\"mapping\"][model_key][\"model_kwargs\"][\n                    \"random_state\"\n                ] = self.config[\"random_seed\"]\n\n    def _log_config(self):\n        \"\"\"Logs the config to Neptune. If None, a Dummy is instantiated.\n\n        Args:\n          run (NeptuneRun): The run to log to (Default value = None)\n\n        Returns:\n          (CrossValidation): self\n\n        \"\"\"\n        run = self.config[\"run\"]\n\n        # run[\"Data/DatasetName\"] = self.config[\"dataset_name\"] if self.config[\"dataset_name\"] is not None else \"None\"\n        run[\"Data/TargetName\"] = self.config[\"target_name\"]\n        run[\"Data/model_effects\"] = self.config[\"model_effects\"]\n        run[\"Data/X\"].upload(File.as_html(self.config[\"X\"]))\n        run[\"Data/y\"].upload(File.as_html(pd.DataFrame(self.config[\"y\"])))\n\n        if self.config[\"groups\"] is not None:\n            run[\"Data/groups\"].upload(File.as_html(pd.DataFrame(self.config[\"groups\"])))\n            run[\"Data/groups_name\"] = self.config[\"groups\"].name\n\n        if self.config[\"slopes\"] is not None:\n            run[\"Data/slopes\"].upload(File.as_html(pd.DataFrame(self.config[\"slopes\"])))\n            run[\"Data/slopes_name\"] = pd.DataFrame(\n                self.config[\"slopes\"]\n            ).columns.tolist()\n\n        try:\n            run[\"Config/Split/cross_val_method_out\"] = self.config[\"split_out\"].value\n        except AttributeError:\n            run[\"Config/Split/cross_val_method_out\"] = self.config[\"split_out\"]\n        try:\n            run[\"Config/Split/cross_val_method_in\"] = self.config[\"split_in\"].value\n        except AttributeError:\n            run[\"Config/Split/cross_val_method_in\"] = self.config[\"split_in\"]\n\n        if self.config[\"metrics\"] is not None:\n            run[\"Config/Split/metrics\"] = pformat(self.config[\"metrics\"])\n        else:\n            run[\"Config/Split/metrics\"] = \"default\"\n\n        run[\"Config/Split/n_splits_out\"] = self.config[\"n_splits_out\"]\n        run[\"Config/Split/n_splits_in\"] = self.config[\"n_splits_in\"]\n        run[\"Config/Split/scale_in\"] = self.config[\"scale_in\"]\n        run[\"Config/Split/scale_out\"] = self.config[\"scale_out\"]\n        run[\"Config/Split/break_cross_val\"] = self.config[\"break_cross_val\"]\n        run[\"ModelMapping\"] = pformat(self.config[\"mapping\"])\n        run[\"Config/Optimization/n_trials\"] = self.config[\"n_trials\"]\n        run[\"Config/Optimization/objective_scorer\"] = pformat(\n            self.config[\"objective_scorer\"]\n        )\n        run[\"Config/MixedEffects/em_max_iterations\"] = self.config[\"em_max_iterations\"]\n        run[\"Config/MixedEffects/em_stopping_threshold\"] = self.config[\n            \"em_stopping_threshold\"\n        ]\n        run[\"Config/MixedEffects/em_stopping_window\"] = self.config[\n            \"em_stopping_window\"\n        ]\n        run[\"Config/MixedEffects/predict_known_groups_lmm\"] = self.config[\n            \"predict_known_groups_lmm\"\n        ]\n        run[\"Config/Run/diagnostics\"] = self.config[\"diagnostics\"]\n        run[\"Config/Run/random_seed\"] = self.config[\"random_seed\"]\n        self._config_logged_ = True\n\n    def _log_results(self):\n        \"\"\"This function logs the results to Neptune.\"\"\"\n        if hasattr(self, \"results_\"):\n            summary_df = self.results_.summary\n            self.config[\"run\"][\"Results/Summary\"].upload(File.as_html(summary_df))\n        else:\n            logger.warning(\n                \"You have not called perform() yet. No results to log. Call perform() to log the results.\"\n            )\n        self.config[\"run\"][\"description\"] = self._describe_config()\n        self._result_logged_ = True\n        return self\n\n    @run_padding\n    def perform(self):\n        \"\"\"Perform the cross validation according to the configuration passed by the user.\n        Checks if a neptune run object has been set. If the user did not provide a neptune run object, a dummy run is instantiated.\n        All logs and plots will be logged to the dummy run and will be lost.\n        However, the cross validation results is created and can be returned via the `CrossValidation.results` property.\n\n        Returns:\n            (CrossValidation): self\n        \"\"\"\n        self._prepare_before_perform()\n        self._log_config()\n        results = cross_validate(**self.config)\n        self._was_performed_ = True\n        self.results_ = CrossValidationResults(results)\n        self._log_results()\n        self._was_logged_ = self._config_logged_ and self._result_logged_\n        return self\n\n    def get_results(self) -&gt; CrossValidationResults:\n        \"\"\"Returns a `CrossValidationResults` object. This results object is a wrapper class around the results dict from the `cross_validate` function.\"\"\"\n        if hasattr(self, \"results_\"):\n            return self.results_\n        else:\n            raise RuntimeError(\n                \"You must call perform() before you can get the results.\"\n            )\n\n    @property\n    def results(self) -&gt; CrossValidationResults:\n        \"\"\"Returns a `CrossValidationResults` object. This results object is a wrapper class around the results dict from the `cross_validate` function.\"\"\"\n        if hasattr(self, \"results_\"):\n            return self.results_\n        else:\n            raise RuntimeError(\n                \"You must call perform() before you can get the results.\"\n            )\n\n    @property\n    def was_performed(self) -&gt; bool:\n        \"\"\"Returns True if the cross validation was performed.\"\"\"\n        return self._was_performed_\n\n    @property\n    def was_logged(self) -&gt; bool:\n        \"\"\"Returns True if the cross validation was logged.\"\"\"\n        return self._was_logged_\n\n    @property\n    def cv_description(self) -&gt; str:\n        \"\"\"Returns a string describing the cross validation configuration.\"\"\"\n        return self._describe_config()\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.cv_description","title":"<code>flexcv.interface.CrossValidation.cv_description: str</code>  <code>property</code>","text":"<p>Returns a string describing the cross validation configuration.</p>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.results","title":"<code>flexcv.interface.CrossValidation.results: CrossValidationResults</code>  <code>property</code>","text":"<p>Returns a <code>CrossValidationResults</code> object. This results object is a wrapper class around the results dict from the <code>cross_validate</code> function.</p>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.was_logged","title":"<code>flexcv.interface.CrossValidation.was_logged: bool</code>  <code>property</code>","text":"<p>Returns True if the cross validation was logged.</p>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.was_performed","title":"<code>flexcv.interface.CrossValidation.was_performed: bool</code>  <code>property</code>","text":"<p>Returns True if the cross validation was performed.</p>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.add_model","title":"<code>flexcv.interface.CrossValidation.add_model(model_class, requires_inner_cv=False, model_name='', post_processor=None, params=None, callbacks=None, model_kwargs=None, fit_kwargs=None, **kwargs)</code>","text":"<p>Add a model to the model mapping dict. This method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>object</code> <p>The model class. Must have a fit() method.</p> required <code>requires_inner_cv</code> <code>bool</code> <p>Whether or not the model requires inner cross validation. (Default value = False)</p> <code>False</code> <code>model_name</code> <code>str</code> <p>The name of the model. Used for logging.</p> <code>''</code> <code>post_processor</code> <code>ModelPostProcessor</code> <p>A post processor to be applied to the model. (Default value = None)</p> <code>None</code> <code>callbacks</code> <code>Optional[Sequence[TrainingCallback]]</code> <p>Callbacks to be passed to the fit method of the model in outer CV. (Default value = None)</p> <code>None</code> <code>kwargs</code> <p>A dict of additional keyword arguments that will be passed to the model constructor.</p> <code>{}</code> <code>fit_kwargs</code> <code>dict</code> <p>A dict of keyword arguments that will be passed to the model fit method.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments that will be passed to the ModelConfigDict.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Example <p><pre><code>&gt;&gt;&gt; from flexcv import CrossValidation\n&gt;&gt;&gt; from flexcv.models import LinearModel\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; cv.add_model(LinearModel, model_name=\"LinearModel\", skip_inner_cv=True)\n</code></pre> Is equivalent to: <pre><code>&gt;&gt;&gt; from flexcv import CrossValidation\n&gt;&gt;&gt; from flexcv.models import LinearModel\n&gt;&gt;&gt; from flexcv.model_mapping import ModelMappingDict, ModelConfigDict\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; mapping = ModelMappingDict(\n...     {\n...         \"LinearModel\": ModelConfigDict(\n...             {\n...                 \"model\": LinearModel,\n...                 \"skip_inner_cv\": True,\n...             }\n...         ),\n...     }\n... )\n&gt;&gt;&gt; cv.set_models(mapping)\n</code></pre> As you can see the <code>add_model()</code> method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes. In cases of multiple models per run, or if you want to reuse the model mapping dict, you should look into the <code>ModelMappingDict</code> and the <code>set_models()</code> method.</p> Source code in <code>flexcv/interface.py</code> <pre><code>def add_model(\n    self,\n    model_class: object,\n    requires_inner_cv: bool = False,\n    model_name: str = \"\",\n    post_processor: ModelPostProcessor = None,\n    params: dict = None,\n    callbacks: Optional[Sequence[TrainingCallback]] = None,\n    model_kwargs: dict = None,\n    fit_kwargs: dict = None,\n    **kwargs,\n):\n    \"\"\"Add a model to the model mapping dict.\n    This method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes.\n\n    Args:\n      model_class (object): The model class. Must have a fit() method.\n      requires_inner_cv (bool): Whether or not the model requires inner cross validation. (Default value = False)\n      model_name (str): The name of the model. Used for logging.\n      post_processor (ModelPostProcessor): A post processor to be applied to the model. (Default value = None)\n      callbacks (Optional[Sequence[TrainingCallback]]): Callbacks to be passed to the fit method of the model in outer CV. (Default value = None)\n      kwargs: A dict of additional keyword arguments that will be passed to the model constructor.\n      fit_kwargs: A dict of keyword arguments that will be passed to the model fit method.\n      **kwargs: Arbitrary keyword arguments that will be passed to the ModelConfigDict.\n\n    Returns:\n        (CrossValidation): self\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from flexcv import CrossValidation\n        &gt;&gt;&gt; from flexcv.models import LinearModel\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; cv.add_model(LinearModel, model_name=\"LinearModel\", skip_inner_cv=True)\n        ```\n        Is equivalent to:\n        ```python\n        &gt;&gt;&gt; from flexcv import CrossValidation\n        &gt;&gt;&gt; from flexcv.models import LinearModel\n        &gt;&gt;&gt; from flexcv.model_mapping import ModelMappingDict, ModelConfigDict\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; mapping = ModelMappingDict(\n        ...     {\n        ...         \"LinearModel\": ModelConfigDict(\n        ...             {\n        ...                 \"model\": LinearModel,\n        ...                 \"skip_inner_cv\": True,\n        ...             }\n        ...         ),\n        ...     }\n        ... )\n        &gt;&gt;&gt; cv.set_models(mapping)\n        ```\n        As you can see the `add_model()` method is a convenience method to add a model to the model mapping dict without needing the ModelMappingDict and ModelConfigDict classes.\n        In cases of multiple models per run, or if you want to reuse the model mapping dict, you should look into the `ModelMappingDict` and the `set_models()` method.\n\n    \"\"\"\n\n    # check values\n    if not isinstance(model_name, str):\n        raise TypeError(\"model_name must be a string\")\n\n    if not issubclass(model_class, object):\n        raise TypeError(\"model_class must be a class\")\n\n    if not isinstance(requires_inner_cv, bool):\n        raise TypeError(\"skip_inner_cv must be a boolean\")\n\n    if params is not None and not isinstance(params, dict):\n        raise TypeError(\"params must be a dict\")\n\n    # check if post_processor is a class that inherits ModelPostProcessor object that is not instantiated\n    if post_processor and not issubclass(post_processor, ModelPostProcessor):\n        raise TypeError(\"post_processor must be a ModelPostProcessor\")\n\n    # params and kwargs may not contain the same keys\n    if kwargs is not None and not isinstance(kwargs, dict):\n        raise TypeError(\"kwargs must be a dict\")\n\n    if (\n        kwargs is not None\n        and params is not None\n        and (set(params.keys()) &amp; set(kwargs.keys()))\n    ):\n        raise ValueError(\n            \"params and additional kwargs may not contain the same keys\"\n        )\n\n    if callbacks is not None and not isinstance(callbacks, Sequence):\n        raise TypeError(\"callbacks must be a Sequence of TrainingCallbacks\")\n\n    if requires_inner_cv and params is None:\n        warnings.warn(\n            f\"You did not provide hyperparameters for the model {model_name} but set requires_inner_cv to True.\",\n            UserWarning,\n        )\n\n    if model_kwargs is not None and not isinstance(model_kwargs, dict):\n        raise TypeError(\"model_kwargs must be a dict\")\n\n    if fit_kwargs is not None and not isinstance(fit_kwargs, dict):\n        raise TypeError(\"fit_kwargs must be a dict\")\n\n    if not requires_inner_cv and params is not None and params != {}:\n        requires_inner_cv = True\n\n    if not params:\n        params = {}\n\n    if not model_name:\n        model_name = model_class.__name__\n\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n\n    if kwargs is None:\n        kwargs = {}\n\n    callbacks_dict = {}\n    if callbacks is not None:\n        callbacks_dict = {\"callbacks\": callbacks}\n\n    config_dict = {\n        \"model\": model_class,\n        \"post_processor\": post_processor,\n        \"requires_inner_cv\": requires_inner_cv,\n        \"params\": params,\n        \"callbacks\": callbacks_dict,\n        \"fit_kwargs\": fit_kwargs,\n        \"model_kwargs\": model_kwargs,\n        **kwargs,\n    }\n\n    self.config[\"mapping\"][model_name] = ModelConfigDict(config_dict)\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.get_results","title":"<code>flexcv.interface.CrossValidation.get_results()</code>","text":"<p>Returns a <code>CrossValidationResults</code> object. This results object is a wrapper class around the results dict from the <code>cross_validate</code> function.</p> Source code in <code>flexcv/interface.py</code> <pre><code>def get_results(self) -&gt; CrossValidationResults:\n    \"\"\"Returns a `CrossValidationResults` object. This results object is a wrapper class around the results dict from the `cross_validate` function.\"\"\"\n    if hasattr(self, \"results_\"):\n        return self.results_\n    else:\n        raise RuntimeError(\n            \"You must call perform() before you can get the results.\"\n        )\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.perform","title":"<code>flexcv.interface.CrossValidation.perform()</code>","text":"<p>Perform the cross validation according to the configuration passed by the user. Checks if a neptune run object has been set. If the user did not provide a neptune run object, a dummy run is instantiated. All logs and plots will be logged to the dummy run and will be lost. However, the cross validation results is created and can be returned via the <code>CrossValidation.results</code> property.</p> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Source code in <code>flexcv/interface.py</code> <pre><code>@run_padding\ndef perform(self):\n    \"\"\"Perform the cross validation according to the configuration passed by the user.\n    Checks if a neptune run object has been set. If the user did not provide a neptune run object, a dummy run is instantiated.\n    All logs and plots will be logged to the dummy run and will be lost.\n    However, the cross validation results is created and can be returned via the `CrossValidation.results` property.\n\n    Returns:\n        (CrossValidation): self\n    \"\"\"\n    self._prepare_before_perform()\n    self._log_config()\n    results = cross_validate(**self.config)\n    self._was_performed_ = True\n    self.results_ = CrossValidationResults(results)\n    self._log_results()\n    self._was_logged_ = self._config_logged_ and self._result_logged_\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_data","title":"<code>flexcv.interface.CrossValidation.set_data(X, y, groups=None, slopes=None, target_name='', dataset_name='')</code>","text":"<p>Set the data for cross validation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The features. Must not contain the target or groups.</p> required <code>y</code> <code>DataFrame | Series</code> <p>The target variable.</p> required <code>groups</code> <code>DataFrame | Series</code> <p>The grouping/clustering variable. (Default value = None)</p> <code>None</code> <code>slopes</code> <code>DataFrame | Series</code> <p>The random slopes variable(s) (Default value = None)</p> <code>None</code> <code>target_name</code> <code>str</code> <p>Customize the target's name. This string will be used in logging. (Default value = \"\")</p> <code>''</code> <code>dataset_name</code> <code>str</code> <p>Customize your datasdet's name. This string will be used in logging. (Default value = \"\")</p> <code>''</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Example <pre><code>&gt;&gt;&gt; X = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"z\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; cv.set_data(X, y)\n</code></pre> Source code in <code>flexcv/interface.py</code> <pre><code>def set_data(\n    self,\n    X: pd.DataFrame,\n    y: pd.DataFrame | pd.Series,\n    groups: pd.DataFrame | pd.Series = None,\n    slopes: pd.DataFrame | pd.Series = None,\n    target_name: str = \"\",\n    dataset_name: str = \"\",\n):\n    \"\"\"Set the data for cross validation.\n\n    Args:\n        X (pd.DataFrame): The features. Must not contain the target or groups.\n        y (pd.DataFrame | pd.Series): The target variable.\n        groups (pd.DataFrame | pd.Series): The grouping/clustering variable. (Default value = None)\n        slopes (pd.DataFrame | pd.Series): The random slopes variable(s) (Default value = None)\n        target_name (str): Customize the target's name. This string will be used in logging. (Default value = \"\")\n        dataset_name (str): Customize your datasdet's name. This string will be used in logging. (Default value = \"\")\n\n    Returns:\n        (CrossValidation): self\n\n    Example:\n        ```python\n        &gt;&gt;&gt; X = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"z\": [1, 2, 3, 4, 5]})\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; cv.set_data(X, y)\n        ```\n    \"\"\"\n    # check values\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"X must be a pandas DataFrame\")\n    if not isinstance(y, pd.DataFrame) and not isinstance(y, pd.Series):\n        raise TypeError(\"y must be a pandas DataFrame or Series\")\n    if (\n        not isinstance(groups, pd.DataFrame)\n        and not isinstance(groups, pd.Series)\n        and groups is not None\n    ):\n        raise TypeError(\"group must be a pandas DataFrame or Series\")\n    if (\n        not isinstance(slopes, pd.DataFrame)\n        and not isinstance(slopes, pd.Series)\n        and slopes is not None\n    ):\n        raise TypeError(\"slopes must be a pandas DataFrame or Series\")\n    if not isinstance(target_name, str):\n        raise TypeError(\"target_name must be a string\")\n    if not isinstance(dataset_name, str):\n        raise TypeError(\"dataset_name must be a string\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of rows\")\n    if groups is not None and X.shape[0] != groups.shape[0]:\n        raise ValueError(\"X and group must have the same number of rows\")\n    if slopes is not None:\n        if X.shape[0] != slopes.shape[0]:\n            raise ValueError(\"X and slopes must have the same number of rows\")\n        # make sure slopes (which can be multiple columns or a series) is in X\n        if isinstance(slopes, pd.Series):\n            if slopes.name not in X.columns:\n                raise ValueError(f\"{slopes.name} is not in X\")\n        elif isinstance(slopes, pd.DataFrame):\n            for slope in slopes.columns:\n                if slope not in X.columns:\n                    raise ValueError(f\"{slope} is not in X\")\n\n    # assign values\n    self.config[\"X\"] = X\n    self.config[\"y\"] = y\n    self.config[\"groups\"] = groups\n    self.config[\"slopes\"] = slopes\n    if target_name:\n        self.config[\"target_name\"] = target_name\n    else:\n        self.config[\"target_name\"] = str(y.name)\n    if dataset_name:\n        self.config[\"dataset_name\"] = dataset_name\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_inner_cv","title":"<code>flexcv.interface.CrossValidation.set_inner_cv(n_trials=100, objective_scorer=None)</code>","text":"<p>Configure parameters regarding inner cross validation and Optuna optimization.</p> <p>Parameters:</p> Name Type Description Default <code>n_trials</code> <code>int</code> <p>Number of trials to sample from the parameter distributions (Default value = 100)</p> <code>100</code> <code>objective_scorer</code> <code>ObjectiveScorer</code> <p>Callable to provide the optimization objective value. Is called during Optuna SearchCV (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_inner_cv(\n    self,\n    n_trials: int = 100,\n    objective_scorer: ObjectiveScorer = None,\n):\n    \"\"\"Configure parameters regarding inner cross validation and Optuna optimization.\n\n    Args:\n      n_trials (int): Number of trials to sample from the parameter distributions (Default value = 100)\n      objective_scorer (ObjectiveScorer): Callable to provide the optimization objective value. Is called during Optuna SearchCV (Default value = None)\n\n    Returns:\n      (CrossValidation): self\n\n    \"\"\"\n    # check values\n    if not isinstance(n_trials, int):\n        raise TypeError(\"n_trials must be an integer\")\n    if objective_scorer and not isinstance(objective_scorer, ObjectiveScorer):\n        raise TypeError(\"objective_scorer must be an ObjectiveScorer\")\n\n    # assign values\n    self.config[\"n_trials\"] = n_trials\n    self.config[\"objective_scorer\"] = objective_scorer\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_lmer","title":"<code>flexcv.interface.CrossValidation.set_lmer(predict_known_groups_lmm=True)</code>","text":"<p>Configure parameters regarding linear mixed effects regression models.</p> <p>Parameters:</p> Name Type Description Default <code>predict_known_groups_lmm</code> <code>bool</code> <p>For use with LMER, whether or not known groups should be predicted (Default value = True)</p> <code>True</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_lmer(self, predict_known_groups_lmm: bool = True):\n    \"\"\"Configure parameters regarding linear mixed effects regression models.\n\n    Args:\n      predict_known_groups_lmm (bool): For use with LMER, whether or not known groups should be predicted (Default value = True)\n\n    Returns:\n      (CrossValidation): self\n\n    \"\"\"\n    # check values\n    if not isinstance(predict_known_groups_lmm, bool):\n        raise TypeError(\"predict_known_groups_lmm must be a boolean\")\n\n    # assign values\n    self.config[\"predict_known_groups_lmm\"] = predict_known_groups_lmm\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_merf","title":"<code>flexcv.interface.CrossValidation.set_merf(add_merf_global=False, em_max_iterations=100, em_stopping_threshold=None, em_stopping_window=None)</code>","text":"<p>Configure mixed effects parameters.</p> <p>Parameters:</p> Name Type Description Default <code>add_merf_global</code> <code>bool</code> <p>If True, the model is passed into the MERF class after it is evaluated, to obtain mixed effects corrected predictions. (Default value = False)</p> <code>False</code> <code>em_max_iterations</code> <code>int</code> <p>For use with EM. Max number of iterations (Default value = 100)</p> <code>100</code> <code>em_stopping_threshold</code> <code>float</code> <p>For use with EM. Threshold of GLL residuals for early stopping (Default value = None)</p> <code>None</code> <code>em_stopping_window</code> <code>int</code> <p>For use with EM. Number of consecutive iterations to be below threshold for early stopping (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_merf(\n    self,\n    add_merf_global: bool = False,\n    em_max_iterations: int = 100,\n    em_stopping_threshold: float = None,\n    em_stopping_window: int = None,\n):\n    \"\"\"Configure mixed effects parameters.\n\n    Args:\n      add_merf_global (bool): If True, the model is passed into the MERF class after it is evaluated, to obtain mixed effects corrected predictions. (Default value = False)\n      em_max_iterations (int): For use with EM. Max number of iterations (Default value = 100)\n      em_stopping_threshold (float): For use with EM. Threshold of GLL residuals for early stopping (Default value = None)\n      em_stopping_window (int): For use with EM. Number of consecutive iterations to be below threshold for early stopping (Default value = None)\n\n\n    Returns:\n      (CrossValidation): self\n\n    \"\"\"\n    if not isinstance(add_merf_global, bool):\n        raise TypeError(\"add_merf must be a boolean\")\n    if not isinstance(em_max_iterations, int):\n        raise TypeError(\"em_max_iterations must be an integer\")\n    if em_stopping_threshold and not isinstance(em_stopping_threshold, float):\n        raise TypeError(\"em_stopping_threshold must be a float\")\n    if em_stopping_window and not isinstance(em_stopping_window, int):\n        raise TypeError(\"em_stopping_window must be an integer\")\n\n    # assign values\n    self.config[\"add_merf_global\"] = add_merf_global\n    self.config[\"em_max_iterations\"] = em_max_iterations\n    self.config[\"em_stopping_threshold\"] = em_stopping_threshold\n    self.config[\"em_stopping_window\"] = em_stopping_window\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_models","title":"<code>flexcv.interface.CrossValidation.set_models(mapping=None, yaml_path=None, yaml_string=None)</code>","text":"<p>Set your models and related parameters. Pass a ModelMappingDict or pass yaml code or a path to a yaml file. The mapping attribute of the class is a ModelMappingDict that contains a ModelConfigDict for each model. The class attribute self.config[\"mapping\"] is always updated in this method.  Therefore, you can call this method multiple times to add models to the mapping. You can also call set_models() with a ModelMappingDict and then call set_models() again with yaml code or a path to a yaml file or after you already called add_models().</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>ModelMappingDict[str, ModelConfigDict]</code> <p>Dict of model names and model configurations. See ModelMappingDict for more information. (Default value = None)</p> <code>None</code> <code>yaml_path</code> <code>str | Path</code> <p>Path to a yaml file containing a model mapping. See flexcv.yaml_parser for more information. (Default value = None)</p> <code>None</code> <code>yaml_string</code> <code>str</code> <p>String containing yaml code. See flexcv.yaml_parser for more information. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Example <p>In your yaml file: <pre><code>RandomForest:\n    model: sklearn.ensemble.RandomForestRegressor\n    post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n    requires_inner_cv: True\n    params:\n        max_depth: !Int\n            low: 1\n            high: 10\n</code></pre> In your code: <pre><code>&gt;&gt;&gt; from flexcv import CrossValidation\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; cv.set_models(yaml_path=\"path/to/your/yaml/file\")\n</code></pre> This will automatically read the yaml file and create a ModelMappingDict. It even takes care of the imports and instantiates the classes of model, postprocessor and for the optune distributions.</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_models(\n    self,\n    mapping: ModelMappingDict = None,\n    yaml_path: str | pathlib.Path = None,\n    yaml_string: str = None,\n):\n    \"\"\"Set your models and related parameters. Pass a ModelMappingDict or pass yaml code or a path to a yaml file.\n    The mapping attribute of the class is a ModelMappingDict that contains a ModelConfigDict for each model.\n    The class attribute self.config[\"mapping\"] is always updated in this method. \n    Therefore, you can call this method multiple times to add models to the mapping.\n    You can also call set_models() with a ModelMappingDict and then call set_models() again with yaml code or a path to a yaml file or after you already called add_models().\n\n\n    Args:\n      mapping (ModelMappingDict[str, ModelConfigDict]): Dict of model names and model configurations. See ModelMappingDict for more information. (Default value = None)\n      yaml_path (str | pathlib.Path): Path to a yaml file containing a model mapping. See flexcv.yaml_parser for more information. (Default value = None)\n      yaml_string (str): String containing yaml code. See flexcv.yaml_parser for more information. (Default value = None)\n\n    Returns:\n      (CrossValidation): self\n\n    Example:\n        In your yaml file:\n        ```yaml\n        RandomForest:\n            model: sklearn.ensemble.RandomForestRegressor\n            post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n            requires_inner_cv: True\n            params:\n                max_depth: !Int\n                    low: 1\n                    high: 10\n        ```\n        In your code:\n        ```python\n        &gt;&gt;&gt; from flexcv import CrossValidation\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; cv.set_models(yaml_path=\"path/to/your/yaml/file\")\n        ```\n        This will automatically read the yaml file and create a ModelMappingDict.\n        It even takes care of the imports and instantiates the classes of model, postprocessor and for the optune distributions.\n    \"\"\"\n    if not any((mapping, yaml_path, yaml_string)):\n        raise ValueError(\n            \"You must provide either mapping, yaml_path, or yaml_string\"\n        )\n\n    if sum(bool(x) for x in (mapping, yaml_path, yaml_string)) &gt; 1:\n        raise ValueError(\n            \"You must provide either mapping, yaml_path or yaml_string, not multiple\"\n        )\n\n    if mapping is not None:\n        if not isinstance(mapping, ModelMappingDict):\n            raise TypeError(\"mapping must be a ModelMappingDict\")\n        self.config[\"mapping\"].update(mapping)\n\n    elif yaml_path is not None:\n        if not isinstance(yaml_path, str) and not isinstance(\n            yaml_path, pathlib.Path\n        ):\n            raise TypeError(\"yaml_path must be a string or pathlib.Path\")\n        self.config[\"mapping\"].update(read_mapping_from_yaml_file(yaml_path))\n\n    elif yaml_string is not None:\n        if not isinstance(yaml_string, str):\n            raise TypeError(\"yaml_string must be a string\")\n        self.config[\"mapping\"].update(read_mapping_from_yaml_string(yaml_string))\n\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_run","title":"<code>flexcv.interface.CrossValidation.set_run(run=None, diagnostics=False, random_seed=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>run</code> <code>Run</code> <p>The run object to use for logging (Default value = None)</p> <code>None</code> <code>diagnostics</code> <code>bool</code> <p>If True, extended diagnostic plots are logged (Default value = False)</p> <code>False</code> <code>random_seed</code> <code>int</code> <p>Seed for random processes (Default value = 42)</p> <code>42</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_run(\n    self,\n    run: NeptuneRun = None,\n    diagnostics: bool = False,\n    random_seed: int = 42,\n):\n    \"\"\"\n\n    Args:\n      run (NeptuneRun): The run object to use for logging (Default value = None)\n      diagnostics (bool): If True, extended diagnostic plots are logged (Default value = False)\n      random_seed (int): Seed for random processes (Default value = 42)\n\n    Returns:\n        (CrossValidation): self\n\n    \"\"\"\n    # check values\n    if run and not isinstance(run, NeptuneRun):\n        raise TypeError(\"run must be a NeptuneRun\")\n    if not isinstance(diagnostics, bool):\n        raise TypeError(\"diagnostics must be a boolean\")\n    if not isinstance(random_seed, int):\n        raise TypeError(\n            \"random_seed is not 42 hahaha. No seriously, random_seed must be an integer\"\n        )\n\n    # assign values\n    self.config[\"run\"] = run\n    self.config[\"diagnostics\"] = diagnostics\n    self.config[\"random_seed\"] = random_seed\n    return self\n</code></pre>"},{"location":"reference/interface/#flexcv.interface.CrossValidation.set_splits","title":"<code>flexcv.interface.CrossValidation.set_splits(split_out=CrossValMethod.KFOLD, split_in=CrossValMethod.KFOLD, n_splits_out=5, n_splits_in=5, scale_out=True, scale_in=True, break_cross_val=False, metrics=None)</code>","text":"<p>Set the cross validation strategy. Set the split method simply by passing the <code>CrossValMethod</code> as a string or enum value. Passing as string might be more convenient for you but could lead to typos. When passing as string, the string must be a valid value of the <code>CrossValMethod</code> enum. See the reference for <code>CrossValMethod</code> for more details.</p> <p>Valid strings for <code>split_out</code> and <code>split_in</code>:     - \"KFold\"     - \"StratifiedKFold\"     - \"CustomStratifiedKFold\"     - \"GroupKFold\"     - \"StratifiedGroupKFold\"     - \"CustomStratifiedGroupKFold\"</p> <p>Parameters:</p> Name Type Description Default <code>split_out</code> <code>str | CrossValMethod</code> <p>Outer split method. (Default value = CrossValMethod.KFOLD)</p> <code>KFOLD</code> <code>split_in</code> <code>str | CrossValMethod</code> <p>Inner split method for hyperparameter tuning. (Default value = CrossValMethod.KFOLD)</p> <code>KFOLD</code> <code>n_splits_out</code> <code>int</code> <p>Number of splits in outer loop. (Default value = 5)</p> <code>5</code> <code>n_splits_in</code> <code>int</code> <p>Number of splits in inner loop. (Default value = 5)</p> <code>5</code> <code>scale_out</code> <code>bool</code> <p>Whether or not the Features of the outer loop will be scaled to mean 0 and variance 1. (Default value = True)</p> <code>True</code> <code>scale_in</code> <code>bool</code> <p>Whether or not the Features of the inner loop will be scaled to mean 0 and variance 1. (Default value = True)</p> <code>True</code> <code>break_cross_val</code> <code>bool</code> <p>If True, the outer loop we break after first iteration. Use for debugging. (Default value = False)</p> <code>False</code> <code>metrics</code> <code>MetricsDict</code> <p>A dict containing evaluation metrics for the outer loop results. See MetricsDict for details. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>CrossValidation</code> <p>self</p> Example <p>Passing the method as instance of CrossValMethod: <pre><code>&gt;&gt;&gt; from flexcv import CrossValidation, CrossValMethod\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; cv.set_splits(split_out=CrossValMethod.KFOLD, split_in=CrossValMethod.KFOLD)\n</code></pre> Passing the method as a string: <pre><code>&gt;&gt;&gt; from flexcv import CrossValidation\n&gt;&gt;&gt; cv = CrossValidation()\n&gt;&gt;&gt; cv.set_splits(split_out=\"KFold\", split_in=\"KFold\")\n# Valid strings: \"KFold\", \"StratifiedKFold\", \"CustomStratifiedKFold\", \"GroupKFold\", \"StratifiedGroupKFold\", \"CustomStratifiedGroupKFold\"\n</code></pre></p> Split methods <p>The split strategy is controlled by the <code>split_out</code> and <code>split_in</code> arguments. You can pass the actual <code>CrossValMethod</code> enum or a string.</p> <p>The <code>split_out</code> argument controls the fold assignment in the outer cross validation loop. In each outer loop the model is fit on the training fold and model performance is evaluated on unseen data of the test fold. The <code>split_in</code> argument controls the inner loop split strategy. The inner loop cross validates the hyperparameters of the model. A model is typically built by sampling from a distribution of hyperparameters. It is fit on the inner training fold and evaluated on the inner test fold. Of course, the inner loop is nested in the outer loop, so the inner split is performed on the outer training fold.</p> <p>Read more about it in the respective documentation of the <code>CrossValMethod</code> enum.</p> Source code in <code>flexcv/interface.py</code> <pre><code>def set_splits(\n    self,\n    split_out: str\n    | CrossValMethod\n    | BaseCrossValidator\n    | Iterator = CrossValMethod.KFOLD,\n    split_in: str\n    | CrossValMethod\n    | BaseCrossValidator\n    | Iterator = CrossValMethod.KFOLD,\n    n_splits_out: int = 5,\n    n_splits_in: int = 5,\n    scale_out: bool = True,\n    scale_in: bool = True,\n    break_cross_val: bool = False,\n    metrics: MetricsDict = None,\n):\n    \"\"\"Set the cross validation strategy.\n    Set the split method simply by passing the `CrossValMethod` as a string or enum value. Passing as string might be more convenient for you but could lead to typos.\n    When passing as string, the string must be a valid value of the `CrossValMethod` enum.\n    See the reference for `CrossValMethod` for more details.\n\n    Valid strings for `split_out` and `split_in`:\n        - \"KFold\"\n        - \"StratifiedKFold\"\n        - \"CustomStratifiedKFold\"\n        - \"GroupKFold\"\n        - \"StratifiedGroupKFold\"\n        - \"CustomStratifiedGroupKFold\"\n\n    Args:\n        split_out (str | CrossValMethod): Outer split method. (Default value = CrossValMethod.KFOLD)\n        split_in (str | CrossValMethod): Inner split method for hyperparameter tuning. (Default value = CrossValMethod.KFOLD)\n        n_splits_out (int): Number of splits in outer loop. (Default value = 5)\n        n_splits_in (int): Number of splits in inner loop. (Default value = 5)\n        scale_out (bool): Whether or not the Features of the outer loop will be scaled to mean 0 and variance 1. (Default value = True)\n        scale_in (bool): Whether or not the Features of the inner loop will be scaled to mean 0 and variance 1. (Default value = True)\n        break_cross_val (bool): If True, the outer loop we break after first iteration. Use for debugging. (Default value = False)\n        metrics (MetricsDict): A dict containing evaluation metrics for the outer loop results. See MetricsDict for details. (Default value = None)\n\n    Returns:\n      (CrossValidation): self\n\n    Example:\n        Passing the method as instance of CrossValMethod:\n        ```python\n        &gt;&gt;&gt; from flexcv import CrossValidation, CrossValMethod\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; cv.set_splits(split_out=CrossValMethod.KFOLD, split_in=CrossValMethod.KFOLD)\n        ```\n        Passing the method as a string:\n        ```python\n        &gt;&gt;&gt; from flexcv import CrossValidation\n        &gt;&gt;&gt; cv = CrossValidation()\n        &gt;&gt;&gt; cv.set_splits(split_out=\"KFold\", split_in=\"KFold\")\n        # Valid strings: \"KFold\", \"StratifiedKFold\", \"CustomStratifiedKFold\", \"GroupKFold\", \"StratifiedGroupKFold\", \"CustomStratifiedGroupKFold\"\n        ```\n\n\n    Split methods:\n        The split strategy is controlled by the `split_out` and `split_in` arguments. You can pass the actual `CrossValMethod` enum or a string.\n\n        The `split_out` argument controls the fold assignment in the outer cross validation loop.\n        In each outer loop the model is fit on the training fold and model performance is evaluated on unseen data of the test fold.\n        The `split_in` argument controls the inner loop split strategy. The inner loop cross validates the hyperparameters of the model.\n        A model is typically built by sampling from a distribution of hyperparameters. It is fit on the inner training fold and evaluated on the inner test fold.\n        Of course, the inner loop is nested in the outer loop, so the inner split is performed on the outer training fold.\n\n        Read more about it in the respective documentation of the `CrossValMethod` enum.\n\n    \"\"\"\n\n    # get values of CrossValMethod enums\n    ALLOWED_STRINGS = [method.value for method in CrossValMethod]\n    ALLOWED_METHODS = [method for method in CrossValMethod]\n\n    if isinstance(split_out, str) and (split_out not in ALLOWED_STRINGS):\n        raise TypeError(\n            f\"split_out must be a valid CrossValMethod name, was {split_out}. Choose from: \"\n            + \", \".join(ALLOWED_STRINGS)\n            + \".\"\n        )\n\n    if isinstance(split_in, str) and (split_in not in ALLOWED_STRINGS):\n        raise TypeError(\n            f\"split_in must be a valid CrossValMethod name, was {split_in}. Choose from: \"\n            + \", \".join(ALLOWED_STRINGS)\n            + \".\"\n        )\n\n    if not any(\n        [\n            isinstance(split_out, str),\n            isinstance(split_out, CrossValMethod),\n            isinstance(split_out, BaseCrossValidator),\n            isinstance(split_out, Iterator),\n        ]\n    ):\n        raise TypeError(\n            \"split_out must be of Type str, CrossValMethod, BaseCrossValidator or Iterator.\"\n        )\n\n    if not any(\n        [\n            isinstance(split_in, str),\n            isinstance(split_in, CrossValMethod),\n            isinstance(split_in, BaseCrossValidator),\n            isinstance(split_in, Iterator),\n        ]\n    ):\n        raise TypeError(\n            \"split_in must be of Type str, CrossValMethod, BaseCrossValidator or Iterator.\"\n        )\n\n    if isinstance(split_out, CrossValMethod) and (split_out not in ALLOWED_METHODS):\n        raise TypeError(\"split_out must be a valid CrossValMethod \")\n\n    if isinstance(split_in, CrossValMethod) and (split_in not in ALLOWED_METHODS):\n        raise TypeError(\"split_in must be a valid CrossValMethod\")\n\n    if not isinstance(n_splits_out, int):\n        raise TypeError(\"n_splits_out must be an integer\")\n    if not isinstance(n_splits_in, int):\n        raise TypeError(\"n_splits_in must be an integer\")\n\n    if not isinstance(scale_in, bool):\n        raise TypeError(\"scale_in must be a boolean\")\n    if not isinstance(scale_out, bool):\n        raise TypeError(\"scale_out must be a boolean\")\n\n    if not isinstance(break_cross_val, bool):\n        raise TypeError(\"break_cross_val must be a boolean\")\n    if metrics and not isinstance(metrics, MetricsDict):\n        raise TypeError(\"metrics must be a MetricsDict\")\n\n    if isinstance(split_out, str):\n        split_out = string_to_crossvalmethod(split_out)\n    if isinstance(split_in, str):\n        split_in = string_to_crossvalmethod(split_in)\n    # assign values\n    self.config[\"split_out\"] = split_out\n    self.config[\"split_in\"] = split_in\n    self.config[\"n_splits_out\"] = n_splits_out\n    self.config[\"n_splits_in\"] = n_splits_in\n    self.config[\"scale_in\"] = scale_in\n    self.config[\"scale_out\"] = scale_out\n    self.config[\"break_cross_val\"] = break_cross_val\n    if metrics:\n        self.config[\"metrics\"] = metrics\n    return self\n</code></pre>"},{"location":"reference/log/","title":"Logging","text":""},{"location":"reference/log/#flexcv.fold_logging","title":"<code>flexcv.fold_logging</code>","text":"<p>This module contains functions for logging results to Neptune.ai. The functions are called during performing cross validation and are used to construct the results metrics dict.</p>"},{"location":"reference/log/#flexcv.fold_logging.CustomNeptuneCallback","title":"<code>flexcv.fold_logging.CustomNeptuneCallback</code>","text":"<p>               Bases: <code>NeptuneCallback</code></p> <p>This class inherits from NeptuneCallback and overrides the call method. The call method is called after each trial and logs the best trial and the plots. The override is necessary because logging each trial is not feasible for multiple models, folds and trials. It would hit Neptune's namespace limits.</p> Source code in <code>flexcv/fold_logging.py</code> <pre><code>class CustomNeptuneCallback(npt_utils.NeptuneCallback):\n    \"\"\"This class inherits from NeptuneCallback and overrides the __call__ method.\n    The __call__ method is called after each trial and logs the best trial and the plots.\n    The override is necessary because logging each trial is not feasible for multiple models, folds and trials.\n    It would hit Neptune's namespace limits.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, study, trial):\n        \"\"\"Logs only the best trial and the plots.\n        Args:\n          study (optuna.study): Optuna study object.\n          trial (optuna.trial): Optuna trial object.\n\n        Returns:\n          (None)\n        \"\"\"\n        self._log_best_trials(study)\n        self._log_plots(study, trial)\n</code></pre>"},{"location":"reference/log/#flexcv.fold_logging.CustomNeptuneCallback.__call__","title":"<code>flexcv.fold_logging.CustomNeptuneCallback.__call__(study, trial)</code>","text":"<p>Logs only the best trial and the plots. Args:   study (optuna.study): Optuna study object.   trial (optuna.trial): Optuna trial object.</p> <p>Returns:</p> Type Description <p>(None)</p> Source code in <code>flexcv/fold_logging.py</code> <pre><code>def __call__(self, study, trial):\n    \"\"\"Logs only the best trial and the plots.\n    Args:\n      study (optuna.study): Optuna study object.\n      trial (optuna.trial): Optuna trial object.\n\n    Returns:\n      (None)\n    \"\"\"\n    self._log_best_trials(study)\n    self._log_plots(study, trial)\n</code></pre>"},{"location":"reference/log/#flexcv.fold_logging.log_diagnostics","title":"<code>flexcv.fold_logging.log_diagnostics(X_train, X_test, y_train, y_test, run, effects, cluster_train=None, cluster_test=None, namestring='out')</code>","text":"<p>Logs histograms of the features and target for diagnostic purposes to neptune.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Training features.</p> required <code>X_test</code> <code>DataFrame</code> <p>Testing features.</p> required <code>y_train</code> <code>Series</code> <p>Training target.</p> required <code>y_test</code> <code>Series</code> <p>Testing target.</p> required <code>run</code> <p>Neptune run object.</p> required <code>effects</code> <code>str</code> <p>Type of effects to be used. Either \"fixed\" or \"mixed\".</p> required <code>cluster_train</code> <code>Series</code> <p>Training clustering or grouping variable. Defaults to None.</p> <code>None</code> <code>cluster_test</code> <code>Series</code> <p>Testing clustering or grouping variable. Defaults to None.</p> <code>None</code> <code>namestring</code> <code>str</code> <p>A string to pass to logging. Use to separate folds: use \"in\" or \"out\". Defaults to \"out\".</p> <code>'out'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>flexcv/fold_logging.py</code> <pre><code>def log_diagnostics(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    run,\n    effects: str,\n    cluster_train: pd.Series = None,\n    cluster_test: pd.Series = None,\n    namestring: str = \"out\",\n) -&gt; None:\n    \"\"\"Logs histograms of the features and target for diagnostic purposes to neptune.\n\n    Args:\n        X_train (pd.DataFrame): Training features.\n        X_test (pd.DataFrame): Testing features.\n        y_train (pd.Series): Training target.\n        y_test (pd.Series): Testing target.\n        run: Neptune run object.\n        effects (str): Type of effects to be used. Either \"fixed\" or \"mixed\".\n        cluster_train (pd.Series, optional): Training clustering or grouping variable. Defaults to None.\n        cluster_test (pd.Series, optional): Testing clustering or grouping variable. Defaults to None.\n        namestring (str, optional): A string to pass to logging. Use to separate folds: use \"in\" or \"out\". Defaults to \"out\".\n\n    Returns:\n        None\n    \"\"\"\n    # function definitions\n\n    def get_df_hist_fig(df: pd.DataFrame) -&gt; plt.Figure:\n        \"\"\"Generates a histogram for each column in the dataframe.\n\n        Args:\n            df (pd.DataFrame): The data to plot histograms for.\n\n        Returns:\n            plt.Figure: The figure object containing the histograms as subplots.\n        \"\"\"\n        fig, axes = plt.subplots(len(df.columns), 1, figsize=(5, 15))\n        ax = axes.flatten()\n\n        for i, col in enumerate(df.columns):\n            sns.histplot(df[col], ax=ax[i])  # histogram call\n            ax[i].set_title(col)\n            # remove scientific notation for both axes\n            ax[i].ticklabel_format(style=\"plain\", axis=\"both\")\n        return fig\n\n    def get_series_hist_fig(ser: pd.Series) -&gt; plt.Figure:\n        \"\"\"Get a histogram for a series.\n\n        Args:\n            ser (pd.Series): The data to plot a histogram for.\n\n        Returns:\n            plt.Figure: The figure object containing the histogram.\n        \"\"\"\n        fig, ax = plt.subplots()\n        sns.histplot(ser, ax=ax)\n        return fig\n\n    # log number of samples in each fold\n    run[f\"diagnostics/fold_{namestring}/n_samples_train\"].append(len(X_train))\n    run[f\"diagnostics/fold_{namestring}/n_samples_test\"].append(len(X_test))\n\n    fig = get_df_hist_fig(X_train)\n    run[f\"diagnostics/fold_{namestring}/train_histograms/X\"].append(fig)\n    del fig\n    plt.close()\n    fig = get_series_hist_fig(y_train)\n    run[f\"diagnostics/fold_{namestring}/train_histograms/y\"].append(fig)\n    del fig\n    plt.close()\n    fig = get_df_hist_fig(X_test)\n    run[f\"diagnostics/fold_{namestring}/test_histograms/X\"].append(fig)\n    del fig\n    plt.close()\n    fig = get_series_hist_fig(y_test)\n    run[f\"diagnostics/fold_{namestring}/test_histograms/y\"].append(fig)\n    del fig\n    plt.close()\n\n    # log groups in each fold\n    if effects == \"mixed\":\n        run[f\"diagnostics/fold_{namestring}/groups_train\"].append(\n            str(cluster_train.unique().tolist())\n        )\n        run[f\"diagnostics/fold_{namestring}/groups_test\"].append(\n            str(cluster_test.unique().tolist())\n        )\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":""},{"location":"reference/metrics/#flexcv.metrics","title":"<code>flexcv.metrics</code>","text":"<p>This module implements the MetricsDict class and it's default metrics. It is used to specify which metrics to calculate in the outer loop of the cross-validation.</p>"},{"location":"reference/metrics/#flexcv.metrics.MetricsDict","title":"<code>flexcv.metrics.MetricsDict</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary that maps metric names to functions. It can be passed to the cross_validate function to specify which metrics to calculate in the outer loop.</p> Default Metrics <p>By default, the following metrics are initialized:</p> <ul> <li> <p>R\u00b2: The coefficient of determination</p> </li> <li> <p>MSE: Mean squared error</p> </li> <li> <p>MAE: Mean absolute error</p> </li> </ul> <p>We decided againt using the RMSE as a default metric, because we would run into trouble wherever we would average over it. RMSE should always be averaged as <code>sqrt(mean(MSE_values))</code> and not as <code>mean(sqrt(MSE_values))</code>. Also, the standard deviation would be calculated incorrectly if RMSE is included at this point.</p> <p>Parameters:</p> Name Type Description Default <code>(dict)</code> <p>A dictionary that maps metric names to functions.</p> required Example <pre><code>from flexcv.metrics import MetricsDict\n\ndef naive_metric(valid, pred):\n    return 42\n\n# instantiate a MetricsDict with the default metrics R\u00b2, MSE and MAE\nmetrics = MetricsDict()\n# add a custom metric\nmetrics[\"naive_metric\"] = naive_metric\n</code></pre> Source code in <code>flexcv/metrics.py</code> <pre><code>class MetricsDict(dict):\n    \"\"\"A dictionary that maps metric names to functions.\n    It can be passed to the cross_validate function to specify which metrics to calculate in the outer loop.\n\n    Default Metrics:\n        By default, the following metrics are initialized:\n\n        - R\u00b2: The coefficient of determination\n\n        - MSE: Mean squared error\n\n        - MAE: Mean absolute error\n\n        We decided againt using the RMSE as a default metric, because we would run into trouble wherever we would average over it.\n        RMSE should always be averaged as `sqrt(mean(MSE_values))` and not as `mean(sqrt(MSE_values))`.\n        Also, the standard deviation would be calculated incorrectly if RMSE is included at this point.\n\n    Parameters:\n        (dict): A dictionary that maps metric names to functions.\n\n    Example:\n        ```python\n        from flexcv.metrics import MetricsDict\n\n        def naive_metric(valid, pred):\n            return 42\n\n        # instantiate a MetricsDict with the default metrics R\u00b2, MSE and MAE\n        metrics = MetricsDict()\n        # add a custom metric\n        metrics[\"naive_metric\"] = naive_metric\n        ```\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self[\"r2\"] = r2_score\n        self[\"mse\"] = mean_squared_error\n        self[\"mae\"] = mean_absolute_error\n</code></pre>"},{"location":"reference/metrics/#flexcv.metrics.mse_wrapper","title":"<code>flexcv.metrics.mse_wrapper(y_valid, y_pred, y_train_in, y_pred_train)</code>","text":"<p>This function is only used to calculate the objective function value for the hyperparameter optimization. In order to allow for customized objective functions it takes the validation and training data and the corresponding predictions as arguments. This can be useful to avoid overfitting. The sklearn MSE function had to be wrapped accordingly</p> <p>Parameters:</p> Name Type Description Default <code>y_valid</code> <code>array - like</code> <p>Target in the validation set.</p> required <code>y_pred</code> <code>array - like</code> <p>Predictions for the validation set.</p> required <code>y_train_in</code> <code>array - like</code> <p>Target in the training set.</p> required <code>y_pred_train</code> <code>array - like</code> <p>Predictions for the training set.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean squared error.</p> Source code in <code>flexcv/metrics.py</code> <pre><code>def mse_wrapper(y_valid, y_pred, y_train_in, y_pred_train):\n    \"\"\"This function is only used to calculate the objective function value for the hyperparameter optimization.\n    In order to allow for customized objective functions it takes the validation and training data and the corresponding predictions as arguments.\n    This can be useful to avoid overfitting. The sklearn MSE function had to be wrapped accordingly\n\n    Args:\n      y_valid (array-like): Target in the validation set.\n      y_pred (array-like): Predictions for the validation set.\n      y_train_in (array-like): Target in the training set.\n      y_pred_train (array-like): Predictions for the training set.\n\n    Returns:\n        (float): Mean squared error.\n\n    \"\"\"\n    return mean_squared_error(y_valid, y_pred)\n</code></pre>"},{"location":"reference/model-mapping/","title":"Model Mapping","text":""},{"location":"reference/model-mapping/#flexcv.model_mapping","title":"<code>flexcv.model_mapping</code>","text":"<p>This module contains the ModelConfigDict and ModelMappingDict classes. They can be used to construct a mapping which can be passed to flexcv.interface.CrossValidation Usage:     <pre><code>model_mapping = ModelMappingDict({\n    \"ModelName1\": ModelConfigDict(\n        {\n            \"requires_inner_cv\": False,\n            \"n_trials\": 100,\n            \"n_jobs_model\": 1,\n            \"n_jobs_cv\": 1,\n            \"model\": model_1,\n            \"params\": {},\n            \"post_processor\": model_1_post_func,\n            \"mixed_model\": mixed_model_1,\n            \"mixed_post_processor\": mixed_model_1_post_func,\n            \"mixed_name\": \"MixedModel1\"\n        }\n    ),\n\n    \"ModelName2\": ModelConfigDict(\n        {\n            \"requires_inner_cv\": True,\n            \"n_trials\": 100,\n            \"n_jobs_model\": 1,\n            \"n_jobs_cv\": 1,\n            \"model\": model_2,\n            \"params\": {},\n            \"post_processor\": model_2_post_func,\n            \"mixed_model\": mixed_model_2,\n            \"mixed_post_processor\": mixed_model_2_post_func,\n            \"mixed_name\": \"MixedModel2\"\n        },\n    ),\n}\n)\n</code></pre></p>"},{"location":"reference/model-mapping/#flexcv.model_mapping.ModelConfigDict","title":"<code>flexcv.model_mapping.ModelConfigDict</code>","text":"<p>               Bases: <code>Dict[str, Type]</code></p> <p>A dictionary that maps model configuration names to their corresponding types.</p> Default Values <p>To make working with this custom Dict-like class easy, we re-implemented the init method to set some default key-value pairs for us. If you don't pass them, it will set</p> <ul> <li>requires_inner_cv = False</li> <li>n_jobs_model = 1</li> <li>n_jobs_cv = 1</li> <li>params = {}</li> </ul> Usage <p><pre><code>    {  # TODO update this to correct defaults\n        \"requires_inner_cv\": bool,\n                # this flag can be set to control if a model is used in the inner cross validation.\n                # if set to False, the model will be instantiated in the outer cross validation without hyper parameter optimization.\n        \"n_trials\": int,\n                # number of trials to be used in hyper parameter optimization.\n        \"n_jobs_model\": 1,\n                # number of jobs to be used in the model. We use the sklearn convention here. We use the sklearn convention here.\n                # If your model does not support n_jobs, you can pass False here.\n        \"n_jobs_cv\": 1,\n                # number of jobs to be used in the inner cross validation/hyper parameter tuning. We use the sklearn convention here.\n        \"model\": BaseEstimator,\n                # pass your sklearn model here. It must be a class, not an instance.\n        \"params\": {},\n                # pass the parameters to be used in the model here. It must be a dictionary of optuna distributions or an empty dict.\n        \"post_processor\": flexcv.model_postprocessing.ModelPostProcessor,\n                # pass the post processor class to be used here. It must inherit from the flexcv.model_postprocessing.ModelPostProcessor abstract base class.\n    }\n</code></pre> See also:     For information on possible optuna distributions, see:     https://optuna.readthedocs.io/en/stable/reference/distributions.html</p> Source code in <code>flexcv/model_mapping.py</code> <pre><code>class ModelConfigDict(Dict[str, Type]):\n    \"\"\"A dictionary that maps model configuration names to their corresponding types.\n\n    Default Values:\n        To make working with this custom Dict-like class easy, we re-implemented the __init__ method to set some default key-value pairs for us.\n        If you don't pass them, it will set\n\n        - requires_inner_cv = False\n        - n_jobs_model = 1\n        - n_jobs_cv = 1\n        - params = {}\n\n    Usage:\n        ```python\n            {  # TODO update this to correct defaults\n                \"requires_inner_cv\": bool,\n                        # this flag can be set to control if a model is used in the inner cross validation.\n                        # if set to False, the model will be instantiated in the outer cross validation without hyper parameter optimization.\n                \"n_trials\": int,\n                        # number of trials to be used in hyper parameter optimization.\n                \"n_jobs_model\": 1,\n                        # number of jobs to be used in the model. We use the sklearn convention here. We use the sklearn convention here.\n                        # If your model does not support n_jobs, you can pass False here.\n                \"n_jobs_cv\": 1,\n                        # number of jobs to be used in the inner cross validation/hyper parameter tuning. We use the sklearn convention here.\n                \"model\": BaseEstimator,\n                        # pass your sklearn model here. It must be a class, not an instance.\n                \"params\": {},\n                        # pass the parameters to be used in the model here. It must be a dictionary of optuna distributions or an empty dict.\n                \"post_processor\": flexcv.model_postprocessing.ModelPostProcessor,\n                        # pass the post processor class to be used here. It must inherit from the flexcv.model_postprocessing.ModelPostProcessor abstract base class.\n            }\n        ```\n        See also:\n            For information on possible optuna distributions, see:\n            https://optuna.readthedocs.io/en/stable/reference/distributions.html\n    \"\"\"\n\n    def __init__(self, mapping=None):\n        if mapping is None:\n            mapping = {}\n        super().__init__(mapping)\n        self._set_defaults()\n\n    def _set_defaults(self) -&gt; None:\n        \"\"\"Sets default values for the model configuration dict. This allows us to use the dict without having to pass all the keys every time.\"\"\"\n        # check if dict key exists, if not, set default value\n        self._check_key_set_default(\"requires_inner_cv\", False)\n        self._check_key_set_default(\"n_jobs_model\", -1)\n        self._check_key_set_default(\"n_jobs_cv\", -1)\n        self._check_key_set_default(\"params\", {})\n\n    def _has_key(self, key) -&gt; bool:\n        \"\"\"Method to check if a key exists in the dict.\n\n        Args:\n          key (str | int) : The key to check for.\n\n        Returns:\n          (bool): True if the key exists, False otherwise.\n        \"\"\"\n        return key in self.keys()\n\n    def _check_key_set_default(self, key, default) -&gt; None:\n        \"\"\"Checks if a key exists in the dict and sets a default value if it doesn't.\n\n        Args:\n          key (str | int): The key to check for.\n          default (str | int):  The default value to set if the key doesn't exist.\n\n        Returns:\n          (None)\n        \"\"\"\n        if not self._has_key(key):\n            self[key] = default\n</code></pre>"},{"location":"reference/model-mapping/#flexcv.model_mapping.ModelMappingDict","title":"<code>flexcv.model_mapping.ModelMappingDict</code>","text":"<p>               Bases: <code>Dict[str, ModelConfigDict]</code></p> <p>A dictionary that maps model names to  model configuration dicts. Usage:     <pre><code>model_mapping = ModelMappingDict({\n    \"LinearModel\": ModelConfigDict(\n        {...}\n    ),\n    \"SecondModel\": ModelConfigDict(\n        {...}\n    ),\n    )\n</code></pre></p> Source code in <code>flexcv/model_mapping.py</code> <pre><code>class ModelMappingDict(Dict[str, ModelConfigDict]):\n    \"\"\"A dictionary that maps model names to  model configuration dicts.\n    Usage:\n        ```python\n        model_mapping = ModelMappingDict({\n            \"LinearModel\": ModelConfigDict(\n                {...}\n            ),\n            \"SecondModel\": ModelConfigDict(\n                {...}\n            ),\n            )\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/model-selection/","title":"Model Selection","text":""},{"location":"reference/model-selection/#flexcv.model_selection","title":"<code>flexcv.model_selection</code>","text":"<p>This module implements customization of the objective function for the hyperparameter optimization. In order to use a custom objective function, we implemented the inner cv loop as follows (pseudo code):</p> <pre><code>objective_cv(\n    if n_jobs == -1:\n        parallel_objective(some_kind_of_scorer)\n    else:\n        objective(some_king_of_scorer)\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.ObjectiveScorer","title":"<code>flexcv.model_selection.ObjectiveScorer</code>","text":"<p>               Bases: <code>Callable[[ndarray, ndarray, ndarray, ndarray], float]</code></p> <p>Callable class that wraps a scorer function to be used as an objective function. The scorer function must match the following signature. Instantiating the class will check the signature.</p> <p>Parameters:</p> Name Type Description Default <code>y_valid</code> <code>ndarray</code> <p>The validation target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted target values.</p> required <code>y_train_in</code> <code>ndarray</code> <p>The training target values.</p> required <code>y_pred_train</code> <code>ndarray</code> <p>The predicted training target values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The objective function value.</p> Source code in <code>flexcv/model_selection.py</code> <pre><code>class ObjectiveScorer(\n    Callable[[np.ndarray, np.ndarray, np.ndarray, np.ndarray], float]\n):\n    \"\"\"Callable class that wraps a scorer function to be used as an objective function.\n    The scorer function must match the following signature. Instantiating the class will check the signature.\n\n    Args:\n        y_valid (np.ndarray): The validation target values.\n        y_pred (np.ndarray): The predicted target values.\n        y_train_in (np.ndarray): The training target values.\n        y_pred_train (np.ndarray): The predicted training target values.\n\n    Returns:\n        (float): The objective function value.\n\n    \"\"\"\n\n    def __init__(\n        self, scorer: Callable[[np.ndarray, np.ndarray, np.ndarray, np.ndarray], float]\n    ):\n        self.scorer = scorer\n        self.check_signature()\n\n    def __call__(\n        self,\n        y_valid: np.ndarray,\n        y_pred: np.ndarray,\n        y_train_in: np.ndarray,\n        y_pred_train: np.ndarray,\n    ) -&gt; float:\n        return self.scorer(y_valid, y_pred, y_train_in, y_pred_train)\n\n    def check_signature(self):\n        \"\"\" \"\"\"\n        expected_args = [\"y_valid\", \"y_pred\", \"y_train_in\", \"y_pred_train\"]\n        signature = inspect.signature(self.scorer)\n        for arg_name, param in signature.parameters.items():\n            if arg_name not in expected_args:\n                raise ValueError(\n                    f\"Invalid argument name '{arg_name}' in scorer function signature.\"\n                )\n            if param.kind != inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                raise ValueError(\n                    f\"Invalid parameter kind '{param.kind}' in scorer function signature.\"\n                )\n        if len(signature.parameters) != len(expected_args):\n            raise ValueError(\n                f\"Invalid number of arguments in scorer function signature. Expected {len(expected_args)}, got {len(signature.parameters)}.\"\n            )\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.ObjectiveScorer.check_signature","title":"<code>flexcv.model_selection.ObjectiveScorer.check_signature()</code>","text":"Source code in <code>flexcv/model_selection.py</code> <pre><code>def check_signature(self):\n    \"\"\" \"\"\"\n    expected_args = [\"y_valid\", \"y_pred\", \"y_train_in\", \"y_pred_train\"]\n    signature = inspect.signature(self.scorer)\n    for arg_name, param in signature.parameters.items():\n        if arg_name not in expected_args:\n            raise ValueError(\n                f\"Invalid argument name '{arg_name}' in scorer function signature.\"\n            )\n        if param.kind != inspect.Parameter.POSITIONAL_OR_KEYWORD:\n            raise ValueError(\n                f\"Invalid parameter kind '{param.kind}' in scorer function signature.\"\n            )\n    if len(signature.parameters) != len(expected_args):\n        raise ValueError(\n            f\"Invalid number of arguments in scorer function signature. Expected {len(expected_args)}, got {len(signature.parameters)}.\"\n        )\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.custom_scorer","title":"<code>flexcv.model_selection.custom_scorer(y_valid, y_pred, y_train_in, y_pred_train)</code>","text":"<p>Objective scorer for the hyperparameter optimization. The function calculates the mean squared error (MSE) for both the validation and training data, and then calculates a weighted sum of the MSEs and their differences. The weights and thresholds used in the calculation are defined in the function. The function returns a float value that represents the objective function value. This function is used in the hyperparameter optimization process to evaluate the performance of different models with different hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>y_valid</code> <code>ndarray</code> <p>The validation target values</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted target values</p> required <code>y_train_in</code> <code>ndarray</code> <p>Inner training target values</p> required <code>y_pred_train</code> <code>ndarray</code> <p>Inner predicted target values</p> required <p>Returns:</p> Type Description <code>float</code> <p>The objective function value.</p> <p>For hyperparameter tuning (inner cv loop) we use the following hierarchy:     <pre><code>objective_cv(\n    if n_jobs == -1:\n        parallel_objective(some_kind_of_scorer)\n    else:\n        objective(some_king_of_scorer)\n</code></pre></p> Source code in <code>flexcv/model_selection.py</code> <pre><code>def custom_scorer(y_valid, y_pred, y_train_in, y_pred_train) -&gt; float:\n    \"\"\"Objective scorer for the hyperparameter optimization.\n    The function calculates the mean squared error (MSE) for both the validation and training data,\n    and then calculates a weighted sum of the MSEs and their differences.\n    The weights and thresholds used in the calculation are defined in the function.\n    The function returns a float value that represents the objective function value.\n    This function is used in the hyperparameter optimization process to evaluate the performance of different models with different hyperparameters.\n\n    Args:\n      y_valid (np.ndarray): The validation target values\n      y_pred (np.ndarray): Predicted target values\n      y_train_in (np.ndarray): Inner training target values\n      y_pred_train (np.ndarray): Inner predicted target values\n\n    Returns:\n      (float): The objective function value.\n\n    For hyperparameter tuning (inner cv loop) we use the following hierarchy:\n        ```python\n        objective_cv(\n            if n_jobs == -1:\n                parallel_objective(some_kind_of_scorer)\n            else:\n                objective(some_king_of_scorer)\n        ```\n\n    \"\"\"\n\n    mse_valid = mean_squared_error(y_valid, y_pred)\n    mse_train = mean_squared_error(y_train_in, y_pred_train)\n\n    mse_delta = mse_train - mse_valid\n    target_delta = 0.05\n\n    return (\n        1 * mse_valid\n        + 0.5 * abs(mse_delta)\n        + 2 * max(0, (mse_delta - target_delta))\n        + 1 * max(0, -mse_delta)\n    )\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.objective","title":"<code>flexcv.model_selection.objective(X_train_in, y_train_in, X_valid, y_valid, pipe, params, objective_scorer)</code>","text":"<p>Objective function for the hyperparameter optimization. Sets the parameters of the pipeline and fits it to the training data. Predicts the validation data and calculates the MSE for both the validation and training data. Then applies the objective scorer to the validation MSE and the training MSE which returns the objective function value. Returns the negative validation and training MSEs as well as the negative objective function value, since optuna maximizes the objective function. This function is called from the objective_cv function if n_jobs_cv is set to 1.</p> <p>Parameters:</p> Name Type Description Default <code>X_train_in</code> <code>DataFrame or ndarray</code> <p>The training data.</p> required <code>y_train_in</code> <code>DataFrame or ndarray</code> <p>The training target values.</p> required <code>X_valid</code> <code>DataFrame or ndarray</code> <p>The validation data.</p> required <code>y_valid</code> <code>DataFrame or ndarray</code> <p>The validation target values.</p> required <code>pipe</code> <code>Pipeline</code> <p>The pipeline to be used for the training.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the negative validation MSE, the negative training MSE and the negative objective function value.</p> Inner CV pseudo code <pre><code>objective_cv(\n    if n_jobs == -1:\n        parallel_objective(some_kind_of_scorer)\n    else:\n        objective(some_king_of_scorer)\n</code></pre> Source code in <code>flexcv/model_selection.py</code> <pre><code>def objective(\n    X_train_in,\n    y_train_in,\n    X_valid,\n    y_valid,\n    pipe,\n    params,\n    objective_scorer: ObjectiveScorer,\n):\n    \"\"\"Objective function for the hyperparameter optimization.\n    Sets the parameters of the pipeline and fits it to the training data.\n    Predicts the validation data and calculates the MSE for both the validation and training data.\n    Then applies the objective scorer to the validation MSE and the training MSE which returns the objective function value.\n    Returns the negative validation and training MSEs as well as the negative objective function value, since optuna maximizes the objective function.\n    This function is called from the objective_cv function if n_jobs_cv is set to 1.\n\n    Args:\n        X_train_in (pd.DataFrame or np.ndarray): The training data.\n        y_train_in (pd.DataFrame or np.ndarray): The training target values.\n        X_valid (pd.DataFrame or np.ndarray): The validation data.\n        y_valid (pd.DataFrame or np.ndarray): The validation target values.\n        pipe (Pipeline): The pipeline to be used for the training.\n\n    Returns:\n        (tuple): A tuple containing the negative validation MSE, the negative training MSE and the negative objective function value.\n\n    Inner CV pseudo code:\n        ```python\n        objective_cv(\n            if n_jobs == -1:\n                parallel_objective(some_kind_of_scorer)\n            else:\n                objective(some_king_of_scorer)\n        ```\n\n    \"\"\"\n\n    pipe.set_params(**params)\n\n    pipe.fit(X_train_in, y_train_in)\n\n    y_pred = pipe.predict(X_valid)\n    y_pred_train = pipe.predict(X_train_in)\n\n    score_valid = mean_squared_error(y_valid, y_pred)\n    score_train = mean_squared_error(y_train_in, y_pred_train)\n    score_of = objective_scorer(y_valid, y_pred, y_train_in, y_pred_train)\n\n    return -score_valid, -score_train, -score_of\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.objective_cv","title":"<code>flexcv.model_selection.objective_cv(trial, cross_val_split, pipe, params, X, y, run, n_jobs, objective_scorer)</code>","text":"<p>Objective function for the hyperparameter optimization with cross validation. n_jobs is the number of processes to use for the parallelization. If n_jobs is -1, the number of processes is set to the number of available CPUs. If n_jobs is 1, the objective function is called sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>trial</code> <p>Optuna trial object.</p> required <code>cross_val_split</code> <code>function</code> <p>Function that returns the indices for the cross validation split.</p> required <code>pipe</code> <code>Pipeline</code> <p>The pipeline to be used for the training.</p> required <code>params</code> <code>dict</code> <p>Dictionary containing the parameters to be set in the pipeline.</p> required <code>X</code> <code>DataFrame or ndarray</code> <p>Features.</p> required <code>y</code> <code>DataFrame or ndarray</code> <p>Target.</p> required <code>run</code> <code>run</code> <p>neptune run object</p> required <code>n_jobs</code> <code>int</code> <p>Sklearn n_jobs parameter to control if CV is run in parallel or sequentially</p> required <code>objective_scorer</code> <code>ObjectiveScorer</code> <p>Callable class that wraps a scorer function to be used as an objective function.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean objective function value. Note: We average per default. If you would like to use the RMSE as the objective function, you have to average the MSEs and then take the square root.</p> Inner CV pseudo code <pre><code>objective_cv(\n    if n_jobs == -1:\n        parallel_objective(some_kind_of_scorer)\n    else:\n        objective(some_king_of_scorer)\n</code></pre> Source code in <code>flexcv/model_selection.py</code> <pre><code>def objective_cv(\n    trial, cross_val_split, pipe, params, X, y, run, n_jobs, objective_scorer\n):\n    \"\"\"Objective function for the hyperparameter optimization with cross validation.\n    n_jobs is the number of processes to use for the parallelization.\n    If n_jobs is -1, the number of processes is set to the number of available CPUs.\n    If n_jobs is 1, the objective function is called sequentially.\n\n    Args:\n      trial (neptune.trial): Optuna trial object.\n      cross_val_split (function): Function that returns the indices for the cross validation split.\n      pipe (Pipeline): The pipeline to be used for the training.\n      params (dict): Dictionary containing the parameters to be set in the pipeline.\n      X (pd.DataFrame or np.ndarray): Features.\n      y (pd.DataFrame or np.ndarray): Target.\n      run (neptune.run): neptune run object\n      n_jobs (int): Sklearn n_jobs parameter to control if CV is run in parallel or sequentially\n      objective_scorer (ObjectiveScorer): Callable class that wraps a scorer function to be used as an objective function.\n\n\n    Returns:\n      (float): The mean objective function value. Note: We average per default. If you would like to use the RMSE as the objective function, you have to average the MSEs and then take the square root.\n\n    Inner CV pseudo code:\n        ```python\n        objective_cv(\n            if n_jobs == -1:\n                parallel_objective(some_kind_of_scorer)\n            else:\n                objective(some_king_of_scorer)\n        ```\n\n    \"\"\"\n\n    params_ = {\n        name: trial._suggest(name, distribution)\n        for name, distribution in params.items()\n    }\n\n    scores_valid = []\n    scores_train = []\n    scores_OF = []\n\n    if n_jobs == -1:\n        # Define the number of processes to use and create a pool\n        num_processes = multiprocessing.cpu_count()\n        pool = multiprocessing.Pool(processes=num_processes)\n\n        # Map the parallel function to the cross validation split\n        results = pool.starmap(\n            parallel_objective,\n            [\n                (train_idx, valid_idx, X, y, pipe, params_, objective_scorer)\n                for train_idx, valid_idx in cross_val_split(X=X, y=y)\n            ],\n        )\n        pool.close()\n\n        for result in results:\n            scores_valid.append(result[0])\n            scores_train.append(result[1])\n            scores_OF.append(result[2])\n    else:\n        for train_idx, valid_idx in cross_val_split(X=X, y=y):\n            X_train_in = X.iloc[train_idx]\n            y_train_in = y.iloc[train_idx]\n\n            X_valid = X.iloc[valid_idx]\n            y_valid = y.iloc[valid_idx]\n\n            score_valid, score_train, score_OF = objective(\n                X_train_in,\n                y_train_in,\n                X_valid,\n                y_valid,\n                pipe,\n                params_,\n                objective_scorer,\n            )\n\n            scores_valid.append(score_valid)\n            scores_train.append(score_train)\n            scores_OF.append(score_OF)\n\n    trial.set_user_attr(\"mean_test_score\", np.mean(scores_valid))\n    trial.set_user_attr(\"mean_train_score\", np.mean(scores_train))\n    trial.set_user_attr(\"mean_OF_score\", np.mean(scores_OF))\n\n    return np.mean(scores_OF)\n</code></pre>"},{"location":"reference/model-selection/#flexcv.model_selection.parallel_objective","title":"<code>flexcv.model_selection.parallel_objective(train_idx, valid_idx, X, y, pipe, params_, objective_scorer)</code>","text":"<p>Objective function for the hyperparameter optimization to be used with multiprocessing.Pool.starmap. Gets the training and validation indices and the data and calls the objective function. Is called from the objective_cv function if n_jobs_cv is set to -1.</p> <p>Parameters:</p> Name Type Description Default <code>train_idx</code> <code>ndarray</code> <p>The training indices.</p> required <code>valid_idx</code> <code>ndarray</code> <p>The validation indices.</p> required <code>X</code> <code>DataFrame or ndarray</code> <p>The data.</p> required <code>y</code> <code>DataFrame or ndarray</code> <p>The target values.</p> required <code>pipe</code> <code>Pipeline</code> <p>The pipeline to be used for the training.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the validation MSE, the training MSE and the objective function value.</p> Inner CV pseudo code <pre><code>objective_cv(\n    if n_jobs == -1:\n        parallel_objective(some_kind_of_scorer)\n    else:\n        objective(some_king_of_scorer)\n</code></pre> Source code in <code>flexcv/model_selection.py</code> <pre><code>def parallel_objective(\n    train_idx, valid_idx, X, y, pipe, params_, objective_scorer: ObjectiveScorer\n):\n    \"\"\"Objective function for the hyperparameter optimization to be used with multiprocessing.Pool.starmap.\n    Gets the training and validation indices and the data and calls the objective function.\n    Is called from the objective_cv function if n_jobs_cv is set to -1.\n\n    Args:\n        train_idx (ndarray): The training indices.\n        valid_idx (ndarray): The validation indices.\n        X (pd.DataFrame or np.ndarray): The data.\n        y (pd.DataFrame or np.ndarray): The target values.\n        pipe (Pipeline): The pipeline to be used for the training.\n\n    Returns:\n      (tuple): A tuple containing the validation MSE, the training MSE and the objective function value.\n\n    Inner CV pseudo code:\n        ```python\n        objective_cv(\n            if n_jobs == -1:\n                parallel_objective(some_kind_of_scorer)\n            else:\n                objective(some_king_of_scorer)\n        ```\n\n    \"\"\"\n    X_train_in = X.iloc[train_idx]\n    y_train_in = y.iloc[train_idx]\n\n    X_valid = X.iloc[valid_idx]\n    y_valid = y.iloc[valid_idx]\n\n    score_valid, score_train, score_OF = objective(\n        X_train_in, y_train_in, X_valid, y_valid, pipe, params_, objective_scorer\n    )\n\n    return score_valid, score_train, score_OF\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#flexcv.models","title":"<code>flexcv.models</code>","text":"<p>This module implements wrapper classes for the Linear Model and the Linear Mixed Effects Model from statsmodels.</p>"},{"location":"reference/models/#flexcv.models.BaseLinearModel","title":"<code>flexcv.models.BaseLinearModel</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Base class for the Linear Model and the Linear Mixed Effects Model.</p> Source code in <code>flexcv/models.py</code> <pre><code>class BaseLinearModel(BaseEstimator, RegressorMixin):\n    \"\"\"Base class for the Linear Model and the Linear Mixed Effects Model.\"\"\"\n\n    def __init__(self, re_formula=None, verbose=0, *args, **kwargs):\n        self.re_formula = re_formula\n        self.verbose = verbose\n        self.best_params = {}\n        self.params = {}\n\n    def get_params(self, deep=True):\n        \"\"\"Return the parameters of the model.\n\n        Args:\n          deep: This argument is not used. (Default value = True)\n\n        Returns:\n            (dict): Parameter names mapped to their values.\n\n        \"\"\"\n        return self.params\n\n    def get_summary(self):\n        \"\"\"Creates a html summary table of the model.\n\n        Returns:\n            (str): HTML table of the model summary.\"\"\"\n        lmer_summary = self.md_.summary()  # type: ignore\n        try:\n            html_tables = \"\"\n            for table in lmer_summary.tables:\n                html_tables += table.as_html()\n        except AttributeError:\n            html_tables = lmer_summary.as_html()\n        return html_tables\n</code></pre>"},{"location":"reference/models/#flexcv.models.BaseLinearModel.get_params","title":"<code>flexcv.models.BaseLinearModel.get_params(deep=True)</code>","text":"<p>Return the parameters of the model.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <p>This argument is not used. (Default value = True)</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Parameter names mapped to their values.</p> Source code in <code>flexcv/models.py</code> <pre><code>def get_params(self, deep=True):\n    \"\"\"Return the parameters of the model.\n\n    Args:\n      deep: This argument is not used. (Default value = True)\n\n    Returns:\n        (dict): Parameter names mapped to their values.\n\n    \"\"\"\n    return self.params\n</code></pre>"},{"location":"reference/models/#flexcv.models.BaseLinearModel.get_summary","title":"<code>flexcv.models.BaseLinearModel.get_summary()</code>","text":"<p>Creates a html summary table of the model.</p> <p>Returns:</p> Type Description <code>str</code> <p>HTML table of the model summary.</p> Source code in <code>flexcv/models.py</code> <pre><code>def get_summary(self):\n    \"\"\"Creates a html summary table of the model.\n\n    Returns:\n        (str): HTML table of the model summary.\"\"\"\n    lmer_summary = self.md_.summary()  # type: ignore\n    try:\n        html_tables = \"\"\n        for table in lmer_summary.tables:\n            html_tables += table.as_html()\n    except AttributeError:\n        html_tables = lmer_summary.as_html()\n    return html_tables\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearMixedEffectsModel","title":"<code>flexcv.models.LinearMixedEffectsModel</code>","text":"<p>               Bases: <code>BaseLinearModel</code></p> <p>Wrapper class for the Linear Mixed Effects Model from statsmodels.</p> Source code in <code>flexcv/models.py</code> <pre><code>class LinearMixedEffectsModel(BaseLinearModel):\n    \"\"\"Wrapper class for the Linear Mixed Effects Model from statsmodels.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def fit(self, X, y, clusters, formula, re_formula, **kwargs):\n        \"\"\"Fit the LMER model to the given training data.\n\n        Args:\n          X (pd.DataFrame): The training input samples.\n          y (pd.Series): The target values.\n          clusters (pd.Series): The clustering data.\n          re_formula (str): The random effects formula for the random slopes and intercepts.\n          **kwargs (dict): Additional parameters to pass to the underlying model's `fit` method.\n\n\n        Returns:\n          (object): Returns self.\n\n        Notes:\n            This method fits a LMER class on the X data.\n        \"\"\"\n        assert (\n            X.shape[0] == y.shape[0]\n        ), \"Number of X samples must match number of y samples.\"\n        assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n        assert type(y) == pd.Series, \"y must be a pandas Series.\"\n        assert type(clusters) == pd.Series, \"clusters must be a pandas Series.\"\n        assert (\n            len(y) == X.shape[0]\n        ), \"Number of target must match number of feature samples.\"\n        assert (\n            len(clusters) == X.shape[0]\n        ), \"Number of clusters must match number of feature samples.\"\n\n        assert (\n            re_formula is not None\n        ), \"re_formula must be specified for the LMER model.\"\n\n        assert (\n            len(clusters.unique()) &gt; 1\n        ), \"Only one cluster found. There might be a problem with the cluster column.\"\n\n        self.X_ = X\n        self.y_ = y\n        self.cluster_counts = clusters.value_counts()\n        self.re_formula = re_formula\n        data = pd.concat([y, X, clusters], axis=1, sort=False)\n        data.columns = [y.name] + list(X.columns) + [clusters.name]\n        # if re_formula is None we pass a empty dict, else we pass the re_formula\n        re_formula_dict = {\"re_formula\": re_formula} if self.re_formula else {}\n        md = smf.mixedlm(\n            formula=formula,\n            data=data,\n            groups=clusters.name,\n            **re_formula_dict,\n        )\n\n        self.md = md\n        self.md_ = md.fit()\n\n        self.best_params = self.get_summary()\n        return self\n\n    def predict(self, X: pd.DataFrame, clusters: pd.Series, **kwargs):\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Args:\n          X (pd.DataFrame): Features\n          clusters (pd.Series): The clustering data.\n          **kwargs: Any other keyword arguments to pass to the underlying model's `predict` method. This is necessary to prevent raising an error when passing the `clusters` argument.\n\n        Returns:\n          (array-like): An array of fitted values.\n\n        \"\"\"\n        check_is_fitted(self, [\"X_\", \"y_\", \"md_\"])\n        predict_known_groups_lmm = kwargs[\"predict_known_groups_lmm\"]\n        Z = kwargs[\"Z\"]\n        assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n        assert type(clusters) == pd.Series, \"clusters must be a pandas Series.\"\n        assert (\n            len(clusters) == X.shape[0]\n        ), \"Number of clusters must match number of samples.\"\n        if len(clusters.unique()) == 1:\n            logger.warning(\n                \"Only one cluster found. There might be a problem with the cluster column.\"\n            )\n\n        if predict_known_groups_lmm == True:\n            yp = self.md_.predict(exog=X)\n\n            for cluster_id in self.cluster_counts.index:\n                indices_i = clusters == cluster_id\n\n                # If cluster doesn't exist move on.\n                if len(indices_i) == 0:\n                    continue\n\n                # # If cluster does exist, apply the correction.\n                b_i = self.md_.random_effects[cluster_id]\n\n                Z_i = Z[indices_i]\n                yp[indices_i] += Z_i.dot(b_i)\n\n            return yp\n        else:\n            return self.md_.predict(exog=X)\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearMixedEffectsModel.fit","title":"<code>flexcv.models.LinearMixedEffectsModel.fit(X, y, clusters, formula, re_formula, **kwargs)</code>","text":"<p>Fit the LMER model to the given training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The training input samples.</p> required <code>y</code> <code>Series</code> <p>The target values.</p> required <code>clusters</code> <code>Series</code> <p>The clustering data.</p> required <code>re_formula</code> <code>str</code> <p>The random effects formula for the random slopes and intercepts.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional parameters to pass to the underlying model's <code>fit</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>Returns self.</p> Notes <p>This method fits a LMER class on the X data.</p> Source code in <code>flexcv/models.py</code> <pre><code>def fit(self, X, y, clusters, formula, re_formula, **kwargs):\n    \"\"\"Fit the LMER model to the given training data.\n\n    Args:\n      X (pd.DataFrame): The training input samples.\n      y (pd.Series): The target values.\n      clusters (pd.Series): The clustering data.\n      re_formula (str): The random effects formula for the random slopes and intercepts.\n      **kwargs (dict): Additional parameters to pass to the underlying model's `fit` method.\n\n\n    Returns:\n      (object): Returns self.\n\n    Notes:\n        This method fits a LMER class on the X data.\n    \"\"\"\n    assert (\n        X.shape[0] == y.shape[0]\n    ), \"Number of X samples must match number of y samples.\"\n    assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n    assert type(y) == pd.Series, \"y must be a pandas Series.\"\n    assert type(clusters) == pd.Series, \"clusters must be a pandas Series.\"\n    assert (\n        len(y) == X.shape[0]\n    ), \"Number of target must match number of feature samples.\"\n    assert (\n        len(clusters) == X.shape[0]\n    ), \"Number of clusters must match number of feature samples.\"\n\n    assert (\n        re_formula is not None\n    ), \"re_formula must be specified for the LMER model.\"\n\n    assert (\n        len(clusters.unique()) &gt; 1\n    ), \"Only one cluster found. There might be a problem with the cluster column.\"\n\n    self.X_ = X\n    self.y_ = y\n    self.cluster_counts = clusters.value_counts()\n    self.re_formula = re_formula\n    data = pd.concat([y, X, clusters], axis=1, sort=False)\n    data.columns = [y.name] + list(X.columns) + [clusters.name]\n    # if re_formula is None we pass a empty dict, else we pass the re_formula\n    re_formula_dict = {\"re_formula\": re_formula} if self.re_formula else {}\n    md = smf.mixedlm(\n        formula=formula,\n        data=data,\n        groups=clusters.name,\n        **re_formula_dict,\n    )\n\n    self.md = md\n    self.md_ = md.fit()\n\n    self.best_params = self.get_summary()\n    return self\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearMixedEffectsModel.predict","title":"<code>flexcv.models.LinearMixedEffectsModel.predict(X, clusters, **kwargs)</code>","text":"<p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Features</p> required <code>clusters</code> <code>Series</code> <p>The clustering data.</p> required <code>**kwargs</code> <p>Any other keyword arguments to pass to the underlying model's <code>predict</code> method. This is necessary to prevent raising an error when passing the <code>clusters</code> argument.</p> <code>{}</code> <p>Returns:</p> Type Description <code>array - like</code> <p>An array of fitted values.</p> Source code in <code>flexcv/models.py</code> <pre><code>def predict(self, X: pd.DataFrame, clusters: pd.Series, **kwargs):\n    \"\"\"\n    Make predictions using the fitted model.\n\n    Args:\n      X (pd.DataFrame): Features\n      clusters (pd.Series): The clustering data.\n      **kwargs: Any other keyword arguments to pass to the underlying model's `predict` method. This is necessary to prevent raising an error when passing the `clusters` argument.\n\n    Returns:\n      (array-like): An array of fitted values.\n\n    \"\"\"\n    check_is_fitted(self, [\"X_\", \"y_\", \"md_\"])\n    predict_known_groups_lmm = kwargs[\"predict_known_groups_lmm\"]\n    Z = kwargs[\"Z\"]\n    assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n    assert type(clusters) == pd.Series, \"clusters must be a pandas Series.\"\n    assert (\n        len(clusters) == X.shape[0]\n    ), \"Number of clusters must match number of samples.\"\n    if len(clusters.unique()) == 1:\n        logger.warning(\n            \"Only one cluster found. There might be a problem with the cluster column.\"\n        )\n\n    if predict_known_groups_lmm == True:\n        yp = self.md_.predict(exog=X)\n\n        for cluster_id in self.cluster_counts.index:\n            indices_i = clusters == cluster_id\n\n            # If cluster doesn't exist move on.\n            if len(indices_i) == 0:\n                continue\n\n            # # If cluster does exist, apply the correction.\n            b_i = self.md_.random_effects[cluster_id]\n\n            Z_i = Z[indices_i]\n            yp[indices_i] += Z_i.dot(b_i)\n\n        return yp\n    else:\n        return self.md_.predict(exog=X)\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearModel","title":"<code>flexcv.models.LinearModel</code>","text":"<p>               Bases: <code>BaseLinearModel</code></p> <p>Wrapper class for the Linear Model from statsmodels.</p> Source code in <code>flexcv/models.py</code> <pre><code>class LinearModel(BaseLinearModel):\n    \"\"\"Wrapper class for the Linear Model from statsmodels.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def fit(self, X, y, formula, **kwargs):\n        \"\"\"Fit the LM to the given training data.\n\n        Args:\n          X (array-like of shape (n_samples, n_features)): The training input samples.\n          y (array-like of shape (n_samples,)): The target values.\n          **kwargs(dict): Additional parameters to pass to the underlying model's `fit` method.\n\n        Returns:\n          (object): Returns the model after fit.\n\n        Notes:\n            This method fits a OLS class on the X data.\n        \"\"\"\n        assert (\n            X.shape[0] == y.shape[0]\n        ), \"Number of X samples must match number of y samples.\"\n        assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n        assert type(y) == pd.Series, \"y must be a pandas Series.\"\n\n        self.X_ = X\n        self.y_ = y\n\n        data = pd.concat([y, X], axis=1, sort=False)\n        data.columns = [y.name] + list(X.columns)\n        md = smf.ols(formula, data)\n        self.md_ = md.fit()\n        self.best_params = self.get_summary()\n        return self\n\n    def predict(self, X, **kwargs):\n        \"\"\"Make predictions using the fitted model.\n\n        Args:\n          X (array-like): Features\n          **kwargs: Used to prevent raising an error when passing the `clusters` argument.\n\n        Returns:\n          (array-like): An array of fitted values.\n\n\n        \"\"\"\n        check_is_fitted(self, [\"X_\", \"y_\", \"md_\"])\n        return self.md_.predict(exog=X)\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearModel.fit","title":"<code>flexcv.models.LinearModel.fit(X, y, formula, **kwargs)</code>","text":"<p>Fit the LM to the given training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features</code> <p>The training input samples.</p> required <code>y</code> <code>array-like of shape (n_samples,</code> <p>The target values.</p> required <code>**kwargs(dict)</code> <p>Additional parameters to pass to the underlying model's <code>fit</code> method.</p> required <p>Returns:</p> Type Description <code>object</code> <p>Returns the model after fit.</p> Notes <p>This method fits a OLS class on the X data.</p> Source code in <code>flexcv/models.py</code> <pre><code>def fit(self, X, y, formula, **kwargs):\n    \"\"\"Fit the LM to the given training data.\n\n    Args:\n      X (array-like of shape (n_samples, n_features)): The training input samples.\n      y (array-like of shape (n_samples,)): The target values.\n      **kwargs(dict): Additional parameters to pass to the underlying model's `fit` method.\n\n    Returns:\n      (object): Returns the model after fit.\n\n    Notes:\n        This method fits a OLS class on the X data.\n    \"\"\"\n    assert (\n        X.shape[0] == y.shape[0]\n    ), \"Number of X samples must match number of y samples.\"\n    assert type(X) == pd.DataFrame, \"X must be a pandas DataFrame.\"\n    assert type(y) == pd.Series, \"y must be a pandas Series.\"\n\n    self.X_ = X\n    self.y_ = y\n\n    data = pd.concat([y, X], axis=1, sort=False)\n    data.columns = [y.name] + list(X.columns)\n    md = smf.ols(formula, data)\n    self.md_ = md.fit()\n    self.best_params = self.get_summary()\n    return self\n</code></pre>"},{"location":"reference/models/#flexcv.models.LinearModel.predict","title":"<code>flexcv.models.LinearModel.predict(X, **kwargs)</code>","text":"<p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> required <code>**kwargs</code> <p>Used to prevent raising an error when passing the <code>clusters</code> argument.</p> <code>{}</code> <p>Returns:</p> Type Description <code>array - like</code> <p>An array of fitted values.</p> Source code in <code>flexcv/models.py</code> <pre><code>def predict(self, X, **kwargs):\n    \"\"\"Make predictions using the fitted model.\n\n    Args:\n      X (array-like): Features\n      **kwargs: Used to prevent raising an error when passing the `clusters` argument.\n\n    Returns:\n      (array-like): An array of fitted values.\n\n\n    \"\"\"\n    check_is_fitted(self, [\"X_\", \"y_\", \"md_\"])\n    return self.md_.predict(exog=X)\n</code></pre>"},{"location":"reference/plotting/","title":"Plotting","text":""},{"location":"reference/plotting/#flexcv.plot","title":"<code>flexcv.plot</code>","text":"<p>This module provides functions for plotting and logging plots to neptune.</p>"},{"location":"reference/plotting/#flexcv.plot.permutation_importance","title":"<code>flexcv.plot.permutation_importance(model, model_name, X, y, features)</code>","text":"<p>Calculates and plots the permutation importance of a model. Args:     model (object): The model to calculate the permutation importance for.     model_name (str): The name of the model.     X (array-like): The features.     y (array-like): The target.     features (array-like | list): The feature names.</p> <p>Returns:</p> Type Description <code>tuple[Figure, DataFrame]</code> <p>A tuple containing the figure and the permutation importance dataframe.</p> Source code in <code>flexcv/plot.py</code> <pre><code>def permutation_importance(\n    model, model_name, X, y, features\n) -&gt; tuple[plt.Figure, pd.DataFrame]:\n    \"\"\"\n    Calculates and plots the permutation importance of a model.\n    Args:\n        model (object): The model to calculate the permutation importance for.\n        model_name (str): The name of the model.\n        X (array-like): The features.\n        y (array-like): The target.\n        features (array-like | list): The feature names.\n\n    Returns:\n        (tuple[plt.Figure, pd.DataFrame]): A tuple containing the figure and the permutation importance dataframe.\n    \"\"\"\n    fig = plt.figure()\n    perm_importance = sk_permutation_importance(\n        model, X, y, n_repeats=10, random_state=42, n_jobs=-1\n    )\n    features = np.array(features)\n    sorted_idx = perm_importance.importances_mean.argsort()\n    plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n    plt.xlabel(f\"{model_name} Permutation Importance\")\n    df = pd.DataFrame(\n        {\n            \"feature\": features[sorted_idx],\n            \"importance\": perm_importance.importances_mean[sorted_idx],\n        }\n    )\n    return fig, df\n</code></pre>"},{"location":"reference/plotting/#flexcv.plot.plot_merf_training_stats","title":"<code>flexcv.plot.plot_merf_training_stats(run, model, model_name, num_clusters_to_plot=5)</code>","text":"<ul> <li>Generalized log-likelihood across iterations</li> <li>trace and determinant of Sigma_b across iterations</li> <li>sigma_e across iterations</li> <li>bi for num_clusters_to_plot across iterations</li> <li>a histogram of the final learned bi</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MERF</code> <p>trained MERF model</p> required <code>num_clusters_to_plot</code> <code>int</code> <p>number of example bi's to plot across iterations</p> <code>5</code> <code>meta_string</code> <code>string</code> <p>A string for use as additional info in filename.</p> required <p>Returns:</p> Type Description <code>fig</code> <p>figure. Also draws to display.</p> Source code in <code>flexcv/plot.py</code> <pre><code>def plot_merf_training_stats(run, model, model_name, num_clusters_to_plot=5) -&gt; None:\n    \"\"\"\n    * Generalized log-likelihood across iterations\n    * trace and determinant of Sigma_b across iterations\n    * sigma_e across iterations\n    * bi for num_clusters_to_plot across iterations\n    * a histogram of the final learned bi\n\n    Args:\n        model (MERF): trained MERF model\n        num_clusters_to_plot (int): number of example bi's to plot across iterations\n        meta_string (string): A string for use as additional info in filename.\n\n    Returns:\n        (matplotlib.pyplot.fig): figure. Also draws to display.\n    \"\"\"\n    # get number of columns of model.trained_b\n\n    dpi = matplotlib_settings()\n    fig, axs = plt.subplots(nrows=2, ncols=2, dpi=dpi)\n\n    # Plot GLL\n    axs[0, 0].plot(model.gll_history)\n    axs[0, 0].grid(\"on\")\n    axs[0, 0].set_ylabel(\"GLL\")\n    axs[0, 0].set_title(\"GLL\")\n    set_axes_params(axs[0, 0])\n\n    # Plot trace and determinant of Sigma_b (covariance matrix)\n    det_sigmaB_history = [np.linalg.det(x) for x in model.D_hat_history]\n    trace_sigmaB_history = [np.trace(x) for x in model.D_hat_history]\n    axs[0, 1].plot(det_sigmaB_history, label=\"$det(\\sigma_b)$\")  # det($sigma_b$)\"\n    axs[0, 1].plot(\n        trace_sigmaB_history, label=\"$trace(\\sigma_b)$\"\n    )  # \"trace($sigma_b$)\"\n    axs[0, 1].grid(\"on\")\n    axs[0, 1].legend()\n    axs[0, 1].set_title(\"Trace and Determinant of $\\sigma_b$\")\n    set_axes_params(axs[0, 1])\n\n    # Plot sigma_e across iterations\n    axs[1, 0].plot(model.sigma2_hat_history)\n    axs[1, 0].grid(\"on\")\n    axs[1, 0].set_ylabel(\"$\\hat\\sigma_e$\")\n    axs[1, 0].set_xlabel(\"Iteration\")\n    axs[1, 0].set_title(\"$\\hat\\sigma_e$ vs iterations\")\n    set_axes_params(axs[1, 0])\n\n    # Plot bi across iterations\n    b_hat_history_df = model.get_bhat_history_df()\n    for cluster_id in model.cluster_counts.index[0:num_clusters_to_plot]:\n        axs[1, 1].plot(\n            b_hat_history_df.xs(cluster_id, level=\"cluster\"), label=cluster_id\n        )\n    axs[1, 1].grid(\"on\")\n    axs[1, 1].set_ylabel(\"$b_hat$\")\n    axs[1, 1].set_xlabel(\"Iteration\")\n    tmp_title = \"$b_i$ vs iterations\\n\" + f\"({num_clusters_to_plot} clusters shown)\"\n    axs[1, 1].set_title(tmp_title)\n    set_axes_params(axs[1, 1])\n    run[f\"{model_name}/Plots/Training_Stats\"].append(fig)\n    plt.close(fig)\n\n    num_random_effects = model.trained_b.shape[1]\n    dpi = matplotlib_settings()\n    fig, axs = plt.subplots(nrows=1, ncols=num_random_effects, dpi=dpi)\n    model.trained_b.hist(bins=100, ax=axs)\n    if num_random_effects == 1:\n        axs.set_xlabel(\"$b_i$\")\n        axs.set_title(\"Distribution of $b_i$ for Random Intercepts\")\n    else:\n        try:\n            for i in range(num_random_effects):\n                axs[i].set_xlabel(\"$b_i$\")\n                axs[i].set_title(\n                    \"Distribution of $b_i$ for Random Effect \" + str(i + 1)\n                )\n\n        except ValueError:\n            print(\"Plotting Error:\")\n            print(\n                \"hist method requires numerical or datetime columns, nothing to plot.\"\n            )\n            run[\n                \"Errors\"\n            ] = \"Error in plot_merf_training_stats: model.trained_b.hist(bins=100, ax=ax)\"\n    plt.tight_layout()\n    run[f\"{model_name}/Plots/Training_Hists\"].append(fig)\n    plt.close(fig)\n\n    return None\n</code></pre>"},{"location":"reference/plotting/#flexcv.plot.plot_qq","title":"<code>flexcv.plot.plot_qq(y, yhat, run=None, model_name='LM', log_destination='LM_Plots/QQ/')</code>","text":"<p>Creates QQ plot and logs it to a Neptune Run.</p> Source code in <code>flexcv/plot.py</code> <pre><code>def plot_qq(\n    y: pd.Series,\n    yhat: pd.Series,\n    run=None,\n    model_name: str = \"LM\",\n    log_destination: str = \"LM_Plots/QQ/\",\n):\n    \"\"\"Creates QQ plot and logs it to a Neptune Run.\"\"\"\n    plt.close()\n    plt.cla()\n    fig = sm.qqplot(y - yhat, line=\"r\")\n    plt.title(f\"QQ plot of residuals - {model_name}\")\n    run[f\"{log_destination}{model_name}_QQ\"].append(fig)\n    del fig\n    return None\n</code></pre>"},{"location":"reference/plotting/#flexcv.plot.plot_shap","title":"<code>flexcv.plot.plot_shap(shap_values, X, run=None, log_destination='SHAP/', dependency=True, k_features=None)</code>","text":"<p>Creates SHAP summary beeswarm and dependency plots (if set to True) and logs them to a Neptune Run.</p> Source code in <code>flexcv/plot.py</code> <pre><code>def plot_shap(\n    shap_values,\n    X: pd.DataFrame,\n    run=None,\n    log_destination=\"SHAP/\",\n    dependency: bool = True,\n    k_features: pd.Series = None,\n):\n    \"\"\"Creates SHAP summary beeswarm and dependency plots (if set to True) and logs them to a Neptune Run.\"\"\"\n    plt.close()\n    plt.cla()\n    shap.summary_plot(shap_values, X, show=False)\n    matplotlib_settings()\n    f = plt.gcf()\n    run[f\"{log_destination}Importance\"].append(f)\n    del f\n\n    if dependency:\n        for col_name in k_features:\n            plt.close()\n            plt.cla()\n            shap.dependence_plot(col_name, shap_values, X, show=False, alpha=0.5)\n            f = plt.gcf()\n            run[f\"{log_destination}Dependency\"].append(f)\n            del f\n\n    return None\n</code></pre>"},{"location":"reference/postprocessing/","title":"Model Postprocessing","text":""},{"location":"reference/postprocessing/#flexcv.model_postprocessing","title":"<code>flexcv.model_postprocessing</code>","text":"<p>This module contains functions that are specifically written to fulfill the postprocessing requirements of the models. They are customized depending on what the model returns and what the user wants to log to Neptune. The functions are called by the <code>cross_validate</code> function in the <code>flexcv.core</code> module after the model has been fitted.</p>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LMERModelPostProcessor","title":"<code>flexcv.model_postprocessing.LMERModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class LMERModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n        \"\"\"Postprocessing function for the linear mixed effects model.\n        Logs the summary of the model to neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        # LM is the only one where the regular logging wont work with get_params()\n        # therefore, we need to get the summary via the model object and simply overwrite the empty logs\n        params = fold_result.fit_result.get_summary()\n\n        run[f\"{fold_result.model_name}/Summary\"].append(\n            File.from_content(params, extension=\"html\")\n        )\n        results_all_folds[fold_result.model_name][\"parameters\"][fold_result.k] = params\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LMERModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.LMERModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the linear mixed effects model. Logs the summary of the model to neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n    \"\"\"Postprocessing function for the linear mixed effects model.\n    Logs the summary of the model to neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    # LM is the only one where the regular logging wont work with get_params()\n    # therefore, we need to get the summary via the model object and simply overwrite the empty logs\n    params = fold_result.fit_result.get_summary()\n\n    run[f\"{fold_result.model_name}/Summary\"].append(\n        File.from_content(params, extension=\"html\")\n    )\n    results_all_folds[fold_result.model_name][\"parameters\"][fold_result.k] = params\n    return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearModelPostProcessor","title":"<code>flexcv.model_postprocessing.LinearModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class LinearModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n        \"\"\"Postprocessing function for the linear regression model.\n        Logs the summary of the model, the VIF and the plots to neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        # LM is the only one where the regular logging wont work with get_params()\n        # therefore, we need to get the summary via the model object and simply overwrite the empty logs\n        params = fold_result.fit_result.get_summary()\n\n        run[f\"{fold_result.model_name}/Summary\"].append(\n            File.from_content(params, extension=\"html\")\n        )\n        results_all_folds[fold_result.model_name][\"parameters\"][fold_result.k] = params\n\n        vif, fig, ax = LinearRegDiagnostic(fold_result.fit_result.md_)()  # type: ignore   instance has to be called after __init__\n        run[f\"{fold_result.model_name}/VIF\"].append(File.as_html(vif))\n        run[f\"{fold_result.model_name}/Plots\"].append(fig)\n        del fig\n        del vif\n        del ax\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.LinearModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the linear regression model. Logs the summary of the model, the VIF and the plots to neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n    \"\"\"Postprocessing function for the linear regression model.\n    Logs the summary of the model, the VIF and the plots to neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    # LM is the only one where the regular logging wont work with get_params()\n    # therefore, we need to get the summary via the model object and simply overwrite the empty logs\n    params = fold_result.fit_result.get_summary()\n\n    run[f\"{fold_result.model_name}/Summary\"].append(\n        File.from_content(params, extension=\"html\")\n    )\n    results_all_folds[fold_result.model_name][\"parameters\"][fold_result.k] = params\n\n    vif, fig, ax = LinearRegDiagnostic(fold_result.fit_result.md_)()  # type: ignore   instance has to be called after __init__\n    run[f\"{fold_result.model_name}/VIF\"].append(File.as_html(vif))\n    run[f\"{fold_result.model_name}/Plots\"].append(fig)\n    del fig\n    del vif\n    del ax\n    return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic</code>","text":"<p>Implementation by https://www.statsmodels.org/stable/examples/notebooks/generated/linear_regression_diagnostics_plots.html#Simple-multiple-linear-regression</p> <p>Diagnostic plots to identify potential problems in a linear regression fit.</p> <p>Mainly,</p> <pre><code>- non-linearity of data\n- Correlation of error terms\n- non-constant variance\n- outliers\n- high-leverage points\n- collinearity\n</code></pre> Authors <p>Prajwal Kafle (p33ajkafle@gmail.com, where 3 = r) Does not come with any sort of warranty. Please test the code one your end before using.</p> <p>Matt Spinelli (m3spinelli@gmail.com, where 3 = r) (1) Fixed incorrect annotation of the top most extreme residuals in     the Residuals vs Fitted and, especially, the Normal Q-Q plots. (2) Changed Residuals vs Leverage plot to match closer the y-axis     range shown in the equivalent plot in the R package ggfortify. (3) Added horizontal line at y=0 in Residuals vs Leverage plot to     match the plots in R package ggfortify and base R. (4) Added option for placing a vertical guideline on the Residuals     vs Leverage plot using the rule of thumb of h = 2p/n to denote     high leverage (high_leverage_threshold=True). (5) Added two more ways to compute the Cook's Distance (D) threshold:     * 'baseR': D &gt; 1 and D &gt; 0.5 (default)     * 'convention': D &gt; 4/n     * 'dof': D &gt; 4 / (n - k - 1) (6) Fixed class name to conform to Pascal casing convention (7) Fixed Residuals vs Leverage legend to work with loc='best'</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class LinearRegDiagnostic:\n    \"\"\"\n    Implementation by https://www.statsmodels.org/stable/examples/notebooks/generated/linear_regression_diagnostics_plots.html#Simple-multiple-linear-regression\n\n    Diagnostic plots to identify potential problems in a linear regression fit.\n\n    Mainly,\n\n        - non-linearity of data\n        - Correlation of error terms\n        - non-constant variance\n        - outliers\n        - high-leverage points\n        - collinearity\n\n    Authors:\n        Prajwal Kafle (p33ajkafle@gmail.com, where 3 = r)\n        Does not come with any sort of warranty.\n        Please test the code one your end before using.\n\n        Matt Spinelli (m3spinelli@gmail.com, where 3 = r)\n        (1) Fixed incorrect annotation of the top most extreme residuals in\n            the Residuals vs Fitted and, especially, the Normal Q-Q plots.\n        (2) Changed Residuals vs Leverage plot to match closer the y-axis\n            range shown in the equivalent plot in the R package ggfortify.\n        (3) Added horizontal line at y=0 in Residuals vs Leverage plot to\n            match the plots in R package ggfortify and base R.\n        (4) Added option for placing a vertical guideline on the Residuals\n            vs Leverage plot using the rule of thumb of h = 2p/n to denote\n            high leverage (high_leverage_threshold=True).\n        (5) Added two more ways to compute the Cook's Distance (D) threshold:\n            * 'baseR': D &gt; 1 and D &gt; 0.5 (default)\n            * 'convention': D &gt; 4/n\n            * 'dof': D &gt; 4 / (n - k - 1)\n        (6) Fixed class name to conform to Pascal casing convention\n        (7) Fixed Residuals vs Leverage legend to work with loc='best'\n    \"\"\"\n\n    def __init__(\n        self,\n        results: Type[statsmodels.regression.linear_model.RegressionResultsWrapper],\n    ) -&gt; None:\n        \"\"\"\n        For a linear regression model, generates following diagnostic plots:\n\n        a. residual\n        b. qq\n        c. scale location and\n        d. leverage\n\n        and a table\n\n        e. vif\n\n        Args:\n            results (Type[statsmodels.regression.linear_model.RegressionResultsWrapper]):\n                must be instance of statsmodels.regression.linear_model object\n\n        Raises:\n            TypeError: if instance does not belong to above object\n\n        Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import statsmodels.formula.api as smf\n        &gt;&gt;&gt; x = np.linspace(-np.pi, np.pi, 100)\n        &gt;&gt;&gt; y = 3*x + 8 + np.random.normal(0,1, 100)\n        &gt;&gt;&gt; df = pd.DataFrame({'x':x, 'y':y})\n        &gt;&gt;&gt; res = smf.ols(formula= \"y ~ x\", data=df).fit()\n        &gt;&gt;&gt; cls = Linear_Reg_Diagnostic(res)\n        &gt;&gt;&gt; cls(plot_context=\"seaborn-paper\")\n\n        In case you do not need all plots you can also independently make an individual plot/table\n        in following ways\n\n        &gt;&gt;&gt; cls = Linear_Reg_Diagnostic(res)\n        &gt;&gt;&gt; cls.residual_plot()\n        &gt;&gt;&gt; cls.qq_plot()\n        &gt;&gt;&gt; cls.scale_location_plot()\n        &gt;&gt;&gt; cls.leverage_plot()\n        &gt;&gt;&gt; cls.vif_table()\n        \"\"\"\n\n        if (\n            isinstance(\n                results, statsmodels.regression.linear_model.RegressionResultsWrapper\n            )\n            is False\n        ):\n            raise TypeError(\n                \"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\"\n            )\n\n        self.results = maybe_unwrap_results(results)\n\n        self.y_true = self.results.model.endog\n        self.y_predict = self.results.fittedvalues\n        self.xvar = self.results.model.exog\n        self.xvar_names = self.results.model.exog_names\n\n        self.residual = np.array(self.results.resid)\n        influence = self.results.get_influence()\n        self.residual_norm = influence.resid_studentized_internal\n        self.leverage = influence.hat_matrix_diag\n        self.cooks_distance = influence.cooks_distance[0]\n        self.nparams = len(self.results.params)\n        self.nresids = len(self.residual_norm)\n\n    def __call__(self, plot_context=\"seaborn-v0_8-paper\", **kwargs):\n        # print(plt.style.available)\n        with plt.style.context(plot_context):\n            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n            self.residual_plot(ax=ax[0, 0])\n            self.qq_plot(ax=ax[0, 1])\n            self.scale_location_plot(ax=ax[1, 0])\n            self.leverage_plot(\n                ax=ax[1, 1],\n                high_leverage_threshold=kwargs.get(\"high_leverage_threshold\"),\n                cooks_threshold=kwargs.get(\"cooks_threshold\"),\n            )\n            # plt.show()\n\n        return (\n            self.vif_table(),\n            fig,\n            ax,\n        )\n\n    def residual_plot(self, ax=None):\n        \"\"\"\n        Residual vs Fitted Plot\n\n        Graphical tool to identify non-linearity.\n        (Roughly) Horizontal red line is an indicator that the residual has a linear pattern\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        sns.residplot(\n            x=self.y_predict,\n            y=self.residual,\n            lowess=True,\n            scatter_kws={\"alpha\": 0.5},\n            line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n            ax=ax,\n        )\n\n        # annotations\n        residual_abs = np.abs(self.residual)\n        abs_resid = np.flip(np.argsort(residual_abs), 0)\n        abs_resid_top_3 = abs_resid[:3]\n        for i in abs_resid_top_3:\n            ax.annotate(i, xy=(self.y_predict[i], self.residual[i]), color=\"C3\")\n\n        ax.set_title(\"Residuals vs Fitted\", fontweight=\"bold\")\n        ax.set_xlabel(\"Fitted values\")\n        ax.set_ylabel(\"Residuals\")\n        return ax\n\n    def qq_plot(self, ax=None):\n        \"\"\"\n        Standarized Residual vs Theoretical Quantile plot\n\n        Used to visually check if residuals are normally distributed.\n        Points spread along the diagonal line will suggest so.\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        QQ = ProbPlot(self.residual_norm)\n        fig = QQ.qqplot(line=\"45\", alpha=0.5, lw=1, ax=ax)\n\n        # annotations\n        abs_norm_resid = np.flip(np.argsort(np.abs(self.residual_norm)), 0)\n        abs_norm_resid_top_3 = abs_norm_resid[:3]\n        for i, x, y in self.__qq_top_resid(\n            QQ.theoretical_quantiles, abs_norm_resid_top_3\n        ):\n            ax.annotate(i, xy=(x, y), ha=\"right\", color=\"C3\")\n\n        ax.set_title(\"Normal Q-Q\", fontweight=\"bold\")\n        ax.set_xlabel(\"Theoretical Quantiles\")\n        ax.set_ylabel(\"Standardized Residuals\")\n        return ax\n\n    def scale_location_plot(self, ax=None):\n        \"\"\"\n        Sqrt(Standarized Residual) vs Fitted values plot\n\n        Used to check homoscedasticity of the residuals.\n        Horizontal line will suggest so.\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        residual_norm_abs_sqrt = np.sqrt(np.abs(self.residual_norm))\n\n        ax.scatter(self.y_predict, residual_norm_abs_sqrt, alpha=0.5)\n        sns.regplot(\n            x=self.y_predict,\n            y=residual_norm_abs_sqrt,\n            scatter=False,\n            ci=False,\n            lowess=True,\n            line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n            ax=ax,\n        )\n\n        # annotations\n        abs_sq_norm_resid = np.flip(np.argsort(residual_norm_abs_sqrt), 0)\n        abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n        for i in abs_sq_norm_resid_top_3:\n            ax.annotate(\n                i, xy=(self.y_predict[i], residual_norm_abs_sqrt[i]), color=\"C3\"\n            )\n\n        ax.set_title(\"Scale-Location\", fontweight=\"bold\")\n        ax.set_xlabel(\"Fitted values\")\n        ax.set_ylabel(r\"$\\sqrt{|\\mathrm{Standardized\\ Residuals}|}$\")\n        return ax\n\n    def leverage_plot(\n        self, ax=None, high_leverage_threshold=False, cooks_threshold=\"baseR\"\n    ):\n        \"\"\"\n        Residual vs Leverage plot\n\n        Points falling outside Cook's distance curves are considered observation that can sway the fit\n        aka are influential.\n        Good to have none outside the curves.\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        ax.scatter(self.leverage, self.residual_norm, alpha=0.5)\n\n        sns.regplot(\n            x=self.leverage,\n            y=self.residual_norm,\n            scatter=False,\n            ci=False,\n            lowess=True,\n            line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n            ax=ax,\n        )\n\n        # annotations\n        leverage_top_3 = np.flip(np.argsort(self.cooks_distance), 0)[:3]\n        for i in leverage_top_3:\n            ax.annotate(i, xy=(self.leverage[i], self.residual_norm[i]), color=\"C3\")\n\n        factors = []\n        if cooks_threshold == \"baseR\" or cooks_threshold is None:\n            factors = [1, 0.5]\n        elif cooks_threshold == \"convention\":\n            factors = [4 / self.nresids]\n        elif cooks_threshold == \"dof\":\n            factors = [4 / (self.nresids - self.nparams)]\n        else:\n            raise ValueError(\n                \"threshold_method must be one of the following: 'convention', 'dof', or 'baseR' (default)\"\n            )\n        for i, factor in enumerate(factors):\n            label = \"Cook's distance\" if i == 0 else None\n            xtemp, ytemp = self.__cooks_dist_line(factor)\n            ax.plot(xtemp, ytemp, label=label, lw=1.25, ls=\"--\", color=\"red\")\n            ax.plot(xtemp, np.negative(ytemp), lw=1.25, ls=\"--\", color=\"red\")\n\n        if high_leverage_threshold:\n            high_leverage = 2 * self.nparams / self.nresids\n            if max(self.leverage) &gt; high_leverage:\n                ax.axvline(\n                    high_leverage, label=\"High leverage\", ls=\"-.\", color=\"purple\", lw=1\n                )\n\n        ax.axhline(0, ls=\"dotted\", color=\"black\", lw=1.25)\n        ax.set_xlim(0, max(self.leverage) + 0.01)\n        ax.set_ylim(min(self.residual_norm) - 0.1, max(self.residual_norm) + 0.1)\n        ax.set_title(\"Residuals vs Leverage\", fontweight=\"bold\")\n        ax.set_xlabel(\"Leverage\")\n        ax.set_ylabel(\"Standardized Residuals\")\n        plt.legend(loc=\"best\")\n        return ax\n\n    def vif_table(self):\n        \"\"\"\n        VIF table\n\n        VIF, the variance inflation factor, is a measure of multicollinearity.\n        VIF &gt; 5 for a variable indicates that it is highly collinear with the\n        other input variables.\n        \"\"\"\n        vif_df = pd.DataFrame()\n        vif_df[\"Features\"] = self.xvar_names\n        vif_df[\"VIF Factor\"] = [\n            variance_inflation_factor(self.xvar, i) for i in range(self.xvar.shape[1])\n        ]\n\n        return vif_df.sort_values(\"VIF Factor\").round(2)\n\n    def __cooks_dist_line(self, factor):\n        \"\"\"\n        Helper function for plotting Cook's distance curves\n        \"\"\"\n        p = self.nparams\n        formula = lambda x: np.sqrt((factor * p * (1 - x)) / x)\n        x = np.linspace(0.001, max(self.leverage), 50)\n        y = formula(x)\n        return x, y\n\n    def __qq_top_resid(self, quantiles, top_residual_indices):\n        \"\"\"\n        Helper generator function yielding the index and coordinates\n        \"\"\"\n        offset = 0\n        quant_index = 0\n        previous_is_negative = None\n        for resid_index in top_residual_indices:\n            y = self.residual_norm[resid_index]\n            is_negative = y &lt; 0\n            if previous_is_negative == None or previous_is_negative == is_negative:\n                offset += 1\n            else:\n                quant_index -= offset\n            x = (\n                quantiles[quant_index]\n                if is_negative\n                else np.flip(quantiles, 0)[quant_index]\n            )\n            quant_index += 1\n            previous_is_negative = is_negative\n            yield resid_index, x, y\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.__cooks_dist_line","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.__cooks_dist_line(factor)</code>","text":"<p>Helper function for plotting Cook's distance curves</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __cooks_dist_line(self, factor):\n    \"\"\"\n    Helper function for plotting Cook's distance curves\n    \"\"\"\n    p = self.nparams\n    formula = lambda x: np.sqrt((factor * p * (1 - x)) / x)\n    x = np.linspace(0.001, max(self.leverage), 50)\n    y = formula(x)\n    return x, y\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.__init__","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.__init__(results)</code>","text":"<p>For a linear regression model, generates following diagnostic plots:</p> <p>a. residual b. qq c. scale location and d. leverage</p> <p>and a table</p> <p>e. vif</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Type[RegressionResultsWrapper]</code> <p>must be instance of statsmodels.regression.linear_model object</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>if instance does not belong to above object</p> <p>Example:</p> <p>import numpy as np import pandas as pd import statsmodels.formula.api as smf x = np.linspace(-np.pi, np.pi, 100) y = 3*x + 8 + np.random.normal(0,1, 100) df = pd.DataFrame({'x':x, 'y':y}) res = smf.ols(formula= \"y ~ x\", data=df).fit() cls = Linear_Reg_Diagnostic(res) cls(plot_context=\"seaborn-paper\")</p> <p>In case you do not need all plots you can also independently make an individual plot/table in following ways</p> <p>cls = Linear_Reg_Diagnostic(res) cls.residual_plot() cls.qq_plot() cls.scale_location_plot() cls.leverage_plot() cls.vif_table()</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __init__(\n    self,\n    results: Type[statsmodels.regression.linear_model.RegressionResultsWrapper],\n) -&gt; None:\n    \"\"\"\n    For a linear regression model, generates following diagnostic plots:\n\n    a. residual\n    b. qq\n    c. scale location and\n    d. leverage\n\n    and a table\n\n    e. vif\n\n    Args:\n        results (Type[statsmodels.regression.linear_model.RegressionResultsWrapper]):\n            must be instance of statsmodels.regression.linear_model object\n\n    Raises:\n        TypeError: if instance does not belong to above object\n\n    Example:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import statsmodels.formula.api as smf\n    &gt;&gt;&gt; x = np.linspace(-np.pi, np.pi, 100)\n    &gt;&gt;&gt; y = 3*x + 8 + np.random.normal(0,1, 100)\n    &gt;&gt;&gt; df = pd.DataFrame({'x':x, 'y':y})\n    &gt;&gt;&gt; res = smf.ols(formula= \"y ~ x\", data=df).fit()\n    &gt;&gt;&gt; cls = Linear_Reg_Diagnostic(res)\n    &gt;&gt;&gt; cls(plot_context=\"seaborn-paper\")\n\n    In case you do not need all plots you can also independently make an individual plot/table\n    in following ways\n\n    &gt;&gt;&gt; cls = Linear_Reg_Diagnostic(res)\n    &gt;&gt;&gt; cls.residual_plot()\n    &gt;&gt;&gt; cls.qq_plot()\n    &gt;&gt;&gt; cls.scale_location_plot()\n    &gt;&gt;&gt; cls.leverage_plot()\n    &gt;&gt;&gt; cls.vif_table()\n    \"\"\"\n\n    if (\n        isinstance(\n            results, statsmodels.regression.linear_model.RegressionResultsWrapper\n        )\n        is False\n    ):\n        raise TypeError(\n            \"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\"\n        )\n\n    self.results = maybe_unwrap_results(results)\n\n    self.y_true = self.results.model.endog\n    self.y_predict = self.results.fittedvalues\n    self.xvar = self.results.model.exog\n    self.xvar_names = self.results.model.exog_names\n\n    self.residual = np.array(self.results.resid)\n    influence = self.results.get_influence()\n    self.residual_norm = influence.resid_studentized_internal\n    self.leverage = influence.hat_matrix_diag\n    self.cooks_distance = influence.cooks_distance[0]\n    self.nparams = len(self.results.params)\n    self.nresids = len(self.residual_norm)\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.__qq_top_resid","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.__qq_top_resid(quantiles, top_residual_indices)</code>","text":"<p>Helper generator function yielding the index and coordinates</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __qq_top_resid(self, quantiles, top_residual_indices):\n    \"\"\"\n    Helper generator function yielding the index and coordinates\n    \"\"\"\n    offset = 0\n    quant_index = 0\n    previous_is_negative = None\n    for resid_index in top_residual_indices:\n        y = self.residual_norm[resid_index]\n        is_negative = y &lt; 0\n        if previous_is_negative == None or previous_is_negative == is_negative:\n            offset += 1\n        else:\n            quant_index -= offset\n        x = (\n            quantiles[quant_index]\n            if is_negative\n            else np.flip(quantiles, 0)[quant_index]\n        )\n        quant_index += 1\n        previous_is_negative = is_negative\n        yield resid_index, x, y\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.leverage_plot","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.leverage_plot(ax=None, high_leverage_threshold=False, cooks_threshold='baseR')</code>","text":"<p>Residual vs Leverage plot</p> <p>Points falling outside Cook's distance curves are considered observation that can sway the fit aka are influential. Good to have none outside the curves.</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def leverage_plot(\n    self, ax=None, high_leverage_threshold=False, cooks_threshold=\"baseR\"\n):\n    \"\"\"\n    Residual vs Leverage plot\n\n    Points falling outside Cook's distance curves are considered observation that can sway the fit\n    aka are influential.\n    Good to have none outside the curves.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.scatter(self.leverage, self.residual_norm, alpha=0.5)\n\n    sns.regplot(\n        x=self.leverage,\n        y=self.residual_norm,\n        scatter=False,\n        ci=False,\n        lowess=True,\n        line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n        ax=ax,\n    )\n\n    # annotations\n    leverage_top_3 = np.flip(np.argsort(self.cooks_distance), 0)[:3]\n    for i in leverage_top_3:\n        ax.annotate(i, xy=(self.leverage[i], self.residual_norm[i]), color=\"C3\")\n\n    factors = []\n    if cooks_threshold == \"baseR\" or cooks_threshold is None:\n        factors = [1, 0.5]\n    elif cooks_threshold == \"convention\":\n        factors = [4 / self.nresids]\n    elif cooks_threshold == \"dof\":\n        factors = [4 / (self.nresids - self.nparams)]\n    else:\n        raise ValueError(\n            \"threshold_method must be one of the following: 'convention', 'dof', or 'baseR' (default)\"\n        )\n    for i, factor in enumerate(factors):\n        label = \"Cook's distance\" if i == 0 else None\n        xtemp, ytemp = self.__cooks_dist_line(factor)\n        ax.plot(xtemp, ytemp, label=label, lw=1.25, ls=\"--\", color=\"red\")\n        ax.plot(xtemp, np.negative(ytemp), lw=1.25, ls=\"--\", color=\"red\")\n\n    if high_leverage_threshold:\n        high_leverage = 2 * self.nparams / self.nresids\n        if max(self.leverage) &gt; high_leverage:\n            ax.axvline(\n                high_leverage, label=\"High leverage\", ls=\"-.\", color=\"purple\", lw=1\n            )\n\n    ax.axhline(0, ls=\"dotted\", color=\"black\", lw=1.25)\n    ax.set_xlim(0, max(self.leverage) + 0.01)\n    ax.set_ylim(min(self.residual_norm) - 0.1, max(self.residual_norm) + 0.1)\n    ax.set_title(\"Residuals vs Leverage\", fontweight=\"bold\")\n    ax.set_xlabel(\"Leverage\")\n    ax.set_ylabel(\"Standardized Residuals\")\n    plt.legend(loc=\"best\")\n    return ax\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.qq_plot","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.qq_plot(ax=None)</code>","text":"<p>Standarized Residual vs Theoretical Quantile plot</p> <p>Used to visually check if residuals are normally distributed. Points spread along the diagonal line will suggest so.</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def qq_plot(self, ax=None):\n    \"\"\"\n    Standarized Residual vs Theoretical Quantile plot\n\n    Used to visually check if residuals are normally distributed.\n    Points spread along the diagonal line will suggest so.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    QQ = ProbPlot(self.residual_norm)\n    fig = QQ.qqplot(line=\"45\", alpha=0.5, lw=1, ax=ax)\n\n    # annotations\n    abs_norm_resid = np.flip(np.argsort(np.abs(self.residual_norm)), 0)\n    abs_norm_resid_top_3 = abs_norm_resid[:3]\n    for i, x, y in self.__qq_top_resid(\n        QQ.theoretical_quantiles, abs_norm_resid_top_3\n    ):\n        ax.annotate(i, xy=(x, y), ha=\"right\", color=\"C3\")\n\n    ax.set_title(\"Normal Q-Q\", fontweight=\"bold\")\n    ax.set_xlabel(\"Theoretical Quantiles\")\n    ax.set_ylabel(\"Standardized Residuals\")\n    return ax\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.residual_plot","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.residual_plot(ax=None)</code>","text":"<p>Residual vs Fitted Plot</p> <p>Graphical tool to identify non-linearity. (Roughly) Horizontal red line is an indicator that the residual has a linear pattern</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def residual_plot(self, ax=None):\n    \"\"\"\n    Residual vs Fitted Plot\n\n    Graphical tool to identify non-linearity.\n    (Roughly) Horizontal red line is an indicator that the residual has a linear pattern\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    sns.residplot(\n        x=self.y_predict,\n        y=self.residual,\n        lowess=True,\n        scatter_kws={\"alpha\": 0.5},\n        line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n        ax=ax,\n    )\n\n    # annotations\n    residual_abs = np.abs(self.residual)\n    abs_resid = np.flip(np.argsort(residual_abs), 0)\n    abs_resid_top_3 = abs_resid[:3]\n    for i in abs_resid_top_3:\n        ax.annotate(i, xy=(self.y_predict[i], self.residual[i]), color=\"C3\")\n\n    ax.set_title(\"Residuals vs Fitted\", fontweight=\"bold\")\n    ax.set_xlabel(\"Fitted values\")\n    ax.set_ylabel(\"Residuals\")\n    return ax\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.scale_location_plot","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.scale_location_plot(ax=None)</code>","text":"<p>Sqrt(Standarized Residual) vs Fitted values plot</p> <p>Used to check homoscedasticity of the residuals. Horizontal line will suggest so.</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def scale_location_plot(self, ax=None):\n    \"\"\"\n    Sqrt(Standarized Residual) vs Fitted values plot\n\n    Used to check homoscedasticity of the residuals.\n    Horizontal line will suggest so.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    residual_norm_abs_sqrt = np.sqrt(np.abs(self.residual_norm))\n\n    ax.scatter(self.y_predict, residual_norm_abs_sqrt, alpha=0.5)\n    sns.regplot(\n        x=self.y_predict,\n        y=residual_norm_abs_sqrt,\n        scatter=False,\n        ci=False,\n        lowess=True,\n        line_kws={\"color\": \"red\", \"lw\": 1, \"alpha\": 0.8},\n        ax=ax,\n    )\n\n    # annotations\n    abs_sq_norm_resid = np.flip(np.argsort(residual_norm_abs_sqrt), 0)\n    abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n    for i in abs_sq_norm_resid_top_3:\n        ax.annotate(\n            i, xy=(self.y_predict[i], residual_norm_abs_sqrt[i]), color=\"C3\"\n        )\n\n    ax.set_title(\"Scale-Location\", fontweight=\"bold\")\n    ax.set_xlabel(\"Fitted values\")\n    ax.set_ylabel(r\"$\\sqrt{|\\mathrm{Standardized\\ Residuals}|}$\")\n    return ax\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.LinearRegDiagnostic.vif_table","title":"<code>flexcv.model_postprocessing.LinearRegDiagnostic.vif_table()</code>","text":"<p>VIF table</p> <p>VIF, the variance inflation factor, is a measure of multicollinearity. VIF &gt; 5 for a variable indicates that it is highly collinear with the other input variables.</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def vif_table(self):\n    \"\"\"\n    VIF table\n\n    VIF, the variance inflation factor, is a measure of multicollinearity.\n    VIF &gt; 5 for a variable indicates that it is highly collinear with the\n    other input variables.\n    \"\"\"\n    vif_df = pd.DataFrame()\n    vif_df[\"Features\"] = self.xvar_names\n    vif_df[\"VIF Factor\"] = [\n        variance_inflation_factor(self.xvar, i) for i in range(self.xvar.shape[1])\n    ]\n\n    return vif_df.sort_values(\"VIF Factor\").round(2)\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.MERFModelPostProcessor","title":"<code>flexcv.model_postprocessing.MERFModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class MERFModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(\n        self, results_all_folds, fold_result, y_pred_base, run, *args, **kwargs\n    ):\n        \"\"\"Postprocessing function for the expectation maximization model (MERF).\n        Logs training and test plots to Neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n            {\n                \"max_iterations\": fold_result.fit_result.max_iterations,\n            }\n        )\n\n        plot.plot_merf_results(\n            y=fold_result.y_test,\n            yhat_base=y_pred_base,\n            yhat_me_model=fold_result.y_pred,\n            model=fold_result.fit_result,\n            model_name=kwargs[\"mixed_name\"],\n            max_iterations=fold_result.fit_result.max_iterations,\n            run=run,\n        )\n        plt.close()\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.MERFModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.MERFModelPostProcessor.__call__(results_all_folds, fold_result, y_pred_base, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the expectation maximization model (MERF). Logs training and test plots to Neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(\n    self, results_all_folds, fold_result, y_pred_base, run, *args, **kwargs\n):\n    \"\"\"Postprocessing function for the expectation maximization model (MERF).\n    Logs training and test plots to Neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n        {\n            \"max_iterations\": fold_result.fit_result.max_iterations,\n        }\n    )\n\n    plot.plot_merf_results(\n        y=fold_result.y_test,\n        yhat_base=y_pred_base,\n        yhat_me_model=fold_result.y_pred,\n        model=fold_result.fit_result,\n        model_name=kwargs[\"mixed_name\"],\n        max_iterations=fold_result.fit_result.max_iterations,\n        run=run,\n    )\n    plt.close()\n    return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.ModelPostProcessor","title":"<code>flexcv.model_postprocessing.ModelPostProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model postprocessing functions. All postprocessing functions must inherit from this class. Implement your own post processing routing by inheriting from this class and implementing the call method. The class instance is called in the cross validation loop.</p> <p>Methods:</p> Name Description <code>__call__ </code> <p>method to be implemented by the user. This method is called in the cross validation loop.</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class ModelPostProcessor(ABC):\n    \"\"\"Abstract base class for model postprocessing functions.\n    All postprocessing functions must inherit from this class.\n    Implement your own post processing routing by inheriting from this class and implementing the __call__ method.\n    The class instance is called in the cross validation loop.\n\n    Methods:\n        __call__ (abstract): method to be implemented by the user. This method is called in the cross validation loop.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def __call__(\n        self,\n        results_all_folds: dict,\n        fold_result: SingleModelFoldResult,\n        features: pd.Index | list[str] | np.ndarray[str],\n        run: Run,\n        *args,\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"This method is called in the cross validation loop.\n        Implement your own post processing routing by inheriting from this class and implementing the __call__ method.\n\n        Args:\n            results_all_folds (dict): A dict of results for all folds\n            fold_result (SingleModelFoldResult): A dataclass containing the results for the current fold\n            features (pd.Index | list[str] | np.ndarray[str]): The features used in the model\n            run (Run): neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            results_all_folds (dict): updated results dictionary\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.ModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.ModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>This method is called in the cross validation loop. Implement your own post processing routing by inheriting from this class and implementing the call method.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <code>dict</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <code>SingleModelFoldResult</code> <p>A dataclass containing the results for the current fold</p> required <code>features</code> <code>Index | list[str] | ndarray[str]</code> <p>The features used in the model</p> required <code>run</code> <code>Run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>results_all_folds</code> <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(\n    self,\n    results_all_folds: dict,\n    fold_result: SingleModelFoldResult,\n    features: pd.Index | list[str] | np.ndarray[str],\n    run: Run,\n    *args,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"This method is called in the cross validation loop.\n    Implement your own post processing routing by inheriting from this class and implementing the __call__ method.\n\n    Args:\n        results_all_folds (dict): A dict of results for all folds\n        fold_result (SingleModelFoldResult): A dataclass containing the results for the current fold\n        features (pd.Index | list[str] | np.ndarray[str]): The features used in the model\n        run (Run): neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        results_all_folds (dict): updated results dictionary\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.RandomForestModelPostProcessor","title":"<code>flexcv.model_postprocessing.RandomForestModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class RandomForestModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n        \"\"\"Postprocessing function for the random forest model.\n        Logs the parameters to Neptune.\n        Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n            fold_result.best_params\n        )\n        explainer = shap.TreeExplainer(fold_result.best_model)\n        shap_values = explainer.shap_values(fold_result.X_train)\n        plot.plot_shap(\n            shap_values=shap_values,\n            X=fold_result.X_train,\n            run=run,\n            log_destination=f\"{fold_result.model_name}/SHAP/Fold\",\n            dependency=False,\n        )\n        if \"shap_values\" not in results_all_folds[fold_result.model_name].keys():\n            results_all_folds[fold_result.model_name][\"shap_values\"] = [shap_values]\n        else:\n            results_all_folds[fold_result.model_name][\"shap_values\"].append(shap_values)\n\n        shap_values_test = explainer.shap_values(fold_result.X_test)\n        plot.plot_shap(\n            shap_values=shap_values_test,\n            X=fold_result.X_test,\n            run=run,\n            log_destination=f\"{fold_result.model_name}/SHAP/Test_Fold\",\n            dependency=False,\n        )\n\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.RandomForestModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.RandomForestModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the random forest model. Logs the parameters to Neptune. Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n    \"\"\"Postprocessing function for the random forest model.\n    Logs the parameters to Neptune.\n    Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n        fold_result.best_params\n    )\n    explainer = shap.TreeExplainer(fold_result.best_model)\n    shap_values = explainer.shap_values(fold_result.X_train)\n    plot.plot_shap(\n        shap_values=shap_values,\n        X=fold_result.X_train,\n        run=run,\n        log_destination=f\"{fold_result.model_name}/SHAP/Fold\",\n        dependency=False,\n    )\n    if \"shap_values\" not in results_all_folds[fold_result.model_name].keys():\n        results_all_folds[fold_result.model_name][\"shap_values\"] = [shap_values]\n    else:\n        results_all_folds[fold_result.model_name][\"shap_values\"].append(shap_values)\n\n    shap_values_test = explainer.shap_values(fold_result.X_test)\n    plot.plot_shap(\n        shap_values=shap_values_test,\n        X=fold_result.X_test,\n        run=run,\n        log_destination=f\"{fold_result.model_name}/SHAP/Test_Fold\",\n        dependency=False,\n    )\n\n    return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.SVRModelPostProcessor","title":"<code>flexcv.model_postprocessing.SVRModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class SVRModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n        \"\"\"Postprocessing function for the SVR model.\n        Logs the parameters to Neptune.\n        Logs permutation importance plots to Neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        with plt.style.context(\"ggplot\"):\n            run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n                fold_result.best_params\n            )\n\n            fig, tmp_df = permutation_importance(\n                fold_result.best_model,\n                fold_result.model_name,\n                fold_result.X_test,\n                fold_result.y_test,\n                fold_result.X_test.columns,\n            )\n            run[f\"{fold_result.model_name}/PermFeatImportance/Figures\"].append(fig)\n            run[f\"{fold_result.model_name}/PermFeatImportance/Table\"].append(File.as_html(tmp_df))\n\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.SVRModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.SVRModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the SVR model. Logs the parameters to Neptune. Logs permutation importance plots to Neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n    \"\"\"Postprocessing function for the SVR model.\n    Logs the parameters to Neptune.\n    Logs permutation importance plots to Neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    with plt.style.context(\"ggplot\"):\n        run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n            fold_result.best_params\n        )\n\n        fig, tmp_df = permutation_importance(\n            fold_result.best_model,\n            fold_result.model_name,\n            fold_result.X_test,\n            fold_result.y_test,\n            fold_result.X_test.columns,\n        )\n        run[f\"{fold_result.model_name}/PermFeatImportance/Figures\"].append(fig)\n        run[f\"{fold_result.model_name}/PermFeatImportance/Table\"].append(File.as_html(tmp_df))\n\n    return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.XGBoostModelPostProcessor","title":"<code>flexcv.model_postprocessing.XGBoostModelPostProcessor</code>","text":"<p>               Bases: <code>ModelPostProcessor</code></p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>class XGBoostModelPostProcessor(ModelPostProcessor):\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n        \"\"\"Postprocessing function for the xgboost model.\n        Logs the parameters to Neptune.\n        Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.\n\n        Args:\n            results_all_folds: A dict of results for all folds\n            fold_result: A dataclass containing the results for the current fold\n            run: neptune run object\n            *args: any additional arguments\n            **kwargs: any additional keyword arguments\n\n        Returns:\n            (dict): updated results dictionary\n        \"\"\"\n        explainer = shap.TreeExplainer(fold_result.best_model)\n        shap_values = explainer.shap_values(fold_result.X_train)\n        plot.plot_shap(\n            shap_values=shap_values,\n            X=fold_result.X_train,\n            run=run,\n            log_destination=f\"{fold_result.model_name}/SHAP/Train_Fold\",\n            dependency=False,\n        )\n        if \"shap_values\" not in results_all_folds[fold_result.model_name].keys():\n            results_all_folds[fold_result.model_name][\"shap_values\"] = [shap_values]\n        else:\n            results_all_folds[fold_result.model_name][\"shap_values\"].append(shap_values)\n\n        run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n            fold_result.best_params\n        )\n\n        shap_values_test = explainer.shap_values(fold_result.X_test)\n        plot.plot_shap(\n            shap_values=shap_values_test,\n            X=fold_result.X_test,\n            run=run,\n            log_destination=f\"{fold_result.model_name}/SHAP/Test_Fold\",\n            dependency=False,\n        )\n\n        return results_all_folds\n</code></pre>"},{"location":"reference/postprocessing/#flexcv.model_postprocessing.XGBoostModelPostProcessor.__call__","title":"<code>flexcv.model_postprocessing.XGBoostModelPostProcessor.__call__(results_all_folds, fold_result, features, run, *args, **kwargs)</code>","text":"<p>Postprocessing function for the xgboost model. Logs the parameters to Neptune. Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.</p> <p>Parameters:</p> Name Type Description Default <code>results_all_folds</code> <p>A dict of results for all folds</p> required <code>fold_result</code> <p>A dataclass containing the results for the current fold</p> required <code>run</code> <p>neptune run object</p> required <code>*args</code> <p>any additional arguments</p> <code>()</code> <code>**kwargs</code> <p>any additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>updated results dictionary</p> Source code in <code>flexcv/model_postprocessing.py</code> <pre><code>def __call__(self, results_all_folds, fold_result, features, run, *args, **kwargs):\n    \"\"\"Postprocessing function for the xgboost model.\n    Logs the parameters to Neptune.\n    Generates beeswarm plots from SHAP explainers for the training and test data and logs them to Neptune.\n\n    Args:\n        results_all_folds: A dict of results for all folds\n        fold_result: A dataclass containing the results for the current fold\n        run: neptune run object\n        *args: any additional arguments\n        **kwargs: any additional keyword arguments\n\n    Returns:\n        (dict): updated results dictionary\n    \"\"\"\n    explainer = shap.TreeExplainer(fold_result.best_model)\n    shap_values = explainer.shap_values(fold_result.X_train)\n    plot.plot_shap(\n        shap_values=shap_values,\n        X=fold_result.X_train,\n        run=run,\n        log_destination=f\"{fold_result.model_name}/SHAP/Train_Fold\",\n        dependency=False,\n    )\n    if \"shap_values\" not in results_all_folds[fold_result.model_name].keys():\n        results_all_folds[fold_result.model_name][\"shap_values\"] = [shap_values]\n    else:\n        results_all_folds[fold_result.model_name][\"shap_values\"].append(shap_values)\n\n    run[f\"{fold_result.model_name}/BestParams/{fold_result.k}\"] = pformat(\n        fold_result.best_params\n    )\n\n    shap_values_test = explainer.shap_values(fold_result.X_test)\n    plot.plot_shap(\n        shap_values=shap_values_test,\n        X=fold_result.X_test,\n        run=run,\n        log_destination=f\"{fold_result.model_name}/SHAP/Test_Fold\",\n        dependency=False,\n    )\n\n    return results_all_folds\n</code></pre>"},{"location":"reference/reference/","title":"Overview","text":"<p>Welcome to the reference documentation for <code>flexcv</code>. This section provides detailed information about the classes, methods, and functions in our codebase. It's intended for those who want to understand the inner workings of our project or use its APIs. Each module, class and function has its own section, with a description, a list of parameters and their types, and the return type. Examples of usage are provided where applicable. This documentation is automatically generated from the docstrings in our source code using <code>mkdocstrings</code>, so it's always up-to-date with the latest version of the code. Navigate through the sections to explore the different parts and modules of our project.</p>"},{"location":"reference/repeated-ref/","title":"Repeated Runs","text":""},{"location":"reference/repeated-ref/#flexcv.repeated","title":"<code>flexcv.repeated</code>","text":""},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV","title":"<code>flexcv.repeated.RepeatedCV</code>","text":"<p>               Bases: <code>CrossValidation</code></p> <p>Class for repeated cross-validation. Inherits from CrossValidation.</p> <p>Attributes:</p> Name Type Description <code>n_repeats</code> <code>int</code> <p>Number of repeated runs. (Default value = 3)</p> <code>seeds</code> <code>list[int]</code> <p>List of seeds for the repeated runs. (Default value = None)</p> <code>generator_seed</code> <code>int</code> <p>Seed to control generation of a list of seeds. (Default value = 42)</p> <code>parent_run</code> <code>run</code> <p>A run to track the repeated meta data. (Default value = None)</p> <code>children_runs</code> <code>list[run]</code> <p>A list of runs to track the single runs. (Default value = None)</p> <p>Methods:</p> Name Description <code>set_n_repeats</code> <p>method that sets the number of repeated runs</p> <code>set_seeds</code> <p>method that sets the random seeds for the repeated runs</p> <code>set_neptune</code> <p>method that sets the neptune credentials for the repeated runs and initializes them</p> <code>set_run</code> <p>in contrast to CrossValidation, the set_run method takes in a parent run and a list of children runs</p> <code>perform</code> <p>method that performs repeated cross-validation</p> Example <pre><code>from flexcv.synthesizer import generate_regression\nfrom flexcv.models import LinearModel\nfrom flexcv.model_mapping import ModelConfigDict, ModelMappingDict\nfrom flexcv.repeated import RepeatedCV\n\n# make sample data\nX, y, group, random_slopes = generate_regression(10, 100, n_slopes=1, noise_level=9.1e-2, random_seed=42), random_seed=42\n\n# create a model mapping\nmodel_map = ModelMappingDict(\n    {\n        \"LinearModel\": ModelConfigDict(\n            {\n                \"model\": LinearModel,\n                \"requires_formula\": True,\n            }\n        ),\n    }\n)\n\ncredentials = {your_neptune_credentials}\n\nrcv = (\n        RepeatedCV()\n        .set_data(X, y, group, dataset_name=\"ExampleData\")\n        .set_models(model_map)\n        .set_n_repeats(3)\n        .set_neptune(credentials)\n        .perform()\n        .get_results()\n)\n\nrcv.summary.to_excel(\"repeated_cv.xlsx\")  # save dataframe to excel file\n</code></pre> Source code in <code>flexcv/repeated.py</code> <pre><code>class RepeatedCV(CrossValidation):\n    \"\"\"Class for repeated cross-validation. Inherits from CrossValidation.\n\n    Attributes:\n        n_repeats (int): Number of repeated runs. (Default value = 3)\n        seeds (list[int]): List of seeds for the repeated runs. (Default value = None)\n        generator_seed (int): Seed to control generation of a list of seeds. (Default value = 42)\n        parent_run (neptune.run): A run to track the repeated meta data. (Default value = None)\n        children_runs list[neptune.run]: A list of runs to track the single runs. (Default value = None)\n\n    Methods:\n        set_n_repeats: method that sets the number of repeated runs\n        set_seeds: method that sets the random seeds for the repeated runs\n        set_neptune: method that sets the neptune credentials for the repeated runs and initializes them\n        set_run: in contrast to CrossValidation, the set_run method takes in a parent run and a list of children runs\n        perform: method that performs repeated cross-validation\n\n    Example:\n        ```python\n        from flexcv.synthesizer import generate_regression\n        from flexcv.models import LinearModel\n        from flexcv.model_mapping import ModelConfigDict, ModelMappingDict\n        from flexcv.repeated import RepeatedCV\n\n        # make sample data\n        X, y, group, random_slopes = generate_regression(10, 100, n_slopes=1, noise_level=9.1e-2, random_seed=42), random_seed=42\n\n        # create a model mapping\n        model_map = ModelMappingDict(\n            {\n                \"LinearModel\": ModelConfigDict(\n                    {\n                        \"model\": LinearModel,\n                        \"requires_formula\": True,\n                    }\n                ),\n            }\n        )\n\n        credentials = {your_neptune_credentials}\n\n        rcv = (\n                RepeatedCV()\n                .set_data(X, y, group, dataset_name=\"ExampleData\")\n                .set_models(model_map)\n                .set_n_repeats(3)\n                .set_neptune(credentials)\n                .perform()\n                .get_results()\n        )\n\n        rcv.summary.to_excel(\"repeated_cv.xlsx\")  # save dataframe to excel file\n        ```\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.n_repeats = 3\n        self.seeds = None\n        self.generator_seed = 42\n\n    def set_n_repeats(self, n_repeats: int):\n        \"\"\"Set the number of repeats for repeated cross-validation.\n\n        Args:\n            n_repeats(int): Number of repeated runs.\n\n        Returns:\n            (RepeatedCV): self\n\n        \"\"\"\n        self.n_repeats = n_repeats\n        return self\n\n    def set_neptune(self, neptune_credentials):\n        \"\"\"Set your neptune credentials and initialize the repeated runs automatically.\n        If you do not want to pass your credentials explicitly, use set_run and call init_repeated_runs from outside the class yourself.\n\n        Args:\n            neptune_credentials (dict): Your API token and project name.\n\n        Returns:\n            (RepeatedCV): self\n\n        \"\"\"\n        self.neptune_credentials = neptune_credentials\n        parent_run, children_runs = init_repeated_runs(\n            self.n_repeats, neptune_credentials\n        )\n        self.parent_run = parent_run\n        self.children_runs = children_runs\n        return self\n\n    def set_seeds(self, seeds=None, generator_seed=42):\n        \"\"\"Set the seeds for the repeated runs.\n        If no seeds are passed, a list of seeds is generated randomly.\n        The random seed for the random seed generation can be set via the generator_seed argument.\n        Therefore, the random seed for the repeated runs is set to the same value for each repeated run and is deterministic.\n\n        Args:\n            seeds (list[int]): The list of seeds to use in the repeats. (Default value = None)\n            generator_seed (int): Seed to control generation of a list of seeds. (Default value = 42)\n\n        Returns:\n            (RepeatedCV): self\n        \"\"\"\n        self.generator_seed = generator_seed\n        if seeds is None:\n            np.random.seed(generator_seed)\n            self.seeds = np.random.randint(42000, size=self.n_repeats).tolist()\n            return self\n\n        self.seeds = seeds\n        return self\n\n    def set_run(self, parent_run=None, children_run=None):\n        \"\"\"Use this method if you want to pass your own parent run and children runs.\n\n        Args:\n            parent_run (neptune.run): A run to track the repeated meta data. (Default value = None)\n            children_run list[neptune.run]: A list of runs to track the single runs. (Default value = None)\n\n        Returns:\n            (RepeatedCV): self\n\n        \"\"\"\n        self.parent_run = parent_run\n        self.children_run = children_run\n        return self\n\n    def _perform_repeats(self):\n        \"\"\"Performs repeated cross-validation.\n\n        Returns:\n            (pd.DataFrame): DataFrame with aggregated metrics.\n\n        \"\"\"\n        add_module_handlers(logger)\n\n        if hasattr(self, \"parent_run\") and self.parent_run is not None:\n            repeated_run = self.parent_run\n        else:\n            logger.info(\"No parent run found. Initializing dummy run.\")\n            repeated_run = Run()\n            self.parent_run = repeated_run\n\n        if not hasattr(self, \"children_runs\") or self.children_runs is None:\n            logger.info(\"No children runs found. Initializing dummy runs.\")\n            self.children_runs = [Run() for _ in range(self.n_repeats)]\n\n        repeated_id = repeated_run[\"sys/id\"].fetch()\n\n        if self.seeds is None:\n            logger.info(\n                f\"No seeds found. Initializing seeds with genearator seed {self.generator_seed}.\"\n            )\n            self.set_seeds()\n\n        run_ids = []\n        run_results = []\n        for seed, inner_run in zip(self.seeds, self.children_runs):\n            # handle neptune\n            inner_id = inner_run[\"sys/id\"].fetch()\n\n            # if inner_id is not a string, we are not using neptune and need to generate a run id\n            if not isinstance(inner_id, str):\n                i = -1\n                inner_id = f\"run_{i+1}\"\n\n            inner_run[\"HostRun\"] = repeated_id\n            inner_run[\"seed\"] = seed\n\n            # instantiate a new CrossValidation instance for each run\n            cv_in = CrossValidation()\n\n            # assign all necessary attributes of self to the inner cv instance\n            for key, value in self.config.items():\n                cv_in.config[key] = value\n\n            # set inner cv config\n            cv_in.config[\"random_seed\"] = seed\n            cv_in.config[\"run\"] = inner_run\n\n            # perform single run and get the results\n            results = cv_in.perform().get_results()\n\n            # append the run id and the run metric to the lists\n            run_ids.append(inner_id)\n            run_results.append(results)\n            inner_run.stop()\n            plt.close()\n        # run_dfs have the same column and index names and we\n        df = aggregate_(run_results)\n\n        # log the repeated run results to neptune\n        repeated_run[\"summary\"].upload(File.as_html(df))\n        repeated_run[\n            \"sys/description\"\n        ] = f\"Host run for repeated runs with {self.n_repeats} repeats. run_ids: {run_ids}\"\n        repeated_run[\"RelatedRuns\"] = \", \".join(run_ids)\n        repeated_run[\"seeds\"] = self.seeds\n        repeated_run[\"mapping\"] = self.config[\"mapping\"]\n        repeated_run.stop()\n        return df\n\n    def perform(self):\n        \"\"\"Wrapper method to perform repeated cross-validation. Overwrites the perform method of CrossValidation.\n\n        Returns:\n            (RepeatedCV): self\n\n        \"\"\"\n        if self.seeds is None:\n            self.set_seeds()\n        summary_df = self._perform_repeats()\n        self._summary_df = summary_df\n        return self\n\n    def get_results(self):\n        \"\"\"Returns the results of repeated cross-validation.\n\n        Returns:\n            (RepeatedResult): RepeatedResult object with summary property that returns a DataFrame with aggregated metrics.\n        \"\"\"\n        return RepeatedResult(self._summary_df)\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.get_results","title":"<code>flexcv.repeated.RepeatedCV.get_results()</code>","text":"<p>Returns the results of repeated cross-validation.</p> <p>Returns:</p> Type Description <code>RepeatedResult</code> <p>RepeatedResult object with summary property that returns a DataFrame with aggregated metrics.</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def get_results(self):\n    \"\"\"Returns the results of repeated cross-validation.\n\n    Returns:\n        (RepeatedResult): RepeatedResult object with summary property that returns a DataFrame with aggregated metrics.\n    \"\"\"\n    return RepeatedResult(self._summary_df)\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.perform","title":"<code>flexcv.repeated.RepeatedCV.perform()</code>","text":"<p>Wrapper method to perform repeated cross-validation. Overwrites the perform method of CrossValidation.</p> <p>Returns:</p> Type Description <code>RepeatedCV</code> <p>self</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def perform(self):\n    \"\"\"Wrapper method to perform repeated cross-validation. Overwrites the perform method of CrossValidation.\n\n    Returns:\n        (RepeatedCV): self\n\n    \"\"\"\n    if self.seeds is None:\n        self.set_seeds()\n    summary_df = self._perform_repeats()\n    self._summary_df = summary_df\n    return self\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.set_n_repeats","title":"<code>flexcv.repeated.RepeatedCV.set_n_repeats(n_repeats)</code>","text":"<p>Set the number of repeats for repeated cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>n_repeats(int)</code> <p>Number of repeated runs.</p> required <p>Returns:</p> Type Description <code>RepeatedCV</code> <p>self</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def set_n_repeats(self, n_repeats: int):\n    \"\"\"Set the number of repeats for repeated cross-validation.\n\n    Args:\n        n_repeats(int): Number of repeated runs.\n\n    Returns:\n        (RepeatedCV): self\n\n    \"\"\"\n    self.n_repeats = n_repeats\n    return self\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.set_neptune","title":"<code>flexcv.repeated.RepeatedCV.set_neptune(neptune_credentials)</code>","text":"<p>Set your neptune credentials and initialize the repeated runs automatically. If you do not want to pass your credentials explicitly, use set_run and call init_repeated_runs from outside the class yourself.</p> <p>Parameters:</p> Name Type Description Default <code>neptune_credentials</code> <code>dict</code> <p>Your API token and project name.</p> required <p>Returns:</p> Type Description <code>RepeatedCV</code> <p>self</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def set_neptune(self, neptune_credentials):\n    \"\"\"Set your neptune credentials and initialize the repeated runs automatically.\n    If you do not want to pass your credentials explicitly, use set_run and call init_repeated_runs from outside the class yourself.\n\n    Args:\n        neptune_credentials (dict): Your API token and project name.\n\n    Returns:\n        (RepeatedCV): self\n\n    \"\"\"\n    self.neptune_credentials = neptune_credentials\n    parent_run, children_runs = init_repeated_runs(\n        self.n_repeats, neptune_credentials\n    )\n    self.parent_run = parent_run\n    self.children_runs = children_runs\n    return self\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.set_run","title":"<code>flexcv.repeated.RepeatedCV.set_run(parent_run=None, children_run=None)</code>","text":"<p>Use this method if you want to pass your own parent run and children runs.</p> <p>Parameters:</p> Name Type Description Default <code>parent_run</code> <code>run</code> <p>A run to track the repeated meta data. (Default value = None)</p> <code>None</code> <code>children_run</code> <code>list[run]</code> <p>A list of runs to track the single runs. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>RepeatedCV</code> <p>self</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def set_run(self, parent_run=None, children_run=None):\n    \"\"\"Use this method if you want to pass your own parent run and children runs.\n\n    Args:\n        parent_run (neptune.run): A run to track the repeated meta data. (Default value = None)\n        children_run list[neptune.run]: A list of runs to track the single runs. (Default value = None)\n\n    Returns:\n        (RepeatedCV): self\n\n    \"\"\"\n    self.parent_run = parent_run\n    self.children_run = children_run\n    return self\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedCV.set_seeds","title":"<code>flexcv.repeated.RepeatedCV.set_seeds(seeds=None, generator_seed=42)</code>","text":"<p>Set the seeds for the repeated runs. If no seeds are passed, a list of seeds is generated randomly. The random seed for the random seed generation can be set via the generator_seed argument. Therefore, the random seed for the repeated runs is set to the same value for each repeated run and is deterministic.</p> <p>Parameters:</p> Name Type Description Default <code>seeds</code> <code>list[int]</code> <p>The list of seeds to use in the repeats. (Default value = None)</p> <code>None</code> <code>generator_seed</code> <code>int</code> <p>Seed to control generation of a list of seeds. (Default value = 42)</p> <code>42</code> <p>Returns:</p> Type Description <code>RepeatedCV</code> <p>self</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def set_seeds(self, seeds=None, generator_seed=42):\n    \"\"\"Set the seeds for the repeated runs.\n    If no seeds are passed, a list of seeds is generated randomly.\n    The random seed for the random seed generation can be set via the generator_seed argument.\n    Therefore, the random seed for the repeated runs is set to the same value for each repeated run and is deterministic.\n\n    Args:\n        seeds (list[int]): The list of seeds to use in the repeats. (Default value = None)\n        generator_seed (int): Seed to control generation of a list of seeds. (Default value = 42)\n\n    Returns:\n        (RepeatedCV): self\n    \"\"\"\n    self.generator_seed = generator_seed\n    if seeds is None:\n        np.random.seed(generator_seed)\n        self.seeds = np.random.randint(42000, size=self.n_repeats).tolist()\n        return self\n\n    self.seeds = seeds\n    return self\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedResult","title":"<code>flexcv.repeated.RepeatedResult</code>","text":"<p>Class for results of repeated cross-validation. Implements a summary property that returns a DataFrame with aggregated metrics.</p> Source code in <code>flexcv/repeated.py</code> <pre><code>class RepeatedResult:\n    \"\"\"Class for results of repeated cross-validation.\n    Implements a summary property that returns a DataFrame with aggregated metrics.\n    \"\"\"\n\n    def __init__(self, df):\n        \"\"\"Constructor method for RepeatedResult class.\n\n        Args:\n            df(pd.DataFrame): DataFrame with aggregated metrics.\n        \"\"\"\n        self._summary_df = df\n\n    @property\n    def summary(self):\n        \"\"\"Summary property that returns a DataFrame with aggregated metrics.\n\n        Returns:\n            (pd.DataFrame): DataFrame with aggregated metrics.\n\n        \"\"\"\n        return self._summary_df\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedResult.summary","title":"<code>flexcv.repeated.RepeatedResult.summary</code>  <code>property</code>","text":"<p>Summary property that returns a DataFrame with aggregated metrics.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with aggregated metrics.</p>"},{"location":"reference/repeated-ref/#flexcv.repeated.RepeatedResult.__init__","title":"<code>flexcv.repeated.RepeatedResult.__init__(df)</code>","text":"<p>Constructor method for RepeatedResult class.</p> <p>Parameters:</p> Name Type Description Default <code>df(pd.DataFrame)</code> <p>DataFrame with aggregated metrics.</p> required Source code in <code>flexcv/repeated.py</code> <pre><code>def __init__(self, df):\n    \"\"\"Constructor method for RepeatedResult class.\n\n    Args:\n        df(pd.DataFrame): DataFrame with aggregated metrics.\n    \"\"\"\n    self._summary_df = df\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.aggregate_","title":"<code>flexcv.repeated.aggregate_(repeated_runs)</code>","text":"<p>Aggregate the results of repeated runs into a single DataFrame. Therefore, the nested dict structure of the results is flattened. First, the model results are averaged over folds of individual runs. Second, the repeated (individual) runs are averaged.</p> <p>Parameters:</p> Name Type Description Default <code>repeated_runs</code> <p>list: List of results of repeated runs.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary statistics of repeated runs.</p> <p>Note:</p> <pre><code>The summary statistics are returned as a DataFrame with the following structure:\n- index: [aggregate]_[metric_name]\n- columns: [model_name]\n- values: [metric_value]\n</code></pre> Source code in <code>flexcv/repeated.py</code> <pre><code>def aggregate_(repeated_runs) -&gt; pd.DataFrame:\n    \"\"\"Aggregate the results of repeated runs into a single DataFrame.\n    Therefore, the nested dict structure of the results is flattened.\n    First, the model results are averaged over folds of individual runs.\n    Second, the repeated (individual) runs are averaged.\n\n    Args:\n        repeated_runs: list: List of results of repeated runs.\n\n    Returns:\n        (pd.DataFrame): Summary statistics of repeated runs.\n\n    Note:\n\n        The summary statistics are returned as a DataFrame with the following structure:\n        - index: [aggregate]_[metric_name]\n        - columns: [model_name]\n        - values: [metric_value]\n\n    \"\"\"\n\n    model_keys = list(repeated_runs[0].keys())\n\n    # we want the metrics keys\n    # index [0] gives us first run\n    # index [model_keys[0]] -&gt; gives us the dict for the first model\n    # index [\"fold_by_metrics\"] -&gt; gives us a dict[metrics: list]\n    # .keys() -&gt; gives us the keys of the metrics dict\n    result_keys = repeated_runs[0][model_keys[0]][\"folds_by_metrics\"].keys()\n    results = []\n    for result_key in result_keys:\n        tmp_df = pd.DataFrame(\n            [\n                pd.Series(\n                    [\n                        try_mean(run[model_key][\"folds_by_metrics\"][result_key])\n                        for model_key in model_keys\n                    ],\n                    index=model_keys,\n                )\n                for run in repeated_runs\n            ]\n        ).agg([\"mean\", \"std\"])\n        tmp_df.index = [f\"{result_key}_{index}\" for index in tmp_df.index]\n        results.append(tmp_df)\n    return pd.concat(results)\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.init_repeated_runs","title":"<code>flexcv.repeated.init_repeated_runs(n_repeats, neptune_credentials)</code>","text":"<p>Initialize a repeated run and n_repeats children runs. The children runs are linked to the parent run via the \"HostRun\" key. The children runs are initialized with the same credentials as the parent run.</p> <p>Parameters:</p> Name Type Description Default <code>n_repeats</code> <code>int</code> <p>Number of repeated runs.</p> required <code>use_neptune</code> <code>bool</code> <p>Whether to use neptune or not.</p> required <code>neptune_credentials</code> <code>dict</code> <p>Credentials for neptune.</p> required <p>Returns:</p> Type Description <code>tuple[Run, list[Run]]</code> <p>parent_run (Run), children_runs (list of n_repeats runs)</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def init_repeated_runs(n_repeats, neptune_credentials) -&gt; tuple:\n    \"\"\"Initialize a repeated run and n_repeats children runs.\n    The children runs are linked to the parent run via the \"HostRun\" key.\n    The children runs are initialized with the same credentials as the parent run.\n\n    Args:\n        n_repeats (int): Number of repeated runs.\n        use_neptune (bool): Whether to use neptune or not.\n        neptune_credentials (dict): Credentials for neptune.\n\n    Returns:\n        (tuple[Run, list[Run]]): parent_run (Run), children_runs (list of n_repeats runs)\n\n    \"\"\"\n    parent_run = neptune.init_run(**neptune_credentials)\n    children_runs = [neptune.init_run(**neptune_credentials) for _ in range(n_repeats)]\n    return parent_run, children_runs\n</code></pre>"},{"location":"reference/repeated-ref/#flexcv.repeated.try_mean","title":"<code>flexcv.repeated.try_mean(x)</code>","text":"<p>Mean function that handles NaN values for cases where the input contains string \"NaN\". If all elements in x are equal to \"NaN\", -99 is returned. If x contains \"NaN\" and other values, -999 is returned.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>(array-like): Input array.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean of the input array.</p> Source code in <code>flexcv/repeated.py</code> <pre><code>def try_mean(x):\n    \"\"\"Mean function that handles NaN values for cases where the input contains string \"NaN\".\n    If all elements in x are equal to \"NaN\", -99 is returned.\n    If x contains \"NaN\" and other values, -999 is returned.\n\n    Args:\n        x: (array-like): Input array.\n\n    Returns:\n        (float): Mean of the input array.\n\n    \"\"\"\n    from numpy.core._exceptions import UFuncTypeError\n\n    try:\n        return np.mean(x)\n    except (ValueError, UFuncTypeError):\n        # entered if x contains str \"NaN\"\n        # check if all elements in x are equal to \"NaN\"\n        if np.all([element == \"NaN\" for element in x]):\n            return -99\n        else:\n            return -999\n</code></pre>"},{"location":"reference/results-handling/","title":"Handling Results","text":""},{"location":"reference/results-handling/#flexcv.results_handling","title":"<code>flexcv.results_handling</code>","text":"<p>This module provides a CrossValidationResults class which can be used to summarize the results of CrossValidation.perform().</p>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults","title":"<code>flexcv.results_handling.CrossValidationResults</code>","text":"<p>               Bases: <code>dict</code></p> <p>A summary of the results of CrossValidation.perform(). Cross validate returns a dictionary of results for each model with the form: <pre><code>{\n    \"model_name_1\": {\n        \"model\": [model_1_fold_1, model_1_fold_2, ...],\n        \"parameters\": [params_1_fold_1, params_1_fold_2, ...],\n        \"metrics\": [\n            {\n                \"metric_1_fold_1\": metric_value_1_fold_1,\n                \"metric_2_fold_1\": metric_value_2_fold_1,\n                    ...\n            },\n            {\n                \"metric_1_fold_2\": metric_value_1_fold_2,\n                \"metric_2_fold_2\": metric_value_2_fold_2,\n                ...\n            },\n        ],\n        \"y_pred\": [y_pred_1_fold_1, y_pred_1_fold_2, ...],\n        \"y_test\": [y_test_1_fold_1, y_test_1_fold_2, ...],\n        \"y_pred_train\": [y_pred_train_1_fold_1, y_pred_train_1_fold_2, ...],\n        \"y_train\": [y_train_1_fold_1, y_train_1_fold_2, ...],\n    },\n    \"model_name_2\": {\n        ...\n    },\n    ...\n}\n</code></pre> This class is a wrapper around this dictionary which provides a summary of the results.</p> <ul> <li><code>_make_summary</code> computes the mean, median and standard deviation of the metrics for each model.</li> <li><code>_make_summary</code> is called the first time the summary property is accessed and the result is cached.</li> <li><code>_get_model</code> returns the model instance corresponding to the given model name.</li> </ul> Properties <p>summary (pd.DataFrame): Summary of the results.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>class CrossValidationResults(dict):\n    \"\"\"A summary of the results of CrossValidation.perform().\n    Cross validate returns a dictionary of results for each model with the form:\n    ```python\n    {\n        \"model_name_1\": {\n            \"model\": [model_1_fold_1, model_1_fold_2, ...],\n            \"parameters\": [params_1_fold_1, params_1_fold_2, ...],\n            \"metrics\": [\n                {\n                    \"metric_1_fold_1\": metric_value_1_fold_1,\n                    \"metric_2_fold_1\": metric_value_2_fold_1,\n                        ...\n                },\n                {\n                    \"metric_1_fold_2\": metric_value_1_fold_2,\n                    \"metric_2_fold_2\": metric_value_2_fold_2,\n                    ...\n                },\n            ],\n            \"y_pred\": [y_pred_1_fold_1, y_pred_1_fold_2, ...],\n            \"y_test\": [y_test_1_fold_1, y_test_1_fold_2, ...],\n            \"y_pred_train\": [y_pred_train_1_fold_1, y_pred_train_1_fold_2, ...],\n            \"y_train\": [y_train_1_fold_1, y_train_1_fold_2, ...],\n        },\n        \"model_name_2\": {\n            ...\n        },\n        ...\n    }\n    ```\n    This class is a wrapper around this dictionary which provides a summary of the results.\n\n    - `_make_summary` computes the mean, median and standard deviation of the metrics for each model.\n    - `_make_summary` is called the first time the summary property is accessed and the result is cached.\n    - `_get_model` returns the model instance corresponding to the given model name.\n\n    Properties:\n        summary (pd.DataFrame): Summary of the results.\n\n    \"\"\"\n\n    def __init__(self, results_dict):\n        super().__init__(results_dict)\n        self._summary = None\n        # TODO add some kind of id or description\n\n    def __repr__(self):\n        return f\"CrossValidationResults {pformat_dict(self)}\"\n\n    @property\n    def summary(self):\n        \"\"\"Property that returns a pandas dataframe with the fold values, mean, median and standard deviation of the metrics for each model.\"\"\"\n        if self._summary is None:\n            self._summary = self._make_summary()\n        return self._summary\n\n    def _make_summary(self):\n        \"\"\"Creates pandas dataframe with the fold values, mean, median and standard deviation of the metrics for each model.\n        Columns: model names\n        Multiindex from tuples: (fold id, metric)\n        1. It reorders the data from\n        ```python\n        \"metrics\": [\n                {\n                    \"metric_1_fold_1\": metric_value_1_fold_1,\n                    \"metric_2_fold_1\": metric_value_2_fold_1,\n                        ...\n                },\n                {\n                    \"metric_1_fold_2\": metric_value_1_fold_2,\n                    \"metric_2_fold_2\": metric_value_2_fold_2,\n                    ...\n                },\n            ],\n        ```\n        to the form\n        ```python\n        \"metrics\": {\n            \"metric_1\": [metric_value_1_fold_1, metric_value_1_fold_2, ...],\n            \"metric_2\": [metric_value_2_fold_1, metric_value_2_fold_2, ...],\n            ...\n        }\n        ```\n\n        \"\"\"\n        model_dfs = []\n        for model in self.keys():\n            metrics_dfs = []\n            for metric in self[model][\"folds_by_metrics\"].keys():\n                # collect the values for each fold for single metric\n                values_df = pd.DataFrame(\n                    self[model][\"folds_by_metrics\"][metric], columns=[metric]\n                )\n                values_df.index.name = \"Fold\"\n                values_df = add_summary_stats(values_df)\n                metrics_dfs.append(values_df)\n            # concatenate the dataframes for each metric\n            model_df = pd.concat(metrics_dfs, axis=1)\n            # reshape to long format and add model name as column\n            new_df = model_df.reset_index().melt(\n                id_vars=\"Fold\", var_name=\"Metric\", value_name=model\n            )\n            # set the index to fold and metric\n            new_df = new_df.set_index([\"Fold\", \"Metric\"])\n            model_dfs.append(new_df)\n        # concatenate the dataframes for each model\n        summary_df = pd.concat(model_dfs, axis=1)\n        return summary_df\n\n    def get_best_model_by_metric(\n        self, model_name=None, metric_name=\"mse\", direction=\"min\"\n    ):\n        \"\"\"Returns the model with the best metric value for the given metric.\n        Direction can be \"min\" or \"max\" and determines whether the best model is the one with the lowest or highest metric value.\n        E.g. for MSE, direction should be \"min\" and for R2, direction should be \"max\".\n\n        Args:\n          model_name (str): Name of the model (Default value = None)\n          metric_name (str): Name for the metric (Default value = \"mse\")\n          direction (str): Minimize or maximize. (Default value = \"min\")\n\n        Returns:\n            (object): The model with the best metric value for the given metric.\n        \"\"\"\n        assert direction in [\n            \"min\",\n            \"max\",\n        ], f\"direction must be 'min' or 'max', got {direction}\"\n        arg_func = np.argmin if direction == \"min\" else np.argmax\n        op_func = operator.lt if direction == \"min\" else operator.gt\n\n        best_model_name = None\n        best_metric_value = None\n        best_metric_index = None\n\n        iter_dict = (\n            self.items() if model_name is None else [(model_name, self[model_name])]\n        )\n\n        for model_name, model_results in iter_dict:\n            metric_values = model_results[\"metrics\"][metric_name]\n            metric_index = arg_func(metric_values)\n            metric_value = metric_values[metric_index]\n\n            if best_metric_value is None or op_func(metric_value, best_metric_value):\n                best_metric_value = metric_value\n                best_metric_index = metric_index\n                best_model_name = model_name\n\n        return self[best_model_name][\"model\"][best_metric_index]\n\n    def get_predictions(self, model_name, fold_id):\n        \"\"\"Returns the predictions for the given model and fold.\n\n        Args:\n          model_name (str): The name of the model.\n          fold_id (int): The id of the fold.\n\n        Returns:\n          (array-like): The predictions for the given model and fold.\n\n        \"\"\"\n        return self[model_name][\"y_pred\"][fold_id]\n\n    def get_true_values(self, model_name, fold_id):\n        \"\"\"Returns the true values for the given model and fold.\n\n        Args:\n          model_name (str): The name of the model.\n          fold_id (int): The id of the fold.\n\n        Returns:\n            (array-like): The true values for the given model and fold.\n        \"\"\"\n        return self[model_name][\"y_test\"][fold_id]\n\n    def get_training_predictions(self, model_name, fold_id):\n        \"\"\"Returns the predictions for the given model and fold.\n\n        Args:\n          model_name (str): The name of the model.\n          fold_id (int): The id of the fold.\n\n        Returns:\n            (array-like): The training predictions for the given model and fold.\n        \"\"\"\n        return self[model_name][\"y_pred_train\"][fold_id]\n\n    def get_training_true_values(self, model_name, fold_id):\n        \"\"\"Returns the true values for the given model and fold.\n\n        Args:\n          model_name (str): The name of the model.\n          fold_id (int): The id of the fold.\n\n        Returns:\n            (array-like): The training true values for the given model and fold.\n        \"\"\"\n        return self[model_name][\"y_train\"][fold_id]\n\n    def get_params(self, model_name, fold_id):\n        \"\"\"Returns the parameters for the given model and fold.\n        If the key is not found, returns None and will not raise an error.\n\n        Args:\n          model_name (str): The name of the model.\n          fold_id (int): The id of the fold.\n\n        Returns:\n            (dict): The parameters for the given model and fold.\n        \"\"\"\n        return self[model_name].get(\"parameters\", [None])[fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.summary","title":"<code>flexcv.results_handling.CrossValidationResults.summary</code>  <code>property</code>","text":"<p>Property that returns a pandas dataframe with the fold values, mean, median and standard deviation of the metrics for each model.</p>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_best_model_by_metric","title":"<code>flexcv.results_handling.CrossValidationResults.get_best_model_by_metric(model_name=None, metric_name='mse', direction='min')</code>","text":"<p>Returns the model with the best metric value for the given metric. Direction can be \"min\" or \"max\" and determines whether the best model is the one with the lowest or highest metric value. E.g. for MSE, direction should be \"min\" and for R2, direction should be \"max\".</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model (Default value = None)</p> <code>None</code> <code>metric_name</code> <code>str</code> <p>Name for the metric (Default value = \"mse\")</p> <code>'mse'</code> <code>direction</code> <code>str</code> <p>Minimize or maximize. (Default value = \"min\")</p> <code>'min'</code> <p>Returns:</p> Type Description <code>object</code> <p>The model with the best metric value for the given metric.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_best_model_by_metric(\n    self, model_name=None, metric_name=\"mse\", direction=\"min\"\n):\n    \"\"\"Returns the model with the best metric value for the given metric.\n    Direction can be \"min\" or \"max\" and determines whether the best model is the one with the lowest or highest metric value.\n    E.g. for MSE, direction should be \"min\" and for R2, direction should be \"max\".\n\n    Args:\n      model_name (str): Name of the model (Default value = None)\n      metric_name (str): Name for the metric (Default value = \"mse\")\n      direction (str): Minimize or maximize. (Default value = \"min\")\n\n    Returns:\n        (object): The model with the best metric value for the given metric.\n    \"\"\"\n    assert direction in [\n        \"min\",\n        \"max\",\n    ], f\"direction must be 'min' or 'max', got {direction}\"\n    arg_func = np.argmin if direction == \"min\" else np.argmax\n    op_func = operator.lt if direction == \"min\" else operator.gt\n\n    best_model_name = None\n    best_metric_value = None\n    best_metric_index = None\n\n    iter_dict = (\n        self.items() if model_name is None else [(model_name, self[model_name])]\n    )\n\n    for model_name, model_results in iter_dict:\n        metric_values = model_results[\"metrics\"][metric_name]\n        metric_index = arg_func(metric_values)\n        metric_value = metric_values[metric_index]\n\n        if best_metric_value is None or op_func(metric_value, best_metric_value):\n            best_metric_value = metric_value\n            best_metric_index = metric_index\n            best_model_name = model_name\n\n    return self[best_model_name][\"model\"][best_metric_index]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_params","title":"<code>flexcv.results_handling.CrossValidationResults.get_params(model_name, fold_id)</code>","text":"<p>Returns the parameters for the given model and fold. If the key is not found, returns None and will not raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>fold_id</code> <code>int</code> <p>The id of the fold.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The parameters for the given model and fold.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_params(self, model_name, fold_id):\n    \"\"\"Returns the parameters for the given model and fold.\n    If the key is not found, returns None and will not raise an error.\n\n    Args:\n      model_name (str): The name of the model.\n      fold_id (int): The id of the fold.\n\n    Returns:\n        (dict): The parameters for the given model and fold.\n    \"\"\"\n    return self[model_name].get(\"parameters\", [None])[fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_predictions","title":"<code>flexcv.results_handling.CrossValidationResults.get_predictions(model_name, fold_id)</code>","text":"<p>Returns the predictions for the given model and fold.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>fold_id</code> <code>int</code> <p>The id of the fold.</p> required <p>Returns:</p> Type Description <code>array - like</code> <p>The predictions for the given model and fold.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_predictions(self, model_name, fold_id):\n    \"\"\"Returns the predictions for the given model and fold.\n\n    Args:\n      model_name (str): The name of the model.\n      fold_id (int): The id of the fold.\n\n    Returns:\n      (array-like): The predictions for the given model and fold.\n\n    \"\"\"\n    return self[model_name][\"y_pred\"][fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_training_predictions","title":"<code>flexcv.results_handling.CrossValidationResults.get_training_predictions(model_name, fold_id)</code>","text":"<p>Returns the predictions for the given model and fold.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>fold_id</code> <code>int</code> <p>The id of the fold.</p> required <p>Returns:</p> Type Description <code>array - like</code> <p>The training predictions for the given model and fold.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_training_predictions(self, model_name, fold_id):\n    \"\"\"Returns the predictions for the given model and fold.\n\n    Args:\n      model_name (str): The name of the model.\n      fold_id (int): The id of the fold.\n\n    Returns:\n        (array-like): The training predictions for the given model and fold.\n    \"\"\"\n    return self[model_name][\"y_pred_train\"][fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_training_true_values","title":"<code>flexcv.results_handling.CrossValidationResults.get_training_true_values(model_name, fold_id)</code>","text":"<p>Returns the true values for the given model and fold.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>fold_id</code> <code>int</code> <p>The id of the fold.</p> required <p>Returns:</p> Type Description <code>array - like</code> <p>The training true values for the given model and fold.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_training_true_values(self, model_name, fold_id):\n    \"\"\"Returns the true values for the given model and fold.\n\n    Args:\n      model_name (str): The name of the model.\n      fold_id (int): The id of the fold.\n\n    Returns:\n        (array-like): The training true values for the given model and fold.\n    \"\"\"\n    return self[model_name][\"y_train\"][fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.CrossValidationResults.get_true_values","title":"<code>flexcv.results_handling.CrossValidationResults.get_true_values(model_name, fold_id)</code>","text":"<p>Returns the true values for the given model and fold.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model.</p> required <code>fold_id</code> <code>int</code> <p>The id of the fold.</p> required <p>Returns:</p> Type Description <code>array - like</code> <p>The true values for the given model and fold.</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def get_true_values(self, model_name, fold_id):\n    \"\"\"Returns the true values for the given model and fold.\n\n    Args:\n      model_name (str): The name of the model.\n      fold_id (int): The id of the fold.\n\n    Returns:\n        (array-like): The true values for the given model and fold.\n    \"\"\"\n    return self[model_name][\"y_test\"][fold_id]\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.MergedSummary","title":"<code>flexcv.results_handling.MergedSummary</code>","text":"<p>               Bases: <code>CrossValidationResults</code></p> Source code in <code>flexcv/results_handling.py</code> <pre><code>class MergedSummary(CrossValidationResults):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        cv_results_1,\n        cv_results_2,\n    ):\n        self.cv_results_1 = cv_results_1\n        self.cv_results_2 = cv_results_2\n\n    @property\n    def summary(self):\n        \"\"\" \"\"\"\n        return self._merge()\n\n    def _merge(self):\n        \"\"\"Merges the two summaries dataframes into one.\"\"\"\n        return pd.concat([self.cv_results_1.summary, self.cv_results_2.summary], axis=1)\n\n    def __add__(self, other):\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/results-handling/#flexcv.results_handling.MergedSummary.summary","title":"<code>flexcv.results_handling.MergedSummary.summary</code>  <code>property</code>","text":""},{"location":"reference/results-handling/#flexcv.results_handling.add_summary_stats","title":"<code>flexcv.results_handling.add_summary_stats(df)</code>","text":"<p>Add summary statistics to a pandas DataFrame. Calculates the mean, median and standard deviation on copies slices of the original data and adds them as rows to the DataFrame. This makes sure, that the summary statistics are not sequentially dependent on each other.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>pd.DataFrame: Input data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data with added summary statistics</p> Source code in <code>flexcv/results_handling.py</code> <pre><code>def add_summary_stats(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add summary statistics to a pandas DataFrame.\n    Calculates the mean, median and standard deviation on copies slices of the original data and adds them as rows to the DataFrame.\n    This makes sure, that the summary statistics are not sequentially dependent on each other.\n\n    Args:\n      df: pd.DataFrame: Input data.\n\n    Returns:\n      (pd.DataFrame): Data with added summary statistics\n\n    \"\"\"\n    original_fold_slice = df.copy(deep=True)\n    df.loc[\"mean\"] = original_fold_slice.mean(skipna=True)\n    df.loc[\"median\"] = original_fold_slice.median(skipna=True)\n    df.loc[\"std\"] = original_fold_slice.std(skipna=True)\n    return df\n</code></pre>"},{"location":"reference/results-handling/#flexcv.fold_results_handling","title":"<code>flexcv.fold_results_handling</code>","text":""},{"location":"reference/results-handling/#flexcv.fold_results_handling.SingleModelFoldResult","title":"<code>flexcv.fold_results_handling.SingleModelFoldResult</code>  <code>dataclass</code>","text":"<p>This dataclass is used to store the fold data as well as the predictions of a single model in a single fold. It's make_results method is used to evaluate the model with the metrics and log the results to Neptune.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>The fold number.</p> <code>model_name</code> <code>str</code> <p>The name of the model.</p> <code>best_model</code> <code>object</code> <p>The best model after inner cv or the model when skipping inner cv.</p> <code>best_params</code> <code>dict | Any</code> <p>The best parameters.</p> <code>y_pred</code> <code>Series</code> <p>The predictions of the model.</p> <code>y_test</code> <code>Series</code> <p>The test data.</p> <code>X_test</code> <code>DataFrame</code> <p>The test data.</p> <code>y_train</code> <code>Series</code> <p>The train data.</p> <code>y_pred_train</code> <code>Series</code> <p>The predictions of the model.</p> <code>X_train</code> <code>DataFrame</code> <p>The train data.</p> <code>fit_result</code> <code>Any</code> <p>The result of the fit method of the model.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the fit method. (default: None)</p> Source code in <code>flexcv/fold_results_handling.py</code> <pre><code>@dataclass\nclass SingleModelFoldResult:\n    \"\"\"This dataclass is used to store the fold data as well as the predictions of a single model in a single fold.\n    It's make_results method is used to evaluate the model with the metrics and log the results to Neptune.\n\n    Attributes:\n        k (int): The fold number.\n        model_name (str): The name of the model.\n        best_model (object): The best model after inner cv or the model when skipping inner cv.\n        best_params (dict | Any): The best parameters.\n        y_pred (pd.Series): The predictions of the model.\n        y_test (pd.Series): The test data.\n        X_test (pd.DataFrame): The test data.\n        y_train (pd.Series): The train data.\n        y_pred_train (pd.Series): The predictions of the model.\n        X_train (pd.DataFrame): The train data.\n        fit_result (Any): The result of the fit method of the model.\n        fit_kwargs (dict): Additional keyword arguments to pass to the fit method. (default: None)\n\n    \"\"\"\n\n    k: int\n    model_name: str\n    best_model: object\n    best_params: dict | Any\n    y_pred: pd.Series\n    y_test: pd.Series\n    X_test: pd.DataFrame\n    y_train: pd.Series\n    y_pred_train: pd.Series\n    X_train: pd.DataFrame\n    fit_result: Any\n    fit_kwargs: dict = None\n\n    def make_results(\n        self,\n        run,\n        results_all_folds,\n        study: Study | None,\n        metrics: MetricsDict = METRICS,\n    ):\n        \"\"\"This method is used to evaluate the model with the metrics and log the results to Neptune.\n\n        Args:\n          run (neptune.run): Neptune run object.\n          results_all_folds (dict): Dictionary containing the results of all models and folds.\n          study (optuna.study): Optuna study object.\n          metrics (dict): Dictionary containing the metrics to be evaluated.\n\n        Returns:\n          (dict): Dictionary containing the results of all models and folds.\n        \"\"\"\n\n        def res_vs_fitted_plot(y_test, y_pred):\n            fig = plt.figure()\n            residuals = y_test - y_pred\n            plt.scatter(y_pred, residuals)\n            plt.xlabel(\"Fitted values\")\n            plt.ylabel(\"Residuals\")\n            plt.title(\"Residuals vs Fitted Values\")\n            return fig\n\n        if metrics is None:\n            metrics = METRICS\n\n        inner_cv_exists = True\n        if not study is None:\n            df = study.trials_dataframe()\n            best_idx = study.best_trial.number\n            mse_in_train = -df.iloc[best_idx][\"user_attrs_mean_train_score\"]\n            mse_in_test = -df.iloc[best_idx][\"user_attrs_mean_test_score\"]\n            of = -df.iloc[best_idx][\"user_attrs_mean_OF_score\"]\n        else:\n            inner_cv_exists = False\n            mse_in_train = np.nan\n            mse_in_test = np.nan\n            of = np.nan\n\n        eval_metrics = {}\n        for metric_name, metric_func in metrics.items():\n            eval_metrics[metric_name] = metric_func(self.y_test, self.y_pred)\n            eval_metrics[metric_name + \"_train\"] = metric_func(\n                self.y_train, self.y_pred_train\n            )\n\n        eval_metrics[\"mse_in_test\"] = mse_in_test\n        eval_metrics[\"mse_in_train\"] = mse_in_train\n\n        if self.model_name not in results_all_folds:\n            results_all_folds[self.model_name] = {\n                \"model\": [],\n                \"parameters\": [],\n                \"metrics\": [],\n                \"folds_by_metrics\": {},\n                \"y_pred\": [],\n                \"y_test\": [],\n                \"y_pred_train\": [],\n                \"y_train\": [],\n            }\n\n        for metric_name in eval_metrics.keys():\n            results_all_folds[self.model_name][\"folds_by_metrics\"].setdefault(\n                metric_name, []\n            ).append(eval_metrics[metric_name])\n\n        results_all_folds[self.model_name][\"metrics\"].append(eval_metrics)\n        results_all_folds[self.model_name][\"model\"].append(self.best_model)\n        results_all_folds[self.model_name][\"parameters\"].append(self.best_params)\n        results_all_folds[self.model_name][\"y_pred\"].append(self.y_pred)\n        results_all_folds[self.model_name][\"y_test\"].append(self.y_test)\n        results_all_folds[self.model_name][\"y_pred_train\"].append(self.y_pred_train)\n        results_all_folds[self.model_name][\"y_train\"].append(self.y_train)\n\n        for key, value in eval_metrics.items():\n            run[f\"{self.model_name}/{key}\"].append(value)\n\n        run[f\"{self.model_name}/ObjectiveValue\"].append(of)\n\n        # saving the model\n        # check if model has method save_raw -&gt; important for xgboost\n\n        try:\n            self.best_model.save_model(f\"{self.model_name}_{self.k}.json\")\n            run[f\"{self.model_name}/Model/{self.k}\"].upload(f\"{self.model_name}_{self.k}.json\")\n            Path(f\"{self.model_name}_{self.k}.json\").unlink()\n        except (AttributeError, KeyError):\n            # AttributeError is raised when model has no method save_raw\n            # KeyError is raised when model has method save_raw but no raw_format='json'\n            run[f\"{self.model_name}/Model/{self.k}\"].upload(File.as_pickle(self.best_model))\n\n        try:\n            run[f\"{self.model_name}/Parameters/\"] = stringify_unsupported(\n                npt_utils.get_estimator_params(self.best_model)\n            )\n        except (RuntimeError, TypeError):\n            # is raised when model is not a scikit-learn model\n            run[f\"{self.model_name}/Parameters/\"].append(pformat(self.best_params))\n\n        run[f\"{self.model_name}/ResidualPlot/\"].append(\n            res_vs_fitted_plot(self.y_test, self.y_pred)\n        )\n\n        try:\n            run[\n                f\"{self.model_name}/RegressionSummary/{self.k}\"\n            ] = npt_utils.create_regressor_summary(\n                self.best_model, self.X_train, self.X_test, self.y_train, self.y_test\n            )\n        except (KeyError, TypeError, RuntimeError, AssertionError) as e:\n            # is raised when model is not a scikit-learn model\n            logger.info(\n                f\"Regression summary not available for model {self.model_name}. Skipping.\\n{e}\")\n            run[f\"{self.model_name}/RegressionSummary/{self.k}\"] = f\"Not available: {e}\"\n\n        plt.close()\n        return results_all_folds\n</code></pre>"},{"location":"reference/results-handling/#flexcv.fold_results_handling.SingleModelFoldResult.make_results","title":"<code>flexcv.fold_results_handling.SingleModelFoldResult.make_results(run, results_all_folds, study, metrics=METRICS)</code>","text":"<p>This method is used to evaluate the model with the metrics and log the results to Neptune.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>run</code> <p>Neptune run object.</p> required <code>results_all_folds</code> <code>dict</code> <p>Dictionary containing the results of all models and folds.</p> required <code>study</code> <code>study</code> <p>Optuna study object.</p> required <code>metrics</code> <code>dict</code> <p>Dictionary containing the metrics to be evaluated.</p> <code>METRICS</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the results of all models and folds.</p> Source code in <code>flexcv/fold_results_handling.py</code> <pre><code>def make_results(\n    self,\n    run,\n    results_all_folds,\n    study: Study | None,\n    metrics: MetricsDict = METRICS,\n):\n    \"\"\"This method is used to evaluate the model with the metrics and log the results to Neptune.\n\n    Args:\n      run (neptune.run): Neptune run object.\n      results_all_folds (dict): Dictionary containing the results of all models and folds.\n      study (optuna.study): Optuna study object.\n      metrics (dict): Dictionary containing the metrics to be evaluated.\n\n    Returns:\n      (dict): Dictionary containing the results of all models and folds.\n    \"\"\"\n\n    def res_vs_fitted_plot(y_test, y_pred):\n        fig = plt.figure()\n        residuals = y_test - y_pred\n        plt.scatter(y_pred, residuals)\n        plt.xlabel(\"Fitted values\")\n        plt.ylabel(\"Residuals\")\n        plt.title(\"Residuals vs Fitted Values\")\n        return fig\n\n    if metrics is None:\n        metrics = METRICS\n\n    inner_cv_exists = True\n    if not study is None:\n        df = study.trials_dataframe()\n        best_idx = study.best_trial.number\n        mse_in_train = -df.iloc[best_idx][\"user_attrs_mean_train_score\"]\n        mse_in_test = -df.iloc[best_idx][\"user_attrs_mean_test_score\"]\n        of = -df.iloc[best_idx][\"user_attrs_mean_OF_score\"]\n    else:\n        inner_cv_exists = False\n        mse_in_train = np.nan\n        mse_in_test = np.nan\n        of = np.nan\n\n    eval_metrics = {}\n    for metric_name, metric_func in metrics.items():\n        eval_metrics[metric_name] = metric_func(self.y_test, self.y_pred)\n        eval_metrics[metric_name + \"_train\"] = metric_func(\n            self.y_train, self.y_pred_train\n        )\n\n    eval_metrics[\"mse_in_test\"] = mse_in_test\n    eval_metrics[\"mse_in_train\"] = mse_in_train\n\n    if self.model_name not in results_all_folds:\n        results_all_folds[self.model_name] = {\n            \"model\": [],\n            \"parameters\": [],\n            \"metrics\": [],\n            \"folds_by_metrics\": {},\n            \"y_pred\": [],\n            \"y_test\": [],\n            \"y_pred_train\": [],\n            \"y_train\": [],\n        }\n\n    for metric_name in eval_metrics.keys():\n        results_all_folds[self.model_name][\"folds_by_metrics\"].setdefault(\n            metric_name, []\n        ).append(eval_metrics[metric_name])\n\n    results_all_folds[self.model_name][\"metrics\"].append(eval_metrics)\n    results_all_folds[self.model_name][\"model\"].append(self.best_model)\n    results_all_folds[self.model_name][\"parameters\"].append(self.best_params)\n    results_all_folds[self.model_name][\"y_pred\"].append(self.y_pred)\n    results_all_folds[self.model_name][\"y_test\"].append(self.y_test)\n    results_all_folds[self.model_name][\"y_pred_train\"].append(self.y_pred_train)\n    results_all_folds[self.model_name][\"y_train\"].append(self.y_train)\n\n    for key, value in eval_metrics.items():\n        run[f\"{self.model_name}/{key}\"].append(value)\n\n    run[f\"{self.model_name}/ObjectiveValue\"].append(of)\n\n    # saving the model\n    # check if model has method save_raw -&gt; important for xgboost\n\n    try:\n        self.best_model.save_model(f\"{self.model_name}_{self.k}.json\")\n        run[f\"{self.model_name}/Model/{self.k}\"].upload(f\"{self.model_name}_{self.k}.json\")\n        Path(f\"{self.model_name}_{self.k}.json\").unlink()\n    except (AttributeError, KeyError):\n        # AttributeError is raised when model has no method save_raw\n        # KeyError is raised when model has method save_raw but no raw_format='json'\n        run[f\"{self.model_name}/Model/{self.k}\"].upload(File.as_pickle(self.best_model))\n\n    try:\n        run[f\"{self.model_name}/Parameters/\"] = stringify_unsupported(\n            npt_utils.get_estimator_params(self.best_model)\n        )\n    except (RuntimeError, TypeError):\n        # is raised when model is not a scikit-learn model\n        run[f\"{self.model_name}/Parameters/\"].append(pformat(self.best_params))\n\n    run[f\"{self.model_name}/ResidualPlot/\"].append(\n        res_vs_fitted_plot(self.y_test, self.y_pred)\n    )\n\n    try:\n        run[\n            f\"{self.model_name}/RegressionSummary/{self.k}\"\n        ] = npt_utils.create_regressor_summary(\n            self.best_model, self.X_train, self.X_test, self.y_train, self.y_test\n        )\n    except (KeyError, TypeError, RuntimeError, AssertionError) as e:\n        # is raised when model is not a scikit-learn model\n        logger.info(\n            f\"Regression summary not available for model {self.model_name}. Skipping.\\n{e}\")\n        run[f\"{self.model_name}/RegressionSummary/{self.k}\"] = f\"Not available: {e}\"\n\n    plt.close()\n    return results_all_folds\n</code></pre>"},{"location":"reference/split/","title":"Split Methods","text":""},{"location":"reference/split/#flexcv.split","title":"<code>flexcv.split</code>","text":"<p>In order to switch cross validation split methods dynamically we need to implement a function that returns the correct cross validation split function. This is necessary because the split methods may have different numbers of arguments. This module also implements a custom stratified cross validation split method for continuous target variables and a custom stratified group cross validation split method for continuous target variables that incorporates grouping information.</p>"},{"location":"reference/split/#flexcv.split.CrossValMethod","title":"<code>flexcv.split.CrossValMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum class to assign CrossValMethods to the cross_val() function. This is useful to return the correct splitting function depending on the cross val method.</p> Members <ul> <li><code>KFOLD</code>: Regular sklearn <code>KFold</code> cross validation. No grouping information is used.</li> <li><code>GROUP</code>: Regular sklearn <code>GroupKFold</code> cross validation. Grouping information is used.</li> <li><code>STRAT</code>: Regular sklearn <code>StratifiedKFold</code> cross validation. No grouping information is used.</li> <li><code>STRATGROUP</code>: Regular sklearn <code>StratifiedGroupKFold</code> cross validation. Grouping information is used.</li> <li><code>CONTISTRAT</code>: Stratified cross validation for continuous targets. No grouping information is used.</li> <li><code>CONTISTRATGROUP</code>: Stratified cross validation for continuous targets. Grouping information is used.</li> <li><code>CONCATSTRATKFOLD</code>: Stratified cross validation. Leaky stratification on element-wise-concatenated target and group labels.</li> </ul> Source code in <code>flexcv/split.py</code> <pre><code>class CrossValMethod(Enum):\n    \"\"\"Enum class to assign CrossValMethods to the cross_val() function.\n    This is useful to return the correct splitting function depending on the cross val method.\n\n    Members:\n        - `KFOLD`: Regular sklearn `KFold` cross validation. No grouping information is used.\n        - `GROUP`: Regular sklearn `GroupKFold` cross validation. Grouping information is used.\n        - `STRAT`: Regular sklearn `StratifiedKFold` cross validation. No grouping information is used.\n        - `STRATGROUP`: Regular sklearn `StratifiedGroupKFold` cross validation. Grouping information is used.\n        - `CONTISTRAT`: Stratified cross validation for continuous targets. No grouping information is used.\n        - `CONTISTRATGROUP`: Stratified cross validation for continuous targets. Grouping information is used.\n        - `CONCATSTRATKFOLD`: Stratified cross validation. Leaky stratification on element-wise-concatenated target and group labels.\n    \"\"\"\n\n    KFOLD = \"KFold\"\n    GROUP = \"GroupKFold\"\n    STRAT = \"StratifiedKFold\"\n    STRATGROUP = \"StratifiedGroupKFold\"\n    CONTISTRAT = \"ContinuousStratifiedKFold\"\n    CONTISTRATGROUP = \"ContinuousStratifiedGroupKFold\"\n    CONCATSTRATKFOLD = \"ConcatenatedStratifiedKFold\"\n</code></pre>"},{"location":"reference/split/#flexcv.split.make_cross_val_split","title":"<code>flexcv.split.make_cross_val_split(*, groups, method, n_splits=5, random_state=42)</code>","text":"<p>This function creates and returns a callable cross validation splitter based on the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>Series | None</code> <p>A pd.Series containing the grouping information for the samples.</p> required <code>method</code> <code>CrossValMethod</code> <p>A CrossValMethod enum value specifying the cross validation method to use.</p> required <code>n_splits</code> <code>int</code> <p>Number of splits (Default value = 5)</p> <code>5</code> <code>random_state</code> <code>int</code> <p>A random seed to control random processes (Default value = 42)</p> <code>42</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A callable cross validation splitter based on the specified method.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the given method is not one of KFOLD</p> Source code in <code>flexcv/split.py</code> <pre><code>def make_cross_val_split(\n    *,\n    groups: pd.Series | None,\n    method: CrossValMethod,\n    n_splits: int = 5,\n    random_state: int = 42,\n) -&gt; Callable[..., Iterator[tuple[ndarray, ndarray]]]:\n    \"\"\"This function creates and returns a callable cross validation splitter based on the specified method.\n\n    Args:\n      groups (pd.Series | None): A pd.Series containing the grouping information for the samples.\n      method (CrossValMethod): A CrossValMethod enum value specifying the cross validation method to use.\n      n_splits (int): Number of splits (Default value = 5)\n      random_state (int): A random seed to control random processes (Default value = 42)\n\n    Returns:\n      (Callable): A callable cross validation splitter based on the specified method.\n\n    Raises:\n      (TypeError): If the given method is not one of KFOLD\n\n    \"\"\"\n\n    match method:\n        case CrossValMethod.KFOLD:\n            kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n            return kf.split\n\n        case CrossValMethod.STRAT:\n            strat_skf = StratifiedKFold(\n                n_splits=n_splits, random_state=random_state, shuffle=True\n            )\n            return strat_skf.split\n\n        case CrossValMethod.CONTISTRAT:\n            conti_skf = ContinuousStratifiedKFold(\n                n_splits=n_splits, random_state=random_state, shuffle=True\n            )\n            return conti_skf.split\n\n        case CrossValMethod.GROUP:\n            gkf = GroupKFold(n_splits=n_splits)\n            return partial(gkf.split, groups=groups)\n\n        case CrossValMethod.STRATGROUP:\n            strat_gkf = StratifiedGroupKFold(\n                n_splits=n_splits, random_state=random_state, shuffle=True\n            )\n            return partial(strat_gkf.split, groups=groups)\n\n        case CrossValMethod.CONTISTRATGROUP:\n            conti_sgkf = ContinuousStratifiedGroupKFold(\n                n_splits=n_splits, random_state=random_state, shuffle=True\n            )\n            return partial(conti_sgkf.split, groups=groups)\n\n        case CrossValMethod.CONCATSTRATKFOLD:\n            concat_skf = ConcatenatedStratifiedKFold(\n                n_splits=n_splits, random_state=random_state, shuffle=True\n            )\n            return partial(concat_skf.split, groups=groups)\n\n        case _:\n            is_cross_validator = isinstance(method, BaseCrossValidator)\n            is_groups_consumer = isinstance(method, GroupsConsumerMixin)\n\n            if is_cross_validator and is_groups_consumer:\n                return partial(method.split, groups=groups)\n\n            if is_cross_validator:\n                return method.split\n\n            if isinstance(method, Iterator):\n                return method\n\n            else:\n                raise TypeError(\"Invalid Cross Validation method given.\")\n</code></pre>"},{"location":"reference/split/#flexcv.split.string_to_crossvalmethod","title":"<code>flexcv.split.string_to_crossvalmethod(method)</code>","text":"<p>Converts a string to a CrossValMethod enum member.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The string to convert.</p> required <p>Returns:</p> Type Description <code>CrossValMethod</code> <p>The CrossValMethod enum value.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the given string does not match any CrossValMethod.</p> Source code in <code>flexcv/split.py</code> <pre><code>def string_to_crossvalmethod(method: str) -&gt; CrossValMethod:\n    \"\"\"Converts a string to a CrossValMethod enum member.\n\n    Args:\n      method (str): The string to convert.\n\n    Returns:\n      (CrossValMethod): The CrossValMethod enum value.\n\n    Raises:\n      (TypeError): If the given string does not match any CrossValMethod.\n\n    \"\"\"\n    keys = [e.value for e in CrossValMethod]\n    values = [e for e in CrossValMethod]\n    method_dict = dict(zip(keys, values))\n\n    if method in method_dict:\n        return method_dict[method]\n    else:\n        raise TypeError(\"Invalid Cross Validation method given.\")\n</code></pre>"},{"location":"reference/split/#flexcv.stratification","title":"<code>flexcv.stratification</code>","text":"<p>This module implements two stratificsation methods that can be used in contexts of regression of hierarchical (i.e. where the target is continuous and the data is grouped).</p>"},{"location":"reference/split/#flexcv.stratification.ConcatenatedStratifiedKFold","title":"<code>flexcv.stratification.ConcatenatedStratifiedKFold</code>","text":"<p>               Bases: <code>GroupsConsumerMixin</code>, <code>BaseCrossValidator</code></p> <p>Group Concatenated Continuous Stratified k-Folds cross validator. This is a variation of StratifiedKFold that uses a concatenation of target and grouping variable.</p> <pre><code>- The target is discretized.\n- Each discrete target label is casted to type(str) and concatenated with the grouping label\n- Stratification is applied to this new temporal concatenated target\n- This preserves the group's *and* the targets distribution in each fold to be roughly equal to the input distribution\n- The procedure allows overlapping groups which could be interpreted as data leakage in many cases.\n- Population (i.e. the input data set) distribution is leaking into the folds' distribution.\n</code></pre> Source code in <code>flexcv/stratification.py</code> <pre><code>class ConcatenatedStratifiedKFold(GroupsConsumerMixin, BaseCrossValidator):\n    \"\"\"Group Concatenated Continuous Stratified k-Folds cross validator.\n    This is a variation of StratifiedKFold that uses a concatenation of target and grouping variable.\n\n        - The target is discretized.\n        - Each discrete target label is casted to type(str) and concatenated with the grouping label\n        - Stratification is applied to this new temporal concatenated target\n        - This preserves the group's *and* the targets distribution in each fold to be roughly equal to the input distribution\n        - The procedure allows overlapping groups which could be interpreted as data leakage in many cases.\n        - Population (i.e. the input data set) distribution is leaking into the folds' distribution.\n    \"\"\"\n\n    def __init__(self, n_splits, shuffle=True, random_state=42, groups=None):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.groups = groups\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Applies target discretization, row-wise concatenation with the group-label, and stratification on this temporal concatenated column.\n\n        Args:\n          X (array-like): Features\n          y (array-like): target\n          groups (array-like): Grouping variable (Default value = None)\n\n        Returns:\n            (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n        \"\"\"\n        self.skf = StratifiedKFold(\n            n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n        )\n        assert y is not None, \"y cannot be None\"\n        kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n        if isinstance(y, pd.Series):\n            y_cat = (\n                kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n            )\n            y_cat = pd.Series(y_cat, index=y.index)\n        else:\n            y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n        # concatenate y_cat and groups such that the stratification is done on both\n        # elementwise concatenation of three arrays\n        try:\n            y_concat = y_cat.astype(str) + \"_\" + groups.astype(str)\n        except UFuncTypeError:\n            # Why easy when you can do it the hard way?\n            y_concat = np.core.defchararray.add(\n                np.core.defchararray.add(y_cat.astype(str), \"_\"), groups.astype(str)\n            )\n\n        return self.skf.split(X, y_concat)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"\n\n        Args:\n          X (array-like): Features\n          y (array-like): target values. (Default value = None)\n          groups (array-like): grouping values. (Default value = None)\n\n        Returns:\n         (int) : The number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ConcatenatedStratifiedKFold.get_n_splits","title":"<code>flexcv.stratification.ConcatenatedStratifiedKFold.get_n_splits(X=None, y=None, groups=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> <code>None</code> <code>y</code> <code>array - like</code> <p>target values. (Default value = None)</p> <code>None</code> <code>groups</code> <code>array - like</code> <p>grouping values. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <p>(int) : The number of splitting iterations in the cross-validator.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def get_n_splits(self, X=None, y=None, groups=None):\n    \"\"\"\n\n    Args:\n      X (array-like): Features\n      y (array-like): target values. (Default value = None)\n      groups (array-like): grouping values. (Default value = None)\n\n    Returns:\n     (int) : The number of splitting iterations in the cross-validator.\n    \"\"\"\n    return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ConcatenatedStratifiedKFold.split","title":"<code>flexcv.stratification.ConcatenatedStratifiedKFold.split(X, y, groups=None)</code>","text":"<p>Generate indices to split data into training and test set. Applies target discretization, row-wise concatenation with the group-label, and stratification on this temporal concatenated column.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> required <code>y</code> <code>array - like</code> <p>target</p> required <code>groups</code> <code>array - like</code> <p>Grouping variable (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[tuple[ndarray, ndarray]]</code> <p>Iterator over the indices of the training and test set.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def split(self, X, y, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n    Applies target discretization, row-wise concatenation with the group-label, and stratification on this temporal concatenated column.\n\n    Args:\n      X (array-like): Features\n      y (array-like): target\n      groups (array-like): Grouping variable (Default value = None)\n\n    Returns:\n        (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n    \"\"\"\n    self.skf = StratifiedKFold(\n        n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n    )\n    assert y is not None, \"y cannot be None\"\n    kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n    if isinstance(y, pd.Series):\n        y_cat = (\n            kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n        )\n        y_cat = pd.Series(y_cat, index=y.index)\n    else:\n        y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n    # concatenate y_cat and groups such that the stratification is done on both\n    # elementwise concatenation of three arrays\n    try:\n        y_concat = y_cat.astype(str) + \"_\" + groups.astype(str)\n    except UFuncTypeError:\n        # Why easy when you can do it the hard way?\n        y_concat = np.core.defchararray.add(\n            np.core.defchararray.add(y_cat.astype(str), \"_\"), groups.astype(str)\n        )\n\n    return self.skf.split(X, y_concat)\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedGroupKFold","title":"<code>flexcv.stratification.ContinuousStratifiedGroupKFold</code>","text":"<p>               Bases: <code>GroupsConsumerMixin</code>, <code>BaseCrossValidator</code></p> <p>Continuous Stratified Group k-Folds cross validator. This is a variation of StratifiedKFold that     - makes a temporal discretization of the target variable.     - apply stratified group k-fold based on the passed groups and the discretized target.     - does not further use this discretized target     - tries to preserve the percentage of samples in each percentile per group given the constraint of non-overlapping groups</p> Source code in <code>flexcv/stratification.py</code> <pre><code>class ContinuousStratifiedGroupKFold(GroupsConsumerMixin, BaseCrossValidator):\n    \"\"\"Continuous Stratified Group k-Folds cross validator.\n    This is a variation of StratifiedKFold that\n        - makes a temporal discretization of the target variable.\n        - apply stratified group k-fold based on the passed groups and the discretized target.\n        - does not further use this discretized target\n        - tries to preserve the percentage of samples in each percentile per group given the constraint of non-overlapping groups\n    \"\"\"\n\n    def __init__(self, n_splits, shuffle=True, random_state=42, groups=None):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.groups = groups\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        The data is first grouped by groups and then split into n_splits folds. The folds are made by preserving the percentage of samples for each class.\n        This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.\n\n        Args:\n          X (array-like): Features\n          y (array-like): target\n          groups (array-like): Grouping/clustering variable (Default value = None)\n\n        Returns:\n            (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n        \"\"\"\n        self.sgkf = StratifiedGroupKFold(\n            n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n        )\n        assert y is not None, \"y cannot be None\"\n        kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n        if isinstance(y, pd.Series):\n            y_cat = (\n                kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n            )\n            y_cat = pd.Series(y_cat, index=y.index)\n        else:\n            y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n        return self.sgkf.split(X, y_cat, groups)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"\n        Returns the number of splitting iterations in the cross-validator.\n\n        Returns:\n          (int): The number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedGroupKFold.get_n_splits","title":"<code>flexcv.stratification.ContinuousStratifiedGroupKFold.get_n_splits(X=None, y=None, groups=None)</code>","text":"<p>Returns the number of splitting iterations in the cross-validator.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of splitting iterations in the cross-validator.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def get_n_splits(self, X=None, y=None, groups=None):\n    \"\"\"\n    Returns the number of splitting iterations in the cross-validator.\n\n    Returns:\n      (int): The number of splitting iterations in the cross-validator.\n    \"\"\"\n    return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedGroupKFold.split","title":"<code>flexcv.stratification.ContinuousStratifiedGroupKFold.split(X, y, groups=None)</code>","text":"<p>Generate indices to split data into training and test set. The data is first grouped by groups and then split into n_splits folds. The folds are made by preserving the percentage of samples for each class. This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> required <code>y</code> <code>array - like</code> <p>target</p> required <code>groups</code> <code>array - like</code> <p>Grouping/clustering variable (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[tuple[ndarray, ndarray]]</code> <p>Iterator over the indices of the training and test set.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def split(self, X, y, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n    The data is first grouped by groups and then split into n_splits folds. The folds are made by preserving the percentage of samples for each class.\n    This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.\n\n    Args:\n      X (array-like): Features\n      y (array-like): target\n      groups (array-like): Grouping/clustering variable (Default value = None)\n\n    Returns:\n        (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n    \"\"\"\n    self.sgkf = StratifiedGroupKFold(\n        n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n    )\n    assert y is not None, \"y cannot be None\"\n    kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n    if isinstance(y, pd.Series):\n        y_cat = (\n            kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n        )\n        y_cat = pd.Series(y_cat, index=y.index)\n    else:\n        y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n    return self.sgkf.split(X, y_cat, groups)\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedKFold","title":"<code>flexcv.stratification.ContinuousStratifiedKFold</code>","text":"<p>               Bases: <code>BaseCrossValidator</code></p> <p>Continuous Stratified k-Folds cross validator, i.e. it works with continuous target variables instead of multiclass targets.</p> <p>This is a variation of StratifiedKFold that</p> <pre><code>- makes a copy of the target variable and discretizes it.\n- applies stratified k-folds based on this discrete target to ensure equal percentile distribution across folds\n- does not further use or pass this discrete target.\n- does not apply grouping rules.\n</code></pre> Source code in <code>flexcv/stratification.py</code> <pre><code>class ContinuousStratifiedKFold(BaseCrossValidator):\n    \"\"\"Continuous Stratified k-Folds cross validator, i.e. it works with *continuous* target variables instead of multiclass targets.\n\n    This is a variation of StratifiedKFold that\n\n        - makes a copy of the target variable and discretizes it.\n        - applies stratified k-folds based on this discrete target to ensure equal percentile distribution across folds\n        - does not further use or pass this discrete target.\n        - does not apply grouping rules.\n    \"\"\"\n\n    def __init__(self, n_splits, shuffle=True, random_state=42, groups=None):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.groups = groups\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        The folds are made by preserving the percentage of samples for each class.\n        This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.\n\n        Args:\n          X (array-like): Features\n          y (array-like): target\n          groups (array-like): Grouping variable (Default value = None)\n\n        Returns:\n            (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n        \"\"\"\n        self.skf = StratifiedKFold(\n            n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n        )\n        assert y is not None, \"y cannot be None\"\n        kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n        if isinstance(y, pd.Series):\n            y_cat = (\n                kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n            )\n            y_cat = pd.Series(y_cat, index=y.index)\n        else:\n            y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n\n        return self.skf.split(X, y_cat)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"\n\n        Args:\n          X (array-like): Features\n          y (array-like): target values. (Default value = None)\n          groups (array-like): grouping values. (Default value = None)\n\n        Returns:\n         (int) : The number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedKFold.get_n_splits","title":"<code>flexcv.stratification.ContinuousStratifiedKFold.get_n_splits(X=None, y=None, groups=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> <code>None</code> <code>y</code> <code>array - like</code> <p>target values. (Default value = None)</p> <code>None</code> <code>groups</code> <code>array - like</code> <p>grouping values. (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <p>(int) : The number of splitting iterations in the cross-validator.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def get_n_splits(self, X=None, y=None, groups=None):\n    \"\"\"\n\n    Args:\n      X (array-like): Features\n      y (array-like): target values. (Default value = None)\n      groups (array-like): grouping values. (Default value = None)\n\n    Returns:\n     (int) : The number of splitting iterations in the cross-validator.\n    \"\"\"\n    return self.n_splits\n</code></pre>"},{"location":"reference/split/#flexcv.stratification.ContinuousStratifiedKFold.split","title":"<code>flexcv.stratification.ContinuousStratifiedKFold.split(X, y, groups=None)</code>","text":"<p>Generate indices to split data into training and test set. The folds are made by preserving the percentage of samples for each class. This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Features</p> required <code>y</code> <code>array - like</code> <p>target</p> required <code>groups</code> <code>array - like</code> <p>Grouping variable (Default value = None)</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[tuple[ndarray, ndarray]]</code> <p>Iterator over the indices of the training and test set.</p> Source code in <code>flexcv/stratification.py</code> <pre><code>def split(self, X, y, groups=None):\n    \"\"\"Generate indices to split data into training and test set.\n    The folds are made by preserving the percentage of samples for each class.\n    This is a variation of StratifiedGroupKFold that uses a custom discretization of the target variable.\n\n    Args:\n      X (array-like): Features\n      y (array-like): target\n      groups (array-like): Grouping variable (Default value = None)\n\n    Returns:\n        (Iterator[tuple[ndarray, ndarray]]): Iterator over the indices of the training and test set.\n    \"\"\"\n    self.skf = StratifiedKFold(\n        n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state\n    )\n    assert y is not None, \"y cannot be None\"\n    kbins = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n    if isinstance(y, pd.Series):\n        y_cat = (\n            kbins.fit_transform(y.to_numpy().reshape(-1, 1)).flatten().astype(int)\n        )\n        y_cat = pd.Series(y_cat, index=y.index)\n    else:\n        y_cat = kbins.fit_transform(y.reshape(-1, 1)).flatten().astype(int)  # type: ignore\n\n    return self.skf.split(X, y_cat)\n</code></pre>"},{"location":"reference/synthesis/","title":"Data Synthesis","text":""},{"location":"reference/synthesis/#flexcv.synthesizer","title":"<code>flexcv.synthesizer</code>","text":""},{"location":"reference/synthesis/#flexcv.synthesizer.generate_regression","title":"<code>flexcv.synthesizer.generate_regression(m_features, n_samples, n_groups=5, n_slopes=1, random_seed=42, noise_level=0.1, fixed_random_ratio=0.01)</code>","text":"<p>Generate a dataset for linear regression using the numpy default rng.</p> <p>Parameters:</p> Name Type Description Default <code>m_features</code> <code>int</code> <p>Number of features, i.e. columns, to be generated.</p> required <code>n_samples</code> <code>int</code> <p>Number of rows to be generated.</p> required <code>n_groups</code> <code>int</code> <p>Number of groups/clusters. (Default value = 5)</p> <code>5</code> <code>n_slopes</code> <code>int</code> <p>Number of columns in the feature matrix to be used as random slopes as well. (Default value = 1)</p> <code>1</code> <code>noise_level</code> <code>float</code> <p>The data will be generated with added standard normal noise which is multiplied with noise_level. (Default value = 0.1)</p> <code>0.1</code> <code>fixed_random_ratio</code> <code>float</code> <p>The ratio of the fixed effects to the random effects. (Default value = 0.01)</p> <code>0.01</code> <code>random_seed</code> <code>int</code> <p>The random seed to be used for reproducibility. (Default value = 42)</p> <code>42</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the following elements:         (The feature matrix DataFrame, the target vector Series, the group labels Series, the random slopes DataFrame)</p> Source code in <code>flexcv/synthesizer.py</code> <pre><code>def generate_regression(\n    m_features: int,\n    n_samples: int,\n    n_groups: int = 5,\n    n_slopes: int = 1,\n    random_seed: int = 42,\n    noise_level: float = 0.1,\n    fixed_random_ratio: float = 0.01,\n) -&gt; tuple[pd.DataFrame, pd.Series, pd.Series, pd.DataFrame]:\n    \"\"\"Generate a dataset for linear regression using the numpy default rng.\n\n    Args:\n      m_features (int): Number of features, i.e. columns, to be generated.\n      n_samples (int): Number of rows to be generated.\n      n_groups (int): Number of groups/clusters. (Default value = 5)\n      n_slopes (int): Number of columns in the feature matrix to be used as random slopes as well. (Default value = 1)\n      noise_level (float): The data will be generated with added standard normal noise which is multiplied with noise_level. (Default value = 0.1)\n      fixed_random_ratio (float): The ratio of the fixed effects to the random effects. (Default value = 0.01)\n      random_seed (int): The random seed to be used for reproducibility. (Default value = 42)\n\n    Returns:\n      (tuple): A tuple containing the following elements:\n                (The feature matrix DataFrame, the target vector Series, the group labels Series, the random slopes DataFrame)\n\n    \"\"\"\n\n    # initialize random number generator and generate predictors randomly\n    rng = np.random.default_rng(random_seed)\n    X = rng.integers(-10, 10, (n_samples, m_features))\n\n    # generate random coefficients with a linear relationship and add noise\n    beta_fixed = rng.random(m_features) * fixed_random_ratio\n    epsilon = rng.standard_normal(n_samples) * noise_level\n\n    # generate random group labels\n    group = rng.integers(0, n_groups, n_samples)\n\n    # generate random effects -&gt; linear relationships for each group\n    beta_random = rng.uniform(0.9, 1.0, size=n_groups)\n    # reorder the random effects betas according to the group labels\n    random_effects = beta_random[group]\n\n    # generate the response variable\n    y = 1 + X @ beta_fixed + random_effects + epsilon\n\n    # convert to pandas dataframe with random column names\n    X = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n    y = pd.Series(y, name=\"y\")\n    group = pd.Series(group, name=\"group\")\n\n    # select a random column to be the random slope\n    column_names = X.columns.tolist()\n    choosen_columns = rng.choice(column_names, size=n_slopes, replace=False)\n    random_slopes = X[choosen_columns]\n\n    return X, y, group, random_slopes\n</code></pre>"},{"location":"reference/utilities/","title":"Utilities","text":""},{"location":"reference/utilities/#flexcv.utilities","title":"<code>flexcv.utilities</code>","text":""},{"location":"reference/utilities/#flexcv.utilities.add_model_to_keys","title":"<code>flexcv.utilities.add_model_to_keys(param_grid)</code>","text":"<p>This function adds the string \"model__\" to avery key of the param_grid dict.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>dict</code> <p>A dictionary of parameters for a model.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of parameters for a model with the string \"model__\" added to each key.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def add_model_to_keys(param_grid):\n    \"\"\"This function adds the string \"model__\" to avery key of the param_grid dict.\n\n    Args:\n      param_grid (dict): A dictionary of parameters for a model.\n\n    Returns:\n      (dict): A dictionary of parameters for a model with the string \"model__\" added to each key.\n    \"\"\"\n    return {f\"model__{key}\": value for key, value in param_grid.items()}\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.add_module_handlers","title":"<code>flexcv.utilities.add_module_handlers(logger)</code>","text":"<p>Adds handlers to the logger for the module.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>logging.Logger: The logger for the module.</p> required <p>Returns:</p> Type Description <code>None</code> <p>(None)</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def add_module_handlers(logger: logging.Logger) -&gt; None:\n    \"\"\"Adds handlers to the logger for the module.\n\n    Args:\n      logger: logging.Logger: The logger for the module.\n\n    Returns:\n      (None)\n    \"\"\"\n    logger = logging.getLogger()  # Get the root logger\n    logger.setLevel(logging.INFO)\n\n    c_handler = logging.StreamHandler()\n    c_format = logging.Formatter(\"%(module)s - %(levelname)s - %(message)s\")\n    c_handler.setFormatter(c_format)\n    c_handler.setLevel(logging.INFO)\n    logger.addHandler(c_handler)\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.get_fixed_effects_formula","title":"<code>flexcv.utilities.get_fixed_effects_formula(target_name, X_data)</code>","text":"<p>Returns the fixed effects formula for the dataset.</p> <p>Scheme: \"target ~ column1 + column2 + ...</p> <p>Parameters:</p> Name Type Description Default <code>target_name</code> <p>str: The name of the target variable in the dataset.</p> required <code>X_data</code> <p>pd.DataFrame: The feature matrix.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The fixed effects formula.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def get_fixed_effects_formula(target_name, X_data) -&gt; str:\n    \"\"\"Returns the fixed effects formula for the dataset.\n\n    Scheme: \"target ~ column1 + column2 + ...\n\n    Args:\n      target_name: str: The name of the target variable in the dataset.\n      X_data: pd.DataFrame: The feature matrix.\n\n    Returns:\n      (str): The fixed effects formula.\n    \"\"\"\n    if X_data.shape[1] == 1:\n        return f\"{target_name} ~ {X_data.columns[0]}\"\n    start = f\"{target_name} ~ {X_data.columns[0]} + \"\n    end = \" + \".join(X_data.columns[1:])\n    return start + end\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.get_re_formula","title":"<code>flexcv.utilities.get_re_formula(random_slopes_data)</code>","text":"<p>Returns a random effects formula for use in statsmodels. Scheme: ~ random_slope1 + random_slope2 + ... Returns an empty string if no random slopes are provided.</p> <p>Parameters:</p> Name Type Description Default <code>random_slopes_data</code> <p>pd.Series | pd.DataFrame: The random slopes data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The random effects formula.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def get_re_formula(random_slopes_data):\n    \"\"\"Returns a random effects formula for use in statsmodels. Scheme: ~ random_slope1 + random_slope2 + ...\n    Returns an empty string if no random slopes are provided.\n\n    Args:\n      random_slopes_data: pd.Series | pd.DataFrame: The random slopes data.\n\n    Returns:\n      (str): The random effects formula.\n    \"\"\"\n    if random_slopes_data is None:\n        return \"\"\n    elif isinstance(random_slopes_data, pd.DataFrame):\n        return \"~ \" + \" + \".join(random_slopes_data.columns)\n    elif isinstance(random_slopes_data, pd.Series):\n        return \"~ \" + random_slopes_data.name\n    else:\n        raise TypeError(\"Random slopes data type not recognized\")\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.get_repeated_cv_metadata","title":"<code>flexcv.utilities.get_repeated_cv_metadata(str_children='Instance of repeated run ', api_dict=None)</code>","text":"<p>This function can be used to fetch metadata from repeated cross-validation runs. We use it to get the ids of the children runs and their descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>str_children</code> <code>str</code> <p>The string that is prepended to the description of each child run.</p> <code>'Instance of repeated run '</code> <code>api_dict</code> <code>dict</code> <p>A dictionary containing the Neptune.ai project name and the api token.</p> <code>None</code> Source code in <code>flexcv/utilities.py</code> <pre><code>def get_repeated_cv_metadata(str_children=\"Instance of repeated run \", api_dict=None):\n    \"\"\"This function can be used to fetch metadata from repeated cross-validation runs.\n    We use it to get the ids of the children runs and their descriptions.\n\n    Args:\n        str_children (str): The string that is prepended to the description of each child run.\n        api_dict (dict): A dictionary containing the Neptune.ai project name and the api token.\n    \"\"\"\n    if api_dict is None:\n        raise ValueError(\"api_dict must be provided\")\n\n    # get a list of all runs in the project\n    project = neptune.init_project(\n        project=api_dict[\"project\"],\n        api_token=api_dict[\"api_token\"],\n        mode=\"read-only\",\n    )\n    runs_table_df = project.fetch_runs_table().to_pandas()\n    # use only rows where \"sys/description\" begins with \"Instance\"\n    # group by run sys/description\n    grouped = runs_table_df[\n        runs_table_df[\"sys/description\"].str.startswith(str_children)\n    ].groupby(\"sys/description\")\n    # get sys/id for each group\n    grouped_ids = grouped[\"sys/id\"].apply(list)\n    # remove \"Instance of repeated run \" and trailing dot from the description\n    grouped_ids.index = grouped_ids.index.str.replace(str_children, \"\")\n    grouped_ids.index = grouped_ids.index.str.replace(\".\", \"\")\n    # rename the index to \"host id\"\n    grouped_ids.index.name = \"host id\"\n    # rename the column to \"children ids\"\n    grouped_ids.name = \"children ids\"\n    metadata = pd.DataFrame(grouped_ids)\n    # use the host ids to get their sys/description and make them a new column in the DataFrame\n    host_ids = grouped_ids.index\n    descriptions = runs_table_df[runs_table_df[\"sys/id\"].isin(host_ids)][\n        \"sys/description\"\n    ]\n    descriptions.index = host_ids\n    descriptions.index.name = \"host id\"\n    descriptions.name = \"description\"\n    # join the two DataFrames\n    metadata = metadata.join(pd.DataFrame(descriptions))\n    # save to excel\n    metadata.to_excel(\"repeated_cv_metadata.xlsx\")\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.handle_duplicate_kwargs","title":"<code>flexcv.utilities.handle_duplicate_kwargs(*args)</code>","text":"<p>This function removes duplicate kwargs from mutiple dicts. If a key is present in multiple dicts, we check if the values are the same. If they are, we keep the key-value pair. If they are not, we raise a ValueError.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>A dict of kwargs.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The dict without duplicate kwargs.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def handle_duplicate_kwargs(*args) -&gt; dict:\n    \"\"\"This function removes duplicate kwargs from mutiple dicts.\n    If a key is present in multiple dicts, we check if the values are the same.\n    If they are, we keep the key-value pair. If they are not, we raise a ValueError.\n\n    Args:\n        kwargs (dict): A dict of kwargs.\n\n    Returns:\n        (dict): The dict without duplicate kwargs.\n    \"\"\"\n    return_kwargs = {}\n    for arg in args:\n        if not isinstance(arg, dict):\n            raise TypeError(\"All arguments must be of type dict\")\n        for key, value in arg.items():\n            if not key in return_kwargs.keys():\n                return_kwargs[key] = value\n                # compare values\n            else:\n                if arg[key] != return_kwargs[key]:\n                    raise ValueError(\n                        f\"Duplicate key {key} found with different values. Overwriting.\"\n                    )\n                else:\n                    logger.info(f\"Duplicate key {key} found with same value. Keeping.\")\n\n    return return_kwargs\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.pformat_dict","title":"<code>flexcv.utilities.pformat_dict(d, indent='')</code>","text":"<p>Pretty-format a dictionary, only printing values that are themselves dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>dictionary to print</p> required <code>indent</code> <code>str</code> <p>Level of indentation for use with recursion (Default value = \"\")</p> <code>''</code> <p>Returns:</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def pformat_dict(d, indent=\"\"):\n    \"\"\"Pretty-format a dictionary, only printing values that are themselves dictionaries.\n\n    Args:\n      d (dict): dictionary to print\n      indent (str): Level of indentation for use with recursion (Default value = \"\")\n\n    Returns:\n\n    \"\"\"\n    formatted = \"\"\n    for key, value in d.items():\n        formatted.join(f\"{indent}{key}\")\n        if isinstance(value, dict):\n            next_layer = pformat_dict(value, indent + \"  \")\n            formatted.join(next_layer)\n    return formatted\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.rm_model_from_keys","title":"<code>flexcv.utilities.rm_model_from_keys(param_grid)</code>","text":"<p>This function removes the string \"model__\" from avery key of the param_grid dict.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>dict</code> <p>A dictionary of parameters for a model.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of parameters for a model with the string \"model__\" removed from each key.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def rm_model_from_keys(param_grid):\n    \"\"\"This function removes the string \"model__\" from avery key of the param_grid dict.\n\n    Args:\n      param_grid (dict): A dictionary of parameters for a model.\n\n    Returns:\n      (dict): A dictionary of parameters for a model with the string \"model__\" removed from each key.\n    \"\"\"\n    return {key.replace(\"model__\", \"\"): value for key, value in param_grid.items()}\n</code></pre>"},{"location":"reference/utilities/#flexcv.utilities.run_padding","title":"<code>flexcv.utilities.run_padding(func)</code>","text":"<p>Decorator to add padding to the output of a function. Helps to visually separate the output of different functions.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Any callable.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Return value of the passed callable.</p> Source code in <code>flexcv/utilities.py</code> <pre><code>def run_padding(func):\n    \"\"\"Decorator to add padding to the output of a function.\n    Helps to visually separate the output of different functions.\n\n    Args:\n      func: Any callable.\n\n    Returns:\n      (Any): Return value of the passed callable.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper_function(*args, **kwargs):\n        print()\n        print(\"~\" * 10, \"STARTING RUN\", \"~\" * 10)\n        print()\n        results = func(*args, **kwargs)\n        print()\n        print(\"~\" * 10, \" END OF RUN\", \"~\" * 10)\n        print()\n        return results\n\n    return wrapper_function\n</code></pre>"},{"location":"reference/yaml/","title":"YAML Parser","text":""},{"location":"reference/yaml/#flexcv.yaml_parser","title":"<code>flexcv.yaml_parser</code>","text":""},{"location":"reference/yaml/#flexcv.yaml_parser.cat_constructor","title":"<code>flexcv.yaml_parser.cat_constructor(loader, node)</code>","text":"<p>Construct an optuna CategoricalDistribution from a yaml node.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>SafeLoader</code> <p>The yaml loader.</p> required <code>node</code> <code>ScalarNode</code> <p>The yaml node.</p> required <p>Returns:</p> Type Description <code>CategoricalDistribution</code> <p>The constructed CategoricalDistribution.</p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def cat_constructor(\n    loader: yaml.SafeLoader, node: yaml.nodes.ScalarNode\n) -&gt; CategoricalDistribution:\n    \"\"\"Construct an optuna CategoricalDistribution from a yaml node.\n\n    Args:\n        loader (yaml.SafeLoader): The yaml loader.\n        node (yaml.nodes.ScalarNode): The yaml node.\n\n    Returns:\n        (CategoricalDistribution): The constructed CategoricalDistribution.\n    \"\"\"\n    value = loader.construct_mapping(node, deep=True)\n    return CategoricalDistribution(**value)\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.float_constructor","title":"<code>flexcv.yaml_parser.float_constructor(loader, node)</code>","text":"<p>Construct an optuna FloatDistribution from a yaml node.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>SafeLoader</code> <p>The yaml loader.</p> required <code>node</code> <code>ScalarNode</code> <p>The yaml node.</p> required <p>Returns:</p> Type Description <code>FloatDistribution</code> <p>The constructed FloatDistribution.</p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def float_constructor(\n    loader: yaml.SafeLoader, node: yaml.nodes.ScalarNode\n) -&gt; FloatDistribution:\n    \"\"\"Construct an optuna FloatDistribution from a yaml node.\n\n    Args:\n        loader (yaml.SafeLoader): The yaml loader.\n        node (yaml.nodes.ScalarNode): The yaml node.\n\n    Returns:\n        (FloatDistribution): The constructed FloatDistribution.\n    \"\"\"\n    return FloatDistribution(**loader.construct_mapping(node))\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.get_loader","title":"<code>flexcv.yaml_parser.get_loader()</code>","text":"<p>This function returns a yaml loader with the custom constructors for the optuna distributions. Custom and safe constructors are added to the following tags:</p> <ul> <li>!Int</li> <li>!Float</li> <li>!Cat</li> </ul> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def get_loader():\n    \"\"\"This function returns a yaml loader with the custom constructors for the optuna distributions.\n    Custom and safe constructors are added to the following tags:\n\n    - !Int\n    - !Float\n    - !Cat\n    \"\"\"\n    loader = yaml.SafeLoader\n    loader.add_constructor(\"!Int\", int_constructor)\n    loader.add_constructor(\"!Float\", float_constructor)\n    loader.add_constructor(\"!Cat\", cat_constructor)\n    return loader\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.int_constructor","title":"<code>flexcv.yaml_parser.int_constructor(loader, node)</code>","text":"<p>Construct an optuna IntDistribution from a yaml node.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>SafeLoader</code> <p>The yaml loader.</p> required <code>node</code> <code>ScalarNode</code> <p>The yaml node.</p> required <p>Returns:</p> Type Description <code>IntDistribution</code> <p>The constructed IntDistribution.</p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def int_constructor(\n    loader: yaml.SafeLoader, node: yaml.nodes.ScalarNode\n) -&gt; IntDistribution:\n    \"\"\"Construct an optuna IntDistribution from a yaml node.\n\n    Args:\n        loader (yaml.SafeLoader): The yaml loader.\n        node (yaml.nodes.ScalarNode): The yaml node.\n\n    Returns:\n        (IntDistribution): The constructed IntDistribution.\n    \"\"\"\n    return IntDistribution(**loader.construct_mapping(node))\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.parse_yaml_output_to_mapping_dict","title":"<code>flexcv.yaml_parser.parse_yaml_output_to_mapping_dict(yaml_dict)</code>","text":"<p>This function parses the output of the yaml parser to a ModelMappingDict object. Models and post processors are imported automatically.</p> Note <p>Despite of automatically importing the classes, no arbitrary code is executed. The yaml parser uses the safe loader and a custom constructors for the optuna distributions. The imports are done by the importlib module.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_dict</code> <code>dict</code> <p>The output of the yaml parser.</p> required <p>Returns:</p> Type Description <code>ModelMappingDict</code> <p>A dictionary of ModelConfigDict objects.</p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def parse_yaml_output_to_mapping_dict(yaml_dict) -&gt; ModelMappingDict:\n    \"\"\"This function parses the output of the yaml parser to a ModelMappingDict object.\n    Models and post processors are imported automatically.\n\n    Note:\n        Despite of automatically importing the classes, no arbitrary code is executed.\n        The yaml parser uses the safe loader and a custom constructors for the optuna distributions.\n        The imports are done by the importlib module.\n\n    Args:\n        yaml_dict (dict): The output of the yaml parser.\n\n    Returns:\n        (ModelMappingDict): A dictionary of ModelConfigDict objects.\n    \"\"\"\n    model_mapping_dict = ModelMappingDict()\n    for model_name, raw_dict in yaml_dict.items():\n        if \"model\" not in raw_dict:\n            raise ValueError(f\"model name is missing for model {model_name}\")\n\n        imports_dict = {}\n        import_list = [\"model\"]\n        if \"post_processor\" in raw_dict:\n            import_list.append(\"post_processor\")\n\n        for key in import_list:\n            if isinstance(raw_dict[key], str):\n                class_name, path = split_import(raw_dict[key])\n                foo = importlib.import_module(path)\n                imports_dict[key] = getattr(foo, class_name)\n            else:\n                imports_dict[key] = raw_dict[key]\n\n        # replace the strings in raw_dict with the imported classes\n        raw_dict.update(imports_dict)\n        model_config_dict = ModelConfigDict(raw_dict)\n        model_mapping_dict[model_name] = model_config_dict\n    return model_mapping_dict\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.read_mapping_from_yaml_file","title":"<code>flexcv.yaml_parser.read_mapping_from_yaml_file(yaml_file_path)</code>","text":"<p>This function reads in a yaml file and returns a ModelMappingDict object. Use the yaml tags !Int, !Float, !Cat to specify the type of the hyperparameter distributions. The parser takes care of importing the classes specified in the yaml file in the fields model and post_processor.</p> Note <p>Despite of automatically importing the classes, no arbitrary code is executed. The yaml parser uses the safe loader and a custom constructors for the optuna distributions. The imports are done by the importlib module.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file_path</code> <code>str</code> <p>The yaml file path.</p> required <p>Returns:</p> Type Description <code>ModelMappingDict</code> <p>A dictionary of ModelConfigDict objects.</p> Example <p>Your file.yaml file could look like this: <pre><code>RandomForest:\n    model: sklearn.ensemble.RandomForestRegressor\n    post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n    requires_inner_cv: True\n    params:\n        max_depth: !Int\n            low: 1\n            high: 10\n</code></pre> And you would read it in like this: <pre><code>model_mapping = read_mapping_from_yaml_file(\"file.yaml\")\n</code></pre></p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def read_mapping_from_yaml_file(yaml_file_path: str) -&gt; ModelMappingDict:\n    \"\"\"This function reads in a yaml file and returns a ModelMappingDict object.\n    Use the yaml tags !Int, !Float, !Cat to specify the type of the hyperparameter distributions.\n    The parser takes care of importing the classes specified in the yaml file in the fields model and post_processor.\n\n    Note:\n        Despite of automatically importing the classes, no arbitrary code is executed.\n        The yaml parser uses the safe loader and a custom constructors for the optuna distributions.\n        The imports are done by the importlib module.\n\n    Args:\n        yaml_file_path (str): The yaml file path.\n\n    Returns:\n        (ModelMappingDict): A dictionary of ModelConfigDict objects.\n\n    Example:\n        Your file.yaml file could look like this:\n        ```yaml\n        RandomForest:\n            model: sklearn.ensemble.RandomForestRegressor\n            post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n            requires_inner_cv: True\n            params:\n                max_depth: !Int\n                    low: 1\n                    high: 10\n        ```\n        And you would read it in like this:\n        ```python\n        model_mapping = read_mapping_from_yaml_file(\"file.yaml\")\n        ```\n    \"\"\"\n    with open(yaml_file_path, \"r\") as f:\n        yaml_output = yaml.load(f, Loader=get_loader())\n    model_mapping = parse_yaml_output_to_mapping_dict(yaml_output)\n    return model_mapping\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.read_mapping_from_yaml_string","title":"<code>flexcv.yaml_parser.read_mapping_from_yaml_string(yaml_code)</code>","text":"<p>This function reads a yaml string and returns a ModelMappingDict object. Use the yaml tags !Int, !Float, !Cat to specify the type of the hyperparameter distributions. The parser takes care of importing the classes specified in the yaml file in the fields model and post_processor.</p> Note <p>Despite of automatically importing the classes, no arbitrary code is executed. The yaml parser uses the safe loader and a custom constructors for the optuna distributions. The imports are done by the importlib module.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_code</code> <code>str</code> <p>The yaml code.</p> required <p>Returns:</p> Type Description <code>ModelMappingDict</code> <p>A dictionary of ModelConfigDict objects.</p> <p>Example:</p> <pre><code>```python\nyaml_code = '''\n            RandomForest:\n                model: sklearn.ensemble.RandomForestRegressor\n                post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n                requires_inner_cv: True\n                params:\n                    max_depth: !Int\n                        low: 1\n                        high: 10\n            '''\nmodel_mapping = read_mapping_from_yaml_string(yaml_code)\n```\n</code></pre> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def read_mapping_from_yaml_string(yaml_code: str) -&gt; ModelMappingDict:\n    \"\"\"This function reads a yaml string and returns a ModelMappingDict object.\n    Use the yaml tags !Int, !Float, !Cat to specify the type of the hyperparameter distributions.\n    The parser takes care of importing the classes specified in the yaml file in the fields model and post_processor.\n\n    Note:\n        Despite of automatically importing the classes, no arbitrary code is executed.\n        The yaml parser uses the safe loader and a custom constructors for the optuna distributions.\n        The imports are done by the importlib module.\n\n    Args:\n        yaml_code (str): The yaml code.\n\n    Returns:\n        (ModelMappingDict): A dictionary of ModelConfigDict objects.\n\n    Example:\n\n        ```python\n        yaml_code = '''\n                    RandomForest:\n                        model: sklearn.ensemble.RandomForestRegressor\n                        post_processor: flexcv.model_postprocessing.MixedEffectsPostProcessor\n                        requires_inner_cv: True\n                        params:\n                            max_depth: !Int\n                                low: 1\n                                high: 10\n                    '''\n        model_mapping = read_mapping_from_yaml_string(yaml_code)\n        ```\n\n    \"\"\"\n    yaml_output = yaml.load(yaml_code, Loader=get_loader())\n    model_mapping = parse_yaml_output_to_mapping_dict(yaml_output)\n    return model_mapping\n</code></pre>"},{"location":"reference/yaml/#flexcv.yaml_parser.split_import","title":"<code>flexcv.yaml_parser.split_import(import_string)</code>","text":"<p>This function splits an import string into the class name and the module path.</p> <p>Parameters:</p> Name Type Description Default <code>import_string</code> <code>str</code> <p>The import string.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The class name and the module path.</p> Source code in <code>flexcv/yaml_parser.py</code> <pre><code>def split_import(import_string) -&gt; tuple[str, str]:\n    \"\"\"This function splits an import string into the class name and the module path.\n\n    Args:\n        import_string (str): The import string.\n\n    Returns:\n        (tuple[str, str]): The class name and the module path.\n    \"\"\"\n    import_list = import_string.split(\".\")\n    class_name = import_list[-1]\n    module_path = \".\".join(import_list[:-1])\n    return class_name, module_path\n</code></pre>"},{"location":"start/getting-started/","title":"Installation","text":"<p>This guide will help you get started with the <code>flexcv</code> package quickly. It will show you how to install the package and how to use it to compare machine learning models on your data. At the end of this section you will have a working environment to start using <code>flexcv</code> for your own projects.</p>"},{"location":"start/getting-started/#installation","title":"Installation","text":"<p>You can just install the package from PyPI using <code>pip</code>:</p> <pre><code>pip install flexcv\n</code></pre> <p>We support Python 3.10 and 3.11 at the moment. We will support 3.12 as soon as some dependencies are updated as well.</p>"},{"location":"start/getting-started/#using-venv","title":"Using venv","text":"<p>To separate Python environments on your system, we recommend to use some kind of virtual environment. One way is to use the <code>venv</code> package from the standard library. Create a directory for your environment, create the environment and activate it. Then install the package and all dependencies.</p> <pre><code>mkdir my_env_name\npython -m venv my_env_name\nmy_env_name/Scripts/activate\npip install flexcv\n</code></pre> <p>Now you have installed everything you need to perform flexible cross validation and machine learning on your tabular data.</p>"},{"location":"start/tutorial/","title":"Tutorial","text":"<p>To get started with <code>flexcv</code>, we take you through a couple of quick and basic code examples. You will learn how to set up <code>flexcv</code> with a linear model and how the interaction with the interface class works in practical use. At the end of this section you will be familiar with the basic concepts of <code>flexcv</code> and will be able to use it for your own projects.</p>"},{"location":"start/tutorial/#linear-model","title":"Linear Model","text":"<p>First we will use a LinearModel on a randomly generated regression dataset. Because Linear Models do not have any hyperparameters, we naturally don't need an inner cross validation loop.</p> <pre><code># import the class interface, data generator and model\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\nfrom flexcv.models import LinearModel\n\n# make sample data\nX, y, group, random_slopes = generate_regression(10, 100, n_slopes=1, noise_level=9.1e-2, random_seed=42)\n\n# instantiate our cross validation class\ncv = CrossValidation()\n\n# now we can use method chaining to set up our configuration perform the cross validation\nresults = (\n    cv\n    .set_data(X, y, group, dataset_name=\"ExampleData\")\n    .set_splits(method_outer_split=\"GroupKFold\", method_inner_split=\"KFold\")\n    .add_model(LinearModel)\n    .set_splits(break_cross_val=True)\n    .perform()\n    .get_results()\n)\n\n# results has a summary property which returns a dataframe\n# we can simply call the pandas method \"to_excel\"\nresults.summary.to_excel(\"my_cv_results.xlsx\")\n</code></pre>"}]}