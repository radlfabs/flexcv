# User Guide

This user guide will give you a detailled guide on how to use `flexcv` functions and objects.

## Fit a Random Forest Regressor

In this section, we will use a Random Forest Regressor to predict a target variable. We use cross validation to estimate the regressor's ability to generalize on unseen data. Also, we want to tune a single hyperparameter, the max_depth of the trees, in the inner cross validation and evaluate the best estimator's performance in the outer cross validation loop. We will use the randomly generated dataset just as in the example in our [Getting-Started](getting-started.md) guide.

Note how we use Optuna distributions to specify the hyperparameter search space. In the same syntax, you can add all kinds of hyperparameter distributions to the params dictionary in the mapping.

Also, we use a model post processing function to extract the feature importances and plot them as SHAP beeswarm plots in Neptune. This is a very powerful feature of `flexcv` and you can use it to implement any kind of post processing you want. You can also use it to log additional metrics or results to Neptune.

```python
import optuna
from sklearn.ensemble import RandomForestRegressor
import flexcv.model_postprocessing as mp

model_map = ModelMappingDict({
    "RandomForest": ModelConfigDict({
	# now we specify, that we do want to evaluate the inner cross validation loop
        "requires_inner_cv": True,
	# let's specify the model's ability to run in parallel
        "n_jobs_model": -1,
        "n_jobs_cv": -1,
	# pass the model class
        "model": RandomForestRegressor,
	# pass the parameter distribution
        "params": {
            # we use optuna distributions to specify the hyperparameter search space
            # let's tune the max_depth of the trees between 5 and 100
            "max_depth": optuna.distributions.IntDistribution(5, 100), 
        },
	# pass a model post processing function
	# this can be useful for plotting, logging or additional results routines...
        "post_processor": mp.rf_post,
    }),
})
 
# instantiate the CrossValidation class
cv = CrossValidation()

# pass everything to CrossValidation using method chaining
results = (
    cv.set_data(X, y)
    .set_models(model_map)
    .set_inner_cv(3)
    .set_splits(n_splits_out=3)
    .set_run(Run())
    .perform()
    .get_results()
)
# Print the averaged RÂ²
n_values = len(results["RandomForest"]["metrics"])
r2_values = [results["RandomForest"]["metrics"][k]["r2"] for k in range(n_values)]
print(np.mean(r2_values))

# save the results table to an excel file
results.summary.to_excel("results.xlsx")
```

Note: If you have a model that does not have the `n_jobs` or `random_state` arguments in it's signature, you can prevent errors by specifying this in the `ModelConfigDict` like so:

```python
model_map = ModelMappingDict({
    "SVR": ModelConfigDict({
	# now we specify, that we do want to evaluate the inner cross validation loop
        "requires_inner_cv": True,
	# If a model can not run in parallel and does not allow a random_state specify
	"allows_seed": False,
	"allows_n_jobs": False,
	# pass the model class
        "model": SupportVectorRegressor,
        "post_processor": mp.svr_post,
    }),
})
```

## Working with Mixed Effects Models

To this point, we used regression models to make predictions based on the features in the data. We used basic kFold cross validation [1] to tune the hyperparameters of our random forest regressor and [2] estimate model generalization.

However, sometimes the data is not only structured by the feature matrix and the target values but also defined by a data hierarchy. This is common in study data that has been generated by participants. Mixed effects models are a type of statistical model used to analyze hierarchical data, where the data is organized into groups or clusters. In mixed effects models, both fixed and random effects are included in the model to account for the variation within and between the groups. The fixed effects are the same for all groups, while the random effects vary between the groups. Mixed effects models are useful when the data has a nested or hierarchical structure, such as in longitudinal studies, where measurements are taken over time on the same individuals, or in clustered data, where individuals are grouped together based on some characteristic. Mixed effects models can provide more accurate estimates of the effects of predictors and can account for the correlation between observations within the same group.

### Choosing a split

In order to respect the hierarchical structure of the data one has to choose an appropriate method for the cross validation splits.
The choice of split method in mixed effects models can have a significant impact on the model performance and the interpretation of the results. In hierarchical data, the observations are often clustered or grouped together, and the choice of split method can affect how the data is partitioned into training and validation sets. For example, if the data is clustered by geographic region, a split method that takes into account the clustering structure, such as the `GroupKFold` or `StratifiedGroupKFold` methods, may be more appropriate than a method that ignores the clustering structure, such as the `KFold` method. The choice of split method can also affect the estimation of the model parameters and the prediction accuracy of the model. Therefore, it's important to carefully consider the choice of split method when fitting mixed effects models to hierarchical data.

`flexcv` makes chosing and switching between split methods straight forward. You can simply exchange methods from the method calls.

```python
cv = CrossValidation().set_split(split_out="GroupKFold", split_in="GroupKFold")
```

`flexcv` solves the problems, where different splitting methods would not share signatures, internally, so you can concentrate on the research questions and do not have to jump into implementation.

### Stratification

In addition to that, we have to talk about stratification. Stratifying the grouping variable guarantees that both training and validation sets include a well-distributed sample of the groups or clusters within the data. This step is instrumental in reducing the likelihood of the model overfitting to specific groups or clusters, ultimately enhancing the model's generalization performance.

Similarly, stratifying the target variable ensures that the training and validation sets encompass a representative sample of target variable values. By doing so, it minimizes the risk of overfitting to particular ranges of target variable values, contributing to improved generalization performance. Usually this is the go-to way for multiclass problems. We implemented stratification for continuous targets by performing stratification based on a discretized copy of the continuous target. The data is discretized into percentiles and the distribution of the percentiles is preserved. The folding is, of course, taken out on the continuous target variable again.

Furthermore, our adapted stratification approach for continuous target variables ensures the inclusion of representative target variable values in both training and validation sets, even when the target variable is continuous. This refinement significantly enhances the model's generalization performance and mitigates the risk of overfitting to specific target variable value ranges.

In summary, stratifying both the grouping variable and the target variable is crucial for boosting the generalization performance of mixed effects models and reducing the risk of overfitting, especially when dealing with small sample sizes.

### Choosing a model type

For mixed effects modeling, `flexcv` offers two great possibilities:

1. Linear Mixed Effects Models (LMM)
2. Mixed Effects for Random Forests (MERF)

A linear mixed effects model, often abbreviated as LMM or just mixed model, is a statistical approach used to analyze data that exhibit both fixed and random effects. It is particularly valuable in situations where the data involves nested or hierarchical structures, repeated measures, or other forms of dependency.

The random effects capture the random variability or "nuisance" components in your data. Random effects are used to account for correlations and variations that may be specific to particular groups or clusters within your data. These effects are often assumed to follow a normal distribution.

The primary advantage of a linear mixed effects model is its ability to model and account for the variability within the data due to both fixed and random effects. This allows for more accurate and efficient modeling in cases where traditional linear regression may not be appropriate, such as when dealing with repeated measurements on the same subjects or when the data has a hierarchical structure.

Linear mixed effects models are widely used in various fields, including biology, psychology, social sciences, and economics, to address complex data analysis problems. They provide a flexible and powerful framework for understanding the underlying structure of data and making meaningful inferences.

`flexcv` provides a wrapper class to use LMM inside a nested cross validation pipeline alongside with other machine learning models.

Here is an example how you would apply an LMM to grouped sample data. 

In our implementation, we think of the model names passed as keys in the `ModelMappingDict` as referring to base estimators, i. e. fixed effects models. Mixed effects models often make use of base estimators. Therefore, we just append the LMM to the linear model's configuration.

```python
X, y, group, random_slopes = generate_regression(
    10, 100, n_slopes=1, noise_level=9.1e-2
)

model_map = ModelMappingDict(
    {
        "LinearModel": ModelConfigDict(
            {
                "requires_inner_cv": False,
                "requires_formula": True,
                "n_jobs_model": 1,
                "n_jobs_cv": 1,
                "model": LinearModel,
                "params": {},
                "post_processor": empty_func,
                "mixed_model": LinearMixedEffectsModel,
                "mixed_post_processor": empty_func,
                "mixed_name": "MixedLM",
            }
        ),
    }
)

cv = CrossValidation()
results = (
    cv.set_data(X, y, group, random_slopes)
    .set_splits(n_splits_out=3)
    .set_models(model_map)
    .set_mixed_effects(True)
    .set_run(Run())
    .perform()
    .get_results()
)
```

### MERF

The MERF class can be used to optimize any base estimator for mixed effects utilizing the expectation maximization (EM) algorithm. In the cross validation process, the base estimator is passed to MERF after hyperparameter tuning. There, a new instantance is created and fit to the data using the EM algorithm.

```python
X, y, group, random_slopes = generate_regression(
    10, 100, n_slopes=1, noise_level=9.1e-2
)

model_map = ModelMappingDict(
    {
        "RandomForest": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "requires_formula": False,
                "allows_seed": True,
                "allows_n_jobs": True,
                "n_jobs_model": -1,
                "n_jobs_cv": -1,
                "model": RandomForestRegressor,
                "params": {
                    "max_depth": optuna.distributions.IntDistribution(5, 100),
                    "n_estimators": optuna.distributions.CategoricalDistribution(
                        [10]
                    ),
                },
                "mixed_model": MERF,
                "post_processor": mp.rf_post,
                "mixed_post_processor": mp.expectation_maximation_post,
                "mixed_name": "MERF",
            }
        ),
    }
)

cv = CrossValidation()
results = (
    cv.set_data(X, y, group, random_slopes)
    .set_models(model_map)
    .set_inner_cv(3)
    .set_splits(n_splits_out=3)
    .set_run(Run(), random_seed=42)
    .set_mixed_effects(True, 25)
    .perform()
    .get_results()
)
```

## Repeated Cross Validation

Some of the cross validation splits are performed with shuffling the data before dividing in train and test splits. Therefore, you might wonder if your evaluation varies for multiple runs.

In the standard configuration, you would seed every run to make it absolutely reproducible. Now we want to explore, how different seeds influence the cross validation results. This is call repeated cross validation. We can still seed this process though by randomly generating a number of seeds. This makes even the repeated CV reproducible.

First, we create our random data set and a basic model mapping just as in a single run.

Second, we instantiate a `RepeatedCV` object. This class not only has the `set`-methods just as CrossValidation but also implements `set_n_repeats()` and `set_neptune()`. We can chain these methods because they also return the class `self` and we use them to set the number of repetitions as well as passing the credentials for Neptune runs. `RepeatedCV` then takes care of instantiating the desired number of runs and logs every single cross validation to it's own neptune run.

Most importantly `RepeatedCV` implements the iteration over single cross validation runs in it's `perform()` method. We can chain `perform()` in the same manner as we are now used to. The last element of our chain should also be `get_results`. This will allow us to inspect summary statistics as a measure of variance in the runs.

Here is the full code to perform cross validation 3 times and get summary statistics for all folds and models.

```python
from flexcv.synthesizer import generate_regression
from flexcv.models import LinearModel
from flexcv.model_mapping import ModelConfigDict, ModelMappingDict
from flexcv.repeated import RepeatedCV

# make sample data
X, y, group, random_slopes =generate_regression(10,100,n_slopes=1,noise_level=9.1e-2)

# create a basic model mapping
model_map = ModelMappingDict(
    {
    	"LinearModel": ModelConfigDict(
    	{
    		"model": LinearModel,
    	}
    ),
    }
)

credentials = {}

rcv = (
    RepeatedCV()
    .set_data(X, y, group,dataset_name="ExampleData")
    .set_models(model_map)
    .set_n_repeats(3)
    .set_neptune(credentials)
    .perform()
    .get_results()
)

rcv.summary.to_excel("repeated_cv.xlsx")  # save dataframe to excel file
```


## Evaluating multiple models

`flexcv` offers a great way of working with multiple models in a single machine learning run. It just iterates through the `ModelMappingDict`. As additional benefit, it provides extensive logging, results summaries and useful information such as progress bars for all layers of processes.

It is actually as simple as

```python
model_map = ModelMappingDict(
    {
        "LinearModel": ModelConfigDict(
            {
                "requires_inner_cv": False,
                "requires_formula": True,
                "n_jobs_model": 1,
                "n_jobs_cv": 1,
                "model": LinearModel,
                "params": {},
                "post_processor": empty_func,
                "mixed_model": LinearMixedEffectsModel,
                "mixed_post_processor": empty_func,
                "mixed_name": "MixedLM",
            },

        "RandomForest": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "requires_formula": False,
                "allows_seed": True,
                "allows_n_jobs": True,
                "n_jobs_model": -1,
                "n_jobs_cv": -1,
                "model": RandomForestRegressor,
                "params": {
                    "max_depth": optuna.distributions.IntDistribution(5, 100),
                    "n_estimators": optuna.distributions.CategoricalDistribution(
                        [10]
                    ),
                },
                "mixed_model": MERF,
                "post_processor": mp.rf_post,
                "mixed_post_processor": mp.expectation_maximation_post,
                "mixed_name": "MERF",
            }
        ),
    }
)
```

Have a look at this section from `flexcv.model_mapping_template` for how to add multiple models to a `ModelMappingDict`:

```python
import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor

from . import model_postprocessing as mp
from .merf import MERF
from .model_mapping import ModelConfigDict, ModelMappingDict
from .models import EarthRegressor, LinearMixedEffectsModel, LinearModel

MODEL_MAPPING = ModelMappingDict(
    {
        "LinearModel": ModelConfigDict(
            {
                "requires_inner_cv": False,
                "n_trials": 100,
                "n_jobs_model": 1,
                "n_jobs_cv": 1,
                "model": LinearModel,
                "params": {},
                "post_processor": mp.lm_post,
                "level_4_model": LinearMixedEffectsModel,
                "level_4_post_processor": mp.lmer_post,
                "level_4_name": "MixedLM",
            }
        ),
        "RandomForest": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "n_trials": 400,
                "n_jobs_model": 1,
                "n_jobs_cv": -1,
                "model": RandomForestRegressor,
                "params": {
                    "max_depth": optuna.distributions.IntDistribution(5, 100),
                    "min_samples_split": optuna.distributions.IntDistribution(
                        2, 1000, log=True
                    ),
                    "min_samples_leaf": optuna.distributions.IntDistribution(
                        2, 5000, log=True
                    ),
                    "max_samples": optuna.distributions.FloatDistribution(0.0021, 0.9),
                    "max_features": optuna.distributions.IntDistribution(1, 10),
                    "max_leaf_nodes": optuna.distributions.IntDistribution(10, 40000),
                    "min_impurity_decrease": optuna.distributions.FloatDistribution(
                        1e-8, 0.02, log=True
                    ),  # >>>> can be (1e-8, .01, log=True)
                    "min_weight_fraction_leaf": optuna.distributions.FloatDistribution(
                        0, 0.5
                    ),  # must be a float in the range [0.0, 0.5]
                    "ccp_alpha": optuna.distributions.FloatDistribution(1e-8, 0.01),
                    "n_estimators": optuna.distributions.IntDistribution(2, 7000),
                },
                "post_processor": mp.rf_post,
                "level_4_model": MERF,
                "level_4_post_processor": mp.expectation_maximation_post,
                "level_4_name": "MERF",
            }
        ),
        "XGBoost": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "n_trials": 300,
                "n_jobs_model": 1,
                "n_jobs_cv": -1,
                "model": XGBRegressor,
                "params": {
                    "max_depth": optuna.distributions.IntDistribution(2, 700),
                    "learning_rate": optuna.distributions.FloatDistribution(0.01, 0.8),
                    "n_estimators": optuna.distributions.IntDistribution(5, 5000),
                    "min_child_weight": optuna.distributions.IntDistribution(2, 100),
                    # "max_delta_step": optuna.distributions.FloatDistribution(0.1, 10.0),
                    # "gamma": optuna.distributions.FloatDistribution(0.0, 50),
                    "subsample": optuna.distributions.FloatDistribution(0.005, 0.97),
                    "colsample_bytree": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "colsample_bylevel": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "colsample_bynode": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "colsample_bytree": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "colsample_bylevel": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "colsample_bynode": optuna.distributions.FloatDistribution(
                        0.1, 1, step=0.1
                    ),
                    "reg_alpha": optuna.distributions.FloatDistribution(0.1, 500),
                    "reg_lambda": optuna.distributions.FloatDistribution(0.001, 800),
                },
                "post_processor": mp.xgboost_post,
                "level_4_model": MERF,
                "level_4_post_processor": mp.expectation_maximation_post,
                "level_4_name": "XGBEM",
            }
        ),
        "MARS": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "n_trials": 200,
                "allows_n_jobs": False
                "model": EarthRegressor,
                "params": {  # 'degree', 'endspan', 'fast_beta', 'fast_k', 'minspan', 'newvar_penalty', 'nk', 'nprune', 'pmethod', 'random_state', 'thresh'
                    "degree": optuna.distributions.IntDistribution(1, 5),
                    "nprune": optuna.distributions.IntDistribution(1, 300),
                    # "fast_k": optuna.distributions.CategoricalDistribution([0, 1, 5, 10, 20]),  #
                    "fast_k": optuna.distributions.IntDistribution(0, 20),  #
                    # "nk": does not help
                    "newvar_penalty": optuna.distributions.FloatDistribution(0.01, 0.2),
                    # "pmethod": # use default: backward
                    # "fast_beta": # default(=1) yielded best results
                },
                "post_processor": mp.mars_post,
                "level_4_model": MERF,
                "level_4_post_processor": mp.expectation_maximation_post,
                "level_4_name": "EarthEM",
            }
        ),
        "SVR": ModelConfigDict(
            {
                "requires_inner_cv": True,
                "n_trials": 450,
                "allows_n_jobs": False,
                "n_jobs_cv": -1,
                "model": SVR,
                "params": {
                    # Most Important: Kernel + C
                    # "kernel": default "rbf" yielded best results
                    # "degree": # for poly only
                    "C": optuna.distributions.FloatDistribution(0.001, 50, log=True),
                    "epsilon": optuna.distributions.FloatDistribution(0.1, 1.3),
                    "gamma": optuna.distributions.FloatDistribution(
                        1e-5, 0.1, log=True
                    ),  # better than default "scale"
                    # "tol": optuna.distributions.FloatDistribution(1e-4, 10),
                    # "shrinking": default "True" yielded best restults
                },
                "post_processor": mp.svr_post,
                "level_4_model": MERF,
                "level_4_post_processor": mp.expectation_maximation_post,
                "level_4_name": "SVREM",
            }
        ),
    }
)

```
