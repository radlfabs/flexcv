{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flexcv\n",
    "## A Machine Learning and Method Comparison Framework with Nested CV\n",
    "Use this example notebook to understand what you can use this repository for and how it can help you with you machine learning tasks.\n",
    "##### We used it to\n",
    "1. compare different models (methods),\n",
    "2. given different sets of predictors\n",
    "3. for different datasets\n",
    "4. using both fixed and random effects as well as random slopes.\n",
    "##### Furthermore it can help you to\n",
    "1. perform nested cross validation: \n",
    "    - in the outer loop we evaluate cross validate model performance,\n",
    "    - in the outer loop we select model parameters fairly without data leakage (inner cross validation) using a state-of-the-art optimization package,\n",
    "3. customize objective functions for optimization to select meaningful model parameters,\n",
    "4. scale inner and outer cross validation loops separately,\n",
    "5. use different cross validation splits: dependant and independant of the clustering structures in your data,\n",
    "6. use a state-of-the-art logging dashboard to track all of your experiments.\n",
    "7. repeated runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Method Comparison Framework\n",
    "\n",
    "This notebook provides an example of how to use the ML Method Comparison Framework to compare different machine learning models on different datasets with different sets of predictors. The framework supports both fixed and random effects, as well as random slopes.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "To use the framework, we will follow these steps in our example:\n",
    "\n",
    "1. We will prepare randomly generated data for regression including a clustering hierarchy and random slopes.\n",
    "\n",
    "## Features\n",
    "\n",
    "The ML Method Comparison Framework provides the following features:\n",
    "\n",
    "1. Cross validation of model performance using different cross validation splits that are dependent or independent of the clustering structures in your data.\n",
    "2. Selection of model parameters fairly without data leakage using an inner cross validation loop and a state-of-the-art optimization package.\n",
    "3. Customization of objective functions for optimization to select meaningful model parameters.\n",
    "4. Scaling of inner and outer cross validation loops separately.\n",
    "5. A state-of-the-art logging dashboard to track all of your experiments.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The ML Method Comparison Framework is a powerful tool for comparing machine learning models on different datasets with different sets of predictors. It provides a range of features for cross validation, parameter selection, and experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really don't want to use Neptune for logging, you can use our DummyLogger instead and run the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Data\n",
    "\n",
    "For example purposes, let's generate some random data for regression. We will create the\n",
    "1. X: a pandas DataFrame with all of our predictors\n",
    "2. y: a pandas Series containing the target variable (continuous, since it's regression)\n",
    "3. group: a Series with same number of rows as y and X indicating which person made the observation or to which semantic cluster the observation belongs. The groups Series will be used to create a random clustering structure in the data. \n",
    "4. Also, we use some of the X columns as random slopes.\n",
    "\n",
    "The target variable is calculated as a linear combination of the predictors plus some noise.\n",
    "\n",
    "$$ y = 1 + X \\cdot \\beta + \\lambda \\cdot \\epsilon $$\n",
    "\n",
    "With $\\beta$ being a randomly generated vector of coefficients and $\\epsilon$ being the noise drawn from a normal distribution with mean 0 and standard deviation 1. $\\lambda$ is a factor that can be changed by the user to adjust the noise level.\n",
    "\n",
    "\n",
    "You can use this LaTeX code to render the equations in your LaTeX document.\n",
    "\n",
    "Then, we write a test function, that returns the fixed effects R² of our regression data. This can be handy to force a specific R² value to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def select_random_columns(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select n random columns from a pandas DataFrame and return a new DataFrame containing only these columns.\n",
    "    :param df: input DataFrame\n",
    "    :param n: number of columns to select\n",
    "    :return: DataFrame containing n randomly selected columns\n",
    "    \"\"\"\n",
    "    column_names = df.columns.tolist()\n",
    "    random_column_names = np.random.choice(column_names, size=n, replace=False)\n",
    "    return df[random_column_names]\n",
    "\n",
    "\n",
    "def generate_regression(m_features: int, n_samples: int, n_groups: int = 100, n_slopes = 1, noise = 0.1):\n",
    "    \"\"\"\n",
    "    Generate a dataset for linear regression using the numpy default rng.\n",
    "    :param m_features: number of features\n",
    "    :param n_samples: number of samples\n",
    "    :param n_groups: number of groups\n",
    "    :param n_slopes: number of slopes\n",
    "    :param noise: noise level\n",
    "    :return: dataset\n",
    "    \"\"\"\n",
    "    FIXED_LEVEL = 0.01\n",
    "    RANDOM_LEVEL = 1\n",
    "\n",
    "    # initialize random number generator and generate predictors randomly \n",
    "    rng = np.random.default_rng(42)\n",
    "    X = rng.integers(-10, 10, (n_samples, m_features))\n",
    "\n",
    "    # generate random coefficients with a linear relationship and add noise\n",
    "    beta_fixed = rng.random(m_features) * FIXED_LEVEL\n",
    "    epsilon = rng.standard_normal(n_samples) * noise\n",
    "    # y = 1 + X @ beta + epsilon\n",
    "\n",
    "    # generate random group labels\n",
    "    group = rng.integers(0, n_groups, n_samples)\n",
    "\n",
    "    # generate random effects -> linear relationships for each group\n",
    "    beta_random = rng.uniform(0.9, 1.0, size=n_groups) * RANDOM_LEVEL\n",
    "    # reorder the random effects betas according to the group labels\n",
    "    random_effects = beta_random[group]\n",
    "\n",
    "    # generate the response variable\n",
    "    y = 1 + X @ beta_fixed + random_effects + epsilon\n",
    "\n",
    "    # convert to pandas dataframe with random column names\n",
    "    X = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n",
    "    y = pd.Series(y, name=\"y\")\n",
    "    group = pd.Series(group, name=\"group\")\n",
    "\n",
    "    # select a random column to be the random slope\n",
    "    random_slopes = select_random_columns(X, n_slopes)\n",
    "    return X, y, group, random_slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we can generate our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, group, random_slopes = generate_regression(10, 1000, n_slopes=1, noise=0.096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, feel free to import your own data set like this.\n",
    "```python\n",
    "    df = pd.read_csv(\"your_data.csv\")\n",
    "    X = df.drop(columns=[\"y\"])\n",
    "    y = df[\"y\"]\n",
    "    group = df[\"group\"]\n",
    "    random_slopes = X[\"a_column_of_choice\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X DataFrame.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   X0      1000 non-null   int64\n",
      " 1   X1      1000 non-null   int64\n",
      " 2   X2      1000 non-null   int64\n",
      " 3   X3      1000 non-null   int64\n",
      " 4   X4      1000 non-null   int64\n",
      " 5   X5      1000 non-null   int64\n",
      " 6   X6      1000 non-null   int64\n",
      " 7   X7      1000 non-null   int64\n",
      " 8   X8      1000 non-null   int64\n",
      " 9   X9      1000 non-null   int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 78.2 KB\n",
      "None\n",
      "\n",
      "Y Series Description.\n",
      "count    1000.000000\n",
      "mean        1.926191\n",
      "std         0.142295\n",
      "min         1.453501\n",
      "25%         1.832760\n",
      "50%         1.922994\n",
      "75%         2.020629\n",
      "max         2.408882\n",
      "Name: y, dtype: float64\n",
      "\n",
      "Group Series Description\n",
      "count    1000.000000\n",
      "mean       47.590000\n",
      "std        29.064491\n",
      "min         0.000000\n",
      "25%        22.000000\n",
      "50%        46.000000\n",
      "75%        73.000000\n",
      "max        99.000000\n",
      "Name: group, dtype: float64\n",
      "\n",
      "Random Slope Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   X7      1000 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 7.9 KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some basic info about the \n",
    "print(\"X DataFrame.\")\n",
    "print(X.info(), end=\"\\n\\n\")\n",
    "print(\"Y Series Description.\")\n",
    "print(y.describe(), end=\"\\n\\n\")\n",
    "print(\"Group Series Description\")\n",
    "print(group.describe(), end=\"\\n\\n\")\n",
    "print(\"Random Slope Info\")\n",
    "print(random_slopes.info(), end=\"\\n\\n\")\n",
    "\n",
    "group.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some classes and functions we will work with in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pprint import pprint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from flexcv.merf_adaptation import MERF\n",
    "from flexcv.models import LinearModel\n",
    "from flexcv.models import LinearMixedEffectsModel\n",
    "import flexcv.model_postprocessing as mp\n",
    "\n",
    "from flexcv.cross_val import cross_validate\n",
    "from flexcv.run import RunConfigurator\n",
    "from flexcv.data_loader import DataLoadConfig\n",
    "from flexcv.cross_val_split import CrossValMethod\n",
    "from flexcv.run import Run\n",
    "dummy_run = Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing our framework allows you to do, is to use different cross validation splits. You can choose between splits that are dependent or independent of the clustering structures in your data. We use the CrossValMethod class to define the cross validation method we want to use. This class is an Enum with the following options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CUSTOM': <CrossValMethod.CUSTOM: 'CustomStratifiedGroupKFold'>,\n",
      " 'GROUPKFOLD': <CrossValMethod.GROUPKFOLD: 'GroupKFold'>,\n",
      " 'KFOLD': <CrossValMethod.KFOLD: 'KFold'>,\n",
      " 'STRAT': <CrossValMethod.STRAT: 'CustomStratifiedKFold'>,\n",
      " 'STRATGROUPKFOLD': <CrossValMethod.STRATGROUPKFOLD: 'StratifiedGroupKFold'>}\n"
     ]
    }
   ],
   "source": [
    "pprint(CrossValMethod._member_map_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Enum class is used to map the cross validation method to a string and make it easier to use in the framework.\n",
    "The most basic split can be obtained with KFOLD. This will split the data into k folds and use one fold for testing and the remaining folds for training. The folds are created randomly and are independent of the clustering structure in the data.\n",
    "If you want to make sure that the distribution of your target variable in the folds is similar to the distribution of the target variable in the whole dataset, you can use STRAT. This will create folds that are similar to the whole dataset in terms of the distribution of the target variable (stratification). The folds are created randomly and are independent of the clustering structure in the data.\n",
    "Next you have the possibility to add dependency to the clustering structure in your data. This can be done with the methods GROUPKFOLD, STRATGROUPKFOLD and CUSTOM. The folds are created randomly and are dependent of the clustering structure in the data in terms of the scikit-learn logic, where groups will not appear in training as well as testing folds and will not be split and the number of groups per fold is approximately the same. Now, you can also use stratification to make sure that the distribution of your target variable in the folds is similar to the distribution of the target variable in the whole dataset. This can be done with the methods STRATGROUPKFOLD and CUSTOM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to know our paper-related convention of using the model_level variable. If set to 4, random effects are modelled. If set to 3, only fixed effects are modelled. This is useful for comparing different models with different random effects structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We control how runs are initiated and conducted with our RunConfigurator class from our run module.\n",
    "Also, we use an instance of the class DataLoadConfig to hold our dataset-related info of the run.\n",
    "Let's create an instance of the class RunConfigurator. We will use it to initiate and conduct our runs.\n",
    "The dataset name is used for logging purposes. Also, some of the model parameters could be made dataset-specific.\n",
    "With the model_level attribute we can control, how the random effects are modelled. In line with our paper published, we use an integer enumeration of model_level. This makes it easy to compare different sets of predictors. If set to 4, which is the highest level in the paper, random effects are modelled. If set to 3, only fixed effects are modelled. This is useful for comparing different models with different random effects structures.\n",
    "For convenience, you can just use the str \"fixed_only\" or \"mixed\" instead of the integer enumeration. Then, \"fixed_only\" we internally set to model_level 3 and \"mixed\" to model_level 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_load_config = DataLoadConfig(\n",
    "    dataset_name=\"random_example\",  # choose arbitrary name\n",
    "    model_level=\"fixed_only\",  # use: \"fixed_only\", \"mixed\"\n",
    "    target_name=y.name,\n",
    "    slopes=random_slopes.columns.tolist(),  # list of column names; does not need to be specified for fixed_only\n",
    ")\n",
    "\n",
    "run_config = RunConfigurator(\n",
    "    data_load_config,\n",
    "    cross_val_method=CrossValMethod.KFOLD,\n",
    "    cross_val_method_in=CrossValMethod.KFOLD,\n",
    "    n_splits=3,\n",
    "    n_trials=3,\n",
    "    run=dummy_run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to set up some models to be fit to and evaluated on our data.\n",
    "We can configure different models at once by using a model_mapping dictionary. We will use it to create our models in the next step.\n",
    "It maps the model names to the model classes which are instantited during fit and evaluation.\n",
    "Our model mapping dictionary is used for the following purposes:\n",
    "1. to create the models in the cross_validate function by iterating over the dictionary keys,\n",
    "2. to control specific parameters in the cross_validate function that will determine how the procedure is conducted for each model (e.g. n_jobs: Some models behave differently when n_jobs is set to 1 or -1. This can be important when using model types that are instantiated using rpy2 under the hood. We use rpy2 for the EARTH estimator.),\n",
    "3. to determine whether inner cross validation is necessary. For example, a simple linear model can not be tuned in inner CV, therefore, we set the inner_cv parameter to False for the LinearRegression model. This will skip the inner CV loop for this model.\n",
    "4. to provide a distribution of model parameters. The optimization algorithm will choose from this distributions in the inner CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_mapping: Dict[str, Dict] = {\n",
    "    \"LinearModel\": {\n",
    "        \"inner_cv\": False,\n",
    "        \"n_trials\": 3,\n",
    "        \"n_jobs_model\": {\"n_jobs\": 1},\n",
    "        \"n_jobs_cv\": 1,\n",
    "        \"model\": LinearModel,\n",
    "        \"params\": {},\n",
    "        \"post_processor\": mp.lm_post,\n",
    "        \"mixed_model\": LinearMixedEffectsModel,\n",
    "        \"mixed_post_processor\": mp.lmer_post,\n",
    "        \"mixed_name\": \"MixedLM\"\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"inner_cv\": True,\n",
    "        \"n_trials\": 3,\n",
    "        \"n_jobs_model\": {\"n_jobs\": -1},\n",
    "        \"n_jobs_cv\": -1,\n",
    "        \"model\": RandomForestRegressor,\n",
    "        \"params\": {\n",
    "            \"max_depth\": optuna.distributions.IntDistribution(5, 15), \n",
    "        },\n",
    "        \"post_processor\": lambda *args, **kwargs: None,\n",
    "        \"mixed_model\": MERF,\n",
    "        \"mixed_post_processor\": lambda *args, **kwargs: None,\n",
    "        \"mixed_name\": \"MERF\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to pass everything to our custom cross_validate function. It will conduct the cross validation and log the results to Neptune or to our Dummy run object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Neptune run object passed. Logging to Neptune will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-10-13 17:31:14,078]\u001b[0m A new study created in memory with name: no-name-83a49dd2-5875-4fec-982f-45229b5593cd\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:31:21,910]\u001b[0m Trial 0 finished with value: -0.012279295467611978 and parameters: {'model__max_depth': 10}. Best is trial 0 with value: -0.012279295467611978.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:31:29,888]\u001b[0m Trial 1 finished with value: -0.012217374156329003 and parameters: {'model__max_depth': 14}. Best is trial 1 with value: -0.012217374156329003.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:31:38,117]\u001b[0m Trial 2 finished with value: -0.01284964372469631 and parameters: {'model__max_depth': 5}. Best is trial 1 with value: -0.012217374156329003.\u001b[0m\n",
      " cv:  33%|███▎      | 1/3 [00:25<00:51, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-10-13 17:31:39,630]\u001b[0m A new study created in memory with name: no-name-54dfab84-3874-48ae-b030-595af6ee277d\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:31:47,558]\u001b[0m Trial 0 finished with value: -0.012340737436364222 and parameters: {'model__max_depth': 10}. Best is trial 0 with value: -0.012340737436364222.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:31:55,529]\u001b[0m Trial 1 finished with value: -0.012338906568062096 and parameters: {'model__max_depth': 14}. Best is trial 1 with value: -0.012338906568062096.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:32:03,595]\u001b[0m Trial 2 finished with value: -0.012558664819044843 and parameters: {'model__max_depth': 5}. Best is trial 1 with value: -0.012338906568062096.\u001b[0m\n",
      " cv:  67%|██████▋   | 2/3 [00:51<00:25, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-10-13 17:32:05,022]\u001b[0m A new study created in memory with name: no-name-425160bd-d7d6-4b6a-ad18-9a100aa86b69\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:32:13,078]\u001b[0m Trial 0 finished with value: -0.013274169204275344 and parameters: {'model__max_depth': 10}. Best is trial 0 with value: -0.013274169204275344.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:32:21,052]\u001b[0m Trial 1 finished with value: -0.01328609671204859 and parameters: {'model__max_depth': 14}. Best is trial 0 with value: -0.013274169204275344.\u001b[0m\n",
      "\u001b[32m[I 2023-10-13 17:32:29,023]\u001b[0m Trial 2 finished with value: -0.013618672761191018 and parameters: {'model__max_depth': 5}. Best is trial 0 with value: -0.013274169204275344.\u001b[0m\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from flexcv.cross_val import cross_validate as cv\n",
    "\n",
    "\n",
    "\n",
    "results = cv(\n",
    "    RunConfiguration=run_config,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    run=dummy_run,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    "    mapping=model_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel:\n",
      "    Test Performance:\n",
      "    R² per Fold: [0.5001471722774329, 0.4428348736973795, 0.5662113835268121, 0.544414529894409, 0.4771107847618129] Mean R²: 0.506\n",
      "    MSE in Training:\n",
      "    MSE per Fold: [0.009233842447746303, 0.008962565104658076, 0.009562860850629237, 0.009068444327763995, 0.008949417737302932] Mean MSE: 0.009\n",
      "    MSE in Test:\n",
      "    MSE per Fold: [0.009109537298773058, 0.0102188709459474, 0.007831051136590442, 0.009672573153155612, 0.0103238508384143] Mean MSE: 0.009\n",
      "    \n",
      "\n",
      "    \n",
      "MixedLM:\n",
      "    Test Performance:\n",
      "    R² per Fold: [0.5025151107145351, 0.44872437532912934, 0.5656435722211428, 0.5431896496072073, 0.47689134351924334] Mean R²: 0.507\n",
      "    MSE in Training:\n",
      "    MSE per Fold: [0.009246331831114658, 0.008978093643965101, 0.00958172794994147, 0.00907908331799589, 0.008950531681055064] Mean MSE: 0.009\n",
      "    MSE in Test:\n",
      "    MSE per Fold: [0.009066382949497367, 0.010110852596862662, 0.007841301657700892, 0.009698578688799824, 0.010328183455324467] Mean MSE: 0.009\n",
      "    \n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for key, value in results.items():\n",
    "    print(f\"\"\"{key}:\n",
    "    Test Performance:\n",
    "    R² per Fold: {value['results']['r2']} Mean R²: {np.mean(value['results']['r2']):.3f}\n",
    "    MSE in Training:\n",
    "    MSE per Fold: {value['results']['mse_train']} Mean MSE: {np.mean(value['results']['mse_train']):.3f}\n",
    "    MSE in Test:\n",
    "    MSE per Fold: {value['results']['mse']} Mean MSE: {np.mean(value['results']['mse']):.3f}\n",
    "    \\n\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our linear model is able to approximately reproduce the fixed effects R² of our data. The basic Random Forest Regressor is not well tuned and therefore performs poorly at the moment. Feel free to play around with the parameters of the Random Forest Regressor and especially increase the n_trials parameter to allow for a longer parameter optimization process and see how it affects the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we want to also incorporate Random Effects into our model. Therefore, we have to change model_level in our DataLoadConfig instance to \"mixed\". This will allow the cross_validate function to fit models with random effects. The corss_validate function will first model the data with the fixed effects only model, and then builds a new model with random effects. So you always have fixed and random effects side by side in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map: Dict[str, Dict] = {\n",
    "    \"LinearModel\": {\n",
    "        \"inner_cv\": False,\n",
    "        \"n_trials\": 100,\n",
    "        \"n_jobs_model\": {\"n_jobs\": 1},\n",
    "        \"n_jobs_cv\": 1,\n",
    "        \"model\": LinearModel,\n",
    "        \"params\": {},\n",
    "        \"post_processor\": mp.lm_post,\n",
    "        \"mixed_model\": LinearMixedEffectsModel,\n",
    "        \"mixed_post_processor\": mp.lmer_post,\n",
    "        \"mixed_name\": \"MixedLM\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Neptune run object passed. Logging to Neptune will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  20%|██        | 1/5 [00:05<00:23,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  40%|████      | 2/5 [00:11<00:17,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  60%|██████    | 3/5 [00:16<00:11,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  80%|████████  | 4/5 [00:22<00:05,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LinearModel:\n",
      "R² per Fold: [0.5001471722774329, 0.4428348736973795, 0.5662113835268121, 0.544414529894409, 0.4771107847618129] Mean R²: 0.506\n",
      "MixedLM:\n",
      "R² per Fold: [0.5025151107145351, 0.44872437532912934, 0.5656435722211428, 0.5431896496072073, 0.47689134351924334] Mean R²: 0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "run_config = RunConfigurator(\n",
    "    data_load_config,\n",
    "    cross_val_method=CrossValMethod.CUSTOM,\n",
    "    cross_val_method_in=CrossValMethod.CUSTOM,\n",
    "    n_splits=5,\n",
    "    n_trials=10,\n",
    "    em_stopping_window=10,\n",
    "    run=dummy_run\n",
    ")\n",
    "results = cross_validate(\n",
    "    RunConfiguration=run_config,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    run=dummy_run,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    "    mapping=model_map\n",
    ")\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\nR² per Fold: {value['results']['r2']} Mean R²: {np.mean(value['results']['r2']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the structure of the results object. It is a nested dictionary, which means that some of the values are dictionaries themselves. The outer dictionary has the model names as keys. The inner dictionaries contain model objects, their parameters or summaries, a separate results dictionary containing only metrics. Also, we find lists of the predictions and the true values (testing and training) for each fold of the cross validation.\n",
    "Results is constructed like this:\n",
    "```python\n",
    "    results: Dict[str, Dict[str, list | dict]] = {\n",
    "        model_name: {\n",
    "            \"model\": [],\n",
    "            \"parameters\": [],\n",
    "            \"metrics\": {},\n",
    "            \"y_pred\": [],\n",
    "            \"y_test\": [],\n",
    "            \"shap_values\": [],\n",
    "        }\n",
    "        for model_name in model_keys\n",
    "    }\n",
    "```\n",
    "With the following function we can quickly get an overview of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel\n",
      "  model\n",
      "  parameters\n",
      "  results\n",
      "    r2\n",
      "    r2_train\n",
      "    mse\n",
      "    mse_train\n",
      "    mse_in_test\n",
      "    mse_in_train\n",
      "    mae\n",
      "    mae_train\n",
      "    rmse\n",
      "    rmse_train\n",
      "  r2\n",
      "  r2_train\n",
      "  y_pred\n",
      "  y_test\n",
      "  shap_values\n",
      "  median_index\n",
      "MixedLM\n",
      "  model\n",
      "  parameters\n",
      "  results\n",
      "    r2\n",
      "    r2_train\n",
      "    mse\n",
      "    mse_train\n",
      "    mse_in_test\n",
      "    mse_in_train\n",
      "    mae\n",
      "    mae_train\n",
      "    rmse\n",
      "    rmse_train\n",
      "  r2\n",
      "  r2_train\n",
      "  y_pred\n",
      "  y_test\n",
      "  shap_values\n",
      "  median_index\n"
     ]
    }
   ],
   "source": [
    "def pprint_dict(d, indent=\"\"):\n",
    "    \"\"\"\n",
    "    Pretty-print a dictionary, only printing values that are themselves dictionaries.\n",
    "    :param d: dictionary to print\n",
    "    \"\"\"\n",
    "    for key, value in d.items():\n",
    "        print(f\"{indent}{key}\")\n",
    "        if isinstance(value, dict):\n",
    "            pprint_dict(value, indent + \"  \")\n",
    "\n",
    "pprint_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Neptune run object passed. Logging to Neptune will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  20%|██        | 1/5 [00:08<00:32,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " cv:  40%|████      | 2/5 [00:16<00:25,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      " cv:  60%|██████    | 3/5 [00:25<00:16,  8.33s/it]More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      " cv:  80%|████████  | 4/5 [00:39<00:10, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LinearModel:\n",
      "R² per Fold: [0.46691057493789, 0.5227469970953811, 0.4934364268372097, 0.4435647719203011, 0.5219123305294142] Mean R²: 0.490\n",
      "MixedLM:\n",
      "R² per Fold: [0.4668807469467139, 0.5225661018398995, 0.49084686987324067, 0.4437517006171371, 0.5219145992976639] Mean R²: 0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "run_config = RunConfigurator(\n",
    "    data_load_config,\n",
    "    cross_val_method=CrossValMethod.KFOLD,\n",
    "    cross_val_method_in=CrossValMethod.KFOLD,\n",
    "    n_splits=5,\n",
    "    n_trials=10,\n",
    "    em_stopping_window=10,\n",
    "    run=dummy_run\n",
    ")\n",
    "results = cross_validate(\n",
    "    RunConfiguration=run_config,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    run=dummy_run,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    "    mapping=model_map\n",
    ")\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\nR² per Fold: {value['results']['r2']} Mean R²: {np.mean(value['results']['r2']):.3f}\")\n",
    "    # TODO show other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated Runs\n",
    "What we have done so far was using nested cross validation for a single time. In our program, there are some processes that make use of randomization. For example, the random forest regressor uses randomization to build the trees. Also the data is split into train and test folds utilizing a random seed. Therefore, we can not be sure that the results we get are not just a result of the randomization. To make sure that our results are not just a result of the randomization, we can repeat the cross validation process multiple times. Let's see how repeated runs can be easily obtained with our framework.\n",
    "In the following implementation, we construct a set of random runs each using a different seed. The set however is also reproducable since we are seeding the random number generator before creating the runs. This way, we can even reproduce the results of the repeated runs exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_repeats = 3\n",
    "\n",
    "dataset = \"repeated_example\"\n",
    "\n",
    "run_config = RunConfigurator(\n",
    "    data_load_config,\n",
    "    cross_val_method=CrossValMethod.CUSTOM,\n",
    "    cross_val_method_in=CrossValMethod.CUSTOM,\n",
    "    n_splits=5,\n",
    "    n_trials=10,\n",
    "    em_stopping_window=10,\n",
    "    run=dummy_run\n",
    ")\n",
    "\n",
    "repeated_run = dummy_run()  # neptune.init_run(**your_credentials)\n",
    "repeated_id = repeated_run[\"sys/id\"].fetch()\n",
    "desc = f\"Instance of repeated run {repeated_id}.\"\n",
    "\n",
    "# set numpy seed to 42. \n",
    "# If you do not want to reproduce the repeated run, change the seed or remove the line\n",
    "np.random.seed(42)\n",
    "\n",
    "# generate a list of random seeds to use in the repeated loop\n",
    "# the random seeds are used to generate the random folds in the repeated loop\n",
    "seeds = np.random.randint(42000, size=n_repeats).tolist()\n",
    "\n",
    "run_ids = []\n",
    "run_metrics = []\n",
    "for seed in seeds:\n",
    "    # create a new run for each repeat\n",
    "    # neptune will log every inner run as well as the host run\n",
    "    inner_run = dummy_run()  # neptune.init_run(**your_credentials)\n",
    "    inner_id = inner_run[\"sys/id\"].fetch()\n",
    "    inner_run[\"HostRun\"] = repeated_id\n",
    "    inner_run[\"seed\"] = seed\n",
    "\n",
    "    # call cross_validate on the random seed\n",
    "    run_metric = cross_validate(\n",
    "        RunConfiguration=run_config,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        run=dummy_run,\n",
    "        group=group,\n",
    "        slopes=random_slopes,\n",
    "        mapping=model_map,\n",
    "        random_seed=seed,  # this line is crucial in the repeated loop\n",
    "    )\n",
    "\n",
    "    # append the run id and the run metric to the lists\n",
    "    run_ids.append(inner_id)\n",
    "    run_metrics.append(run_metric)\n",
    "\n",
    "from run_repeated_cv import aggregate_\n",
    "\n",
    "df = aggregate_(run_metrics)\n",
    "df.to_excel(\"repeated_cv.xlsx\")  # save dataframe to excel file\n",
    "print(df)  # print dataframe to console\n",
    "\n",
    "from neptune import File\n",
    "# log the repeated run results to neptune\n",
    "repeated_id = repeated_run[\"sys/id\"].fetch()\n",
    "repeated_run[\"summary\"].upload(File.as_html(df))\n",
    "repeated_run[\"sys/description\"] = f\"Host run for repeated runs with {n_repeats} repeats. run_ids: {run_ids}\"\n",
    "repeated_run[\"RelatedRuns\"] = \", \".join(run_ids)\n",
    "repeated_run[\"seeds\"] = seeds\n",
    "repeated_run[\"mapping\"] = model_map\n",
    "repeated_run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a follow up step, we can now use a custom aggregation function to aggregate the results of the repeated runs. For example, we can calculate the mean and standard deviation of the metrics over the repeated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Evaluation Metrics\n",
    "If you use cross_validate without the metrics keyword argument, the function uses the following mapping by default:\n",
    "\n",
    "```python\n",
    "METRICS = {\n",
    "    \"r2\": r2_score,\n",
    "    \"mse\": mean_squared_error,\n",
    "    \"mae\": mean_absolute_error,\n",
    "    \"rmse\": rmse_score,\n",
    "}\n",
    "```\n",
    "In this dictionary, the keys are the names of the metrics and the values are the functions that calculate the metrics. You can add your own metrics to this dictionary or overwrite the existing ones. The function will then calculate the metrics and add them to the results dictionary. Per default the scoring functions are imported from the sklearn.metrics module. You can also use your own functions. Just make sure that they have the same signature as the functions in the METRICS dictionary (y_true, y_pred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to add an evaluation metric to the cross_validate function\n",
    "# get machine precison\n",
    "EPS = np.finfo(np.float64).eps\n",
    "metrics = {\n",
    "    \"res_below_threshold\": lambda y_true, y_pred: int(np.abs(y_true - y_pred) < EPS)  # this metric is 1 if the residual is below machine precision and 0 otherwise\n",
    "}\n",
    "\n",
    "results = cross_validate(\n",
    "    RunConfiguration=run_config,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    run=dummy_run,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    "    mapping=model_map,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the objective scorer function\n",
    "In the nner cross validation the optuna package uses a Bayesian sampler to sample a set of parameters from the parameter space (defined by the parameter distributions). The objective function is used to evaluate the performance of the model with the specific set of parameters. Each set of parameters is cross validated using a scorer method which is used as the objective function that is to be maximized. Per default, the scorer function is the mean squared error. You can change the scorer function by passing a different function to the scorer keyword argument of the cross_validate function. The function will then be used as the objective function in the inner cross validation loop.\n",
    "For example you can import this custom function, that incorporates also overfitting and underfitting in the inner folds:\n",
    "```python\n",
    "def objective_scorer(MSE_valid, MSE_train):\n",
    "    \"\"\"Objective scorer for the hyperparameter optimization.\"\"\"\n",
    "\n",
    "    MSE_delta = MSE_train - MSE_valid\n",
    "    weight_MSE_delta = 0.5\n",
    "    target_delta = .05\n",
    "        \n",
    "        #MSE_valid + weight_MSE_delta * abs(MSE_delta)\n",
    "    return (1 * MSE_valid + \n",
    "            0.5 * abs(MSE_delta) + \n",
    "            2 * max(0, (MSE_delta - target_delta)) + \n",
    "            1 * max(0, -MSE_delta)\n",
    "            )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how to use the custom objective scorer function in the cross_validate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexcv.scorer import custom_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mse_wrapper(y_valid, y_pred, y_train_in, y_pred_train):\n",
    "    return mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "results = cross_validate(\n",
    "    RunConfiguration=run_config,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    run=dummy_run,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    "    mapping=model_map,\n",
    "    scorer=custom_scorer  # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for Earth using rpy2\n",
    "Adaptation of statsmodels classes with random slopes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexcv.cross_val import DataLoadConfigurator\n",
    "from flexcv.cross_val import CVData\n",
    "from flexcv.cross_val import CrossValConfigurator\n",
    "from flexcv.cross_val import MixedEffectsConfigurator\n",
    "from flexcv.cross_val import RunConfigurator\n",
    "from flexcv.cross_val import OptimizationConfigurator\n",
    "from flexcv.cross_val import CrossValidation\n",
    "from flexcv.cross_val import perform\n",
    "\n",
    "\n",
    "data_config = DataLoadConfigurator(\n",
    "    dataset_name=\"random_example\",\n",
    "    model_level=\"fixed_only\",\n",
    "    target_name=y.name,\n",
    "    slopes=random_slopes.columns.tolist(),\n",
    ")\n",
    "\n",
    "cv_config = CrossValConfigurator(\n",
    "    cross_val_method=CrossValMethod.KFOLD,\n",
    "    cross_val_method_in=CrossValMethod.KFOLD,\n",
    "    n_splits=5,\n",
    "    scale_in=True,\n",
    "    scale_out=True,\n",
    "    break_cross_val=False,\n",
    "    # metrics={...} would allow to pass a dict of custom metrics\n",
    ")\n",
    "\n",
    "mixed_config = MixedEffectsConfigurator(\n",
    "    em_max_iterations=50,\n",
    "    em_stopping_window=10,\n",
    "    em_tolerance=1e-6,\n",
    ")\n",
    "\n",
    "run_config = RunConfigurator(\n",
    "    run=dummy_run\n",
    ")\n",
    "\n",
    "opt_config = OptimizationConfigurator(\n",
    "    optuna=True,\n",
    "    n_trials=10,\n",
    ")\n",
    "\n",
    "cv_data = CVData(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    group=group,\n",
    "    slopes=random_slopes,\n",
    ")\n",
    "\n",
    "cv = CrossValidation(\n",
    "    DataLoadConfiguration=data_config,\n",
    "    CrossValConfiguration=cv_config,\n",
    "    MixedEffectsConfiguration=mixed_config,\n",
    "    RunConfiguration=run_config,\n",
    "    OptimizationConfiguration=opt_config,\n",
    "    model_mapping=model_map,\n",
    "    CVData=cv_data,\n",
    ")\n",
    "\n",
    "for seed in [42, 43, 44]:\n",
    "    cv.random_seed = seed\n",
    "    results = perform(cv)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
